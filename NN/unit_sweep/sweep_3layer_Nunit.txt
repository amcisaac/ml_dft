Using data for  U0
Training data size:  40000
Testing data size:  10000
Number of layers:  3
Number of units in layer [1,2,3]:  [25, 25, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  5310.600758299836
Training MSE:   5310.600758299836
Training MAE:   18.22563007621765
Testing loss:   0.1719762561440468
Testing MSE:    0.1719762561440468
Testing MAE:    0.2751443496704102

Epoch  1
Training loss:  0.1084383157491684
Training MSE:   0.1084383157491684
Training MAE:   0.22390743255615234
Testing loss:   0.08342908874154091
Testing MSE:    0.08342908874154091
Testing MAE:    0.20072339630126954

Epoch  2
Training loss:  0.08439074933677912
Training MSE:   0.08439074933677912
Training MAE:   0.20943166847229003
Testing loss:   0.06316366304755211
Testing MSE:    0.06316366304755211
Testing MAE:    0.17679993591308593

Epoch  3
Training loss:  0.07775215225741267
Training MSE:   0.07775215225741267
Training MAE:   0.20368163528442382
Testing loss:   0.05644041785597801
Testing MSE:    0.05644041785597801
Testing MAE:    0.17339270935058593

Epoch  4
Training loss:  0.07922866804897785
Training MSE:   0.07922866804897785
Training MAE:   0.20900197944641113
Testing loss:   0.05332722182869911
Testing MSE:    0.05332722182869911
Testing MAE:    0.16882274475097656

Epoch  5
Training loss:  0.09449334458708764
Training MSE:   0.09449334458708764
Training MAE:   0.23251898498535156
Testing loss:   0.04767356794774532
Testing MSE:    0.04767356794774532
Testing MAE:    0.15398665924072266

Epoch  6
Training loss:  0.14507569249868393
Training MSE:   0.14507569249868393
Training MAE:   0.2955420639038086
Testing loss:   0.059355656629800795
Testing MSE:    0.059355656629800795
Testing MAE:    0.1890050735473633

Epoch  7
Training loss:  0.17122516804262997
Training MSE:   0.17122516804262997
Training MAE:   0.32306508750915525
Testing loss:   0.6123104447364807
Testing MSE:    0.6123104447364807
Testing MAE:    0.7547470596313477

Epoch  8
Training loss:  0.19306908909529447
Training MSE:   0.19306908909529447
Training MAE:   0.3462692211151123
Testing loss:   0.11910234858989716
Testing MSE:    0.11910234858989716
Testing MAE:    0.301710124206543

Epoch  9
Training loss:  0.17175225685983897
Training MSE:   0.17175225685983897
Training MAE:   0.3273745834350586
Testing loss:   0.04540763944387436
Testing MSE:    0.04540763944387436
Testing MAE:    0.1622106216430664

Training loss:  0.04591258105188608
Training MSE:   0.04591258105188608
Training MAE:   0.1614529613494873

Testing loss:  0.04540763944387436
Testing MSE:   0.04540763944387436
Testing MAE:   0.1622106216430664

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 25, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  5988.544127328109
Training MSE:   5988.544127328109
Training MAE:   22.79687355041504
Testing loss:   11.89140204925537
Testing MSE:    11.89140204925537
Testing MAE:    2.6534299880981447

Epoch  1
Training loss:  5.209324447107315
Training MSE:   5.209324447107315
Training MAE:   1.6964660675048828
Testing loss:   1.5316360948562622
Testing MSE:    1.5316360948562622
Testing MAE:    0.9544399627685547

Epoch  2
Training loss:  0.7119491116166115
Training MSE:   0.7119491116166115
Training MAE:   0.6194800521850586
Testing loss:   0.24505077776908873
Testing MSE:    0.24505077776908873
Testing MAE:    0.36100624084472654

Epoch  3
Training loss:  0.17136463934630156
Training MSE:   0.17136463934630156
Training MAE:   0.2854557697296143
Testing loss:   0.11593894471526146
Testing MSE:    0.11593894471526146
Testing MAE:    0.2230828414916992

Epoch  4
Training loss:  0.1154807929880917
Training MSE:   0.1154807929880917
Training MAE:   0.2314912769317627
Testing loss:   0.16161773545742034
Testing MSE:    0.16161773545742034
Testing MAE:    0.32498580627441404

Epoch  5
Training loss:  0.11194792481064797
Training MSE:   0.11194792481064797
Training MAE:   0.23887442932128905
Testing loss:   0.07194528834819794
Testing MSE:    0.07194528834819794
Testing MAE:    0.17914556732177733

Epoch  6
Training loss:  0.161714082570374
Training MSE:   0.161714082570374
Training MAE:   0.301087837600708
Testing loss:   0.07129411938786506
Testing MSE:    0.07129411938786506
Testing MAE:    0.18660085601806642

Epoch  7
Training loss:  0.19322383147329092
Training MSE:   0.19322383147329092
Training MAE:   0.3374861610412598
Testing loss:   0.2046091223478317
Testing MSE:    0.2046091223478317
Testing MAE:    0.39561667785644533

Epoch  8
Training loss:  0.1996507488399744
Training MSE:   0.1996507488399744
Training MAE:   0.342594718170166
Testing loss:   0.23253554916381836
Testing MSE:    0.23253554916381836
Testing MAE:    0.4304267822265625

Epoch  9
Training loss:  0.2071875063613057
Training MSE:   0.2071875063613057
Training MAE:   0.3514850875854492
Testing loss:   0.11939678173065185
Testing MSE:    0.11939678173065185
Testing MAE:    0.2815561126708984

Training loss:  0.11853727852702141
Training MSE:   0.11853727852702141
Training MAE:   0.2809190490722656

Testing loss:  0.11939678173065185
Testing MSE:   0.11939678173065185
Testing MAE:   0.2815561126708984

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 25, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  6043.50448272171
Training MSE:   6043.50448272171
Training MAE:   23.63968365936279
Testing loss:   19.019852160644533
Testing MSE:    19.019852160644533
Testing MAE:    3.3294093872070314

Epoch  1
Training loss:  11.190139575004578
Training MSE:   11.190139575004578
Training MAE:   2.454782740020752
Testing loss:   4.848964618301392
Testing MSE:    4.848964618301392
Testing MAE:    1.6342241012573242

Epoch  2
Training loss:  2.5006584856271745
Training MSE:   2.5006584856271745
Training MAE:   1.1181455535888671
Testing loss:   1.0090711811065673
Testing MSE:    1.0090711811065673
Testing MAE:    0.7344569961547851

Epoch  3
Training loss:  0.6087767772495747
Training MSE:   0.6087767772495747
Training MAE:   0.5575357414245605
Testing loss:   0.30237566158771517
Testing MSE:    0.30237566158771517
Testing MAE:    0.3833304733276367

Epoch  4
Training loss:  0.21484905214309694
Training MSE:   0.21484905214309694
Training MAE:   0.3160361846923828
Testing loss:   0.35146654357910156
Testing MSE:    0.35146654357910156
Testing MAE:    0.5137092544555664

Epoch  5
Training loss:  0.1334697715267539
Training MSE:   0.1334697715267539
Training MAE:   0.24481679382324217
Testing loss:   0.10546979937553405
Testing MSE:    0.10546979937553405
Testing MAE:    0.21008754577636718

Epoch  6
Training loss:  0.11507585043758153
Training MSE:   0.11507585043758153
Training MAE:   0.2370769557952881
Testing loss:   0.10674212428331376
Testing MSE:    0.10674212428331376
Testing MAE:    0.24349928588867187

Epoch  7
Training loss:  0.130865568959713
Training MSE:   0.130865568959713
Training MAE:   0.2686901847839355
Testing loss:   0.2214743944644928
Testing MSE:    0.2214743944644928
Testing MAE:    0.4107470275878906

Epoch  8
Training loss:  0.16070093177706002
Training MSE:   0.16070093177706002
Training MAE:   0.30462031745910645
Testing loss:   0.14919378077983855
Testing MSE:    0.14919378077983855
Testing MAE:    0.32195485534667967

Epoch  9
Training loss:  0.14469047045111655
Training MSE:   0.14469047045111655
Training MAE:   0.2878873497009277
Testing loss:   0.13295754607915877
Testing MSE:    0.13295754607915877
Testing MAE:    0.3014249588012695

Training loss:  0.1308150943517685
Training MSE:   0.1308150943517685
Training MAE:   0.3017582538604736

Testing loss:  0.13295754607915877
Testing MSE:   0.13295754607915877
Testing MAE:   0.3014249588012695

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 25, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10048.353543498355
Training MSE:   10048.353543498355
Training MAE:   31.19063955345154
Testing loss:   0.3077162250757217
Testing MSE:    0.3077162250757217
Testing MAE:    0.3839848907470703

Epoch  1
Training loss:  0.14515347189754246
Training MSE:   0.14515347189754246
Training MAE:   0.23721321144104004
Testing loss:   0.10243211384415626
Testing MSE:    0.10243211384415626
Testing MAE:    0.20130891876220702

Epoch  2
Training loss:  0.08663782933577895
Training MSE:   0.08663782933577895
Training MAE:   0.19121366729736328
Testing loss:   0.08056689556241035
Testing MSE:    0.08056689556241035
Testing MAE:    0.200861962890625

Epoch  3
Training loss:  0.07473230732828379
Training MSE:   0.07473230732828379
Training MAE:   0.18642370262145996
Testing loss:   0.08493031202554703
Testing MSE:    0.08493031202554703
Testing MAE:    0.2201933319091797

Epoch  4
Training loss:  0.0720394069686532
Training MSE:   0.0720394069686532
Training MAE:   0.19029987182617186
Testing loss:   0.10896049157381058
Testing MSE:    0.10896049157381058
Testing MAE:    0.25758274536132814

Epoch  5
Training loss:  0.07177103391438723
Training MSE:   0.07177103391438723
Training MAE:   0.192973934173584
Testing loss:   0.07931433476805687
Testing MSE:    0.07931433476805687
Testing MAE:    0.22000314025878906

Epoch  6
Training loss:  0.07764321919158101
Training MSE:   0.07764321919158101
Training MAE:   0.20498496437072755
Testing loss:   0.0636720500767231
Testing MSE:    0.0636720500767231
Testing MAE:    0.18216019744873047

Epoch  7
Training loss:  0.10534041463583708
Training MSE:   0.10534041463583708
Training MAE:   0.24630441284179688
Testing loss:   0.20571214101314544
Testing MSE:    0.20571214101314544
Testing MAE:    0.4072798141479492

Epoch  8
Training loss:  0.10355742130354047
Training MSE:   0.10355742130354047
Training MAE:   0.24381270637512206
Testing loss:   0.07578194170594216
Testing MSE:    0.07578194170594216
Testing MAE:    0.21343065490722657

Epoch  9
Training loss:  0.1134386540517211
Training MSE:   0.1134386540517211
Training MAE:   0.2590149017333984
Testing loss:   0.03944475892484188
Testing MSE:    0.03944475892484188
Testing MAE:    0.1376393615722656

Training loss:  0.040257384698837995
Training MSE:   0.040257384698837995
Training MAE:   0.13766333198547362

Testing loss:  0.03944475892484188
Testing MSE:   0.03944475892484188
Testing MAE:   0.1376393615722656

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 25, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7695.6566308321
Training MSE:   7695.6566308321
Training MAE:   28.243863488006593
Testing loss:   2.166820550537109
Testing MSE:    2.166820550537109
Testing MAE:    1.1262674118041993

Epoch  1
Training loss:  0.41841147549450397
Training MSE:   0.41841147549450397
Training MAE:   0.42915210609436033
Testing loss:   0.11010545979738236
Testing MSE:    0.11010545979738236
Testing MAE:    0.2355708709716797

Epoch  2
Training loss:  0.09047535908669234
Training MSE:   0.09047535908669234
Training MAE:   0.19868657455444336
Testing loss:   0.17034524073600768
Testing MSE:    0.17034524073600768
Testing MAE:    0.34994555969238283

Epoch  3
Training loss:  0.07963817768096924
Training MSE:   0.07963817768096924
Training MAE:   0.19523916282653808
Testing loss:   0.07451990010738373
Testing MSE:    0.07451990010738373
Testing MAE:    0.20093864135742187

Epoch  4
Training loss:  0.07503718587756157
Training MSE:   0.07503718587756157
Training MAE:   0.19316388092041015
Testing loss:   0.11027345839738846
Testing MSE:    0.11027345839738846
Testing MAE:    0.26833069458007813

Epoch  5
Training loss:  0.08170826249569654
Training MSE:   0.08170826249569654
Training MAE:   0.20793980598449707
Testing loss:   0.05650735929012299
Testing MSE:    0.05650735929012299
Testing MAE:    0.1677801315307617

Epoch  6
Training loss:  0.09066467133909464
Training MSE:   0.09066467133909464
Training MAE:   0.22501382675170897
Testing loss:   0.07247590503692627
Testing MSE:    0.07247590503692627
Testing MAE:    0.20721087036132813

Epoch  7
Training loss:  0.12389291214048863
Training MSE:   0.12389291214048863
Training MAE:   0.26787452964782715
Testing loss:   0.05690722643733025
Testing MSE:    0.05690722643733025
Testing MAE:    0.17649464569091797

Epoch  8
Training loss:  0.12170690739154816
Training MSE:   0.12170690739154816
Training MAE:   0.26824242668151854
Testing loss:   0.06337000461816787
Testing MSE:    0.06337000461816787
Testing MAE:    0.19314268951416017

Epoch  9
Training loss:  0.12753361710458994
Training MSE:   0.12753361710458994
Training MAE:   0.2747303382873535
Testing loss:   0.04592493494749069
Testing MSE:    0.04592493494749069
Testing MAE:    0.15569538879394532

Training loss:  0.04803429470360279
Training MSE:   0.04803429470360279
Training MAE:   0.1571193920135498

Testing loss:  0.04592493494749069
Testing MSE:   0.04592493494749069
Testing MAE:   0.15569538879394532

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 20, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  5436.821904262924
Training MSE:   5436.821904262924
Training MAE:   23.064700568008423
Testing loss:   7.307717251586914
Testing MSE:    7.307717251586914
Testing MAE:    2.170573435974121

Epoch  1
Training loss:  1.1585013797551393
Training MSE:   1.1585013797551393
Training MAE:   0.692806396484375
Testing loss:   0.12340910912752151
Testing MSE:    0.12340910912752151
Testing MAE:    0.24426467742919922

Epoch  2
Training loss:  0.10760967623591423
Training MSE:   0.10760967623591423
Training MAE:   0.21777383308410644
Testing loss:   0.08647610489726067
Testing MSE:    0.08647610489726067
Testing MAE:    0.20033355712890624

Epoch  3
Training loss:  0.09686184336990118
Training MSE:   0.09686184336990118
Training MAE:   0.21737223205566406
Testing loss:   0.09716150411367416
Testing MSE:    0.09716150411367416
Testing MAE:    0.22003500061035156

Epoch  4
Training loss:  0.09352662410587072
Training MSE:   0.09352662410587072
Training MAE:   0.2202432014465332
Testing loss:   0.10108580377101899
Testing MSE:    0.10108580377101899
Testing MAE:    0.23115284729003907

Epoch  5
Training loss:  0.10011660529226064
Training MSE:   0.10011660529226064
Training MAE:   0.23289072647094727
Testing loss:   0.19538113117218017
Testing MSE:    0.19538113117218017
Testing MAE:    0.38045048217773436

Epoch  6
Training loss:  0.1288577389165759
Training MSE:   0.1288577389165759
Training MAE:   0.26915639228820804
Testing loss:   0.13263145387172698
Testing MSE:    0.13263145387172698
Testing MAE:    0.30395868682861327

Epoch  7
Training loss:  0.16727923842221498
Training MSE:   0.16727923842221498
Training MAE:   0.31574502487182615
Testing loss:   0.11820248415470123
Testing MSE:    0.11820248415470123
Testing MAE:    0.28507123107910154

Epoch  8
Training loss:  0.18435967026799918
Training MSE:   0.18435967026799918
Training MAE:   0.3364390426635742
Testing loss:   0.08197154474854469
Testing MSE:    0.08197154474854469
Testing MAE:    0.21774773406982423

Epoch  9
Training loss:  0.16454901253432036
Training MSE:   0.16454901253432036
Training MAE:   0.313653910446167
Testing loss:   0.08899083459377288
Testing MSE:    0.08899083459377288
Testing MAE:    0.24357261810302736

Training loss:  0.0894495973110199
Training MSE:   0.0894495973110199
Training MAE:   0.24385765991210936

Testing loss:  0.08899083459377288
Testing MSE:   0.08899083459377288
Testing MAE:   0.24357261810302736

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 20, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  5080.963530680632
Training MSE:   5080.963530680632
Training MAE:   19.59021615600586
Testing loss:   0.3193784470796585
Testing MSE:    0.3193784470796585
Testing MAE:    0.416100032043457

Epoch  1
Training loss:  0.21347193753421306
Training MSE:   0.21347193753421306
Training MAE:   0.2895431774139404
Testing loss:   0.1589816087603569
Testing MSE:    0.1589816087603569
Testing MAE:    0.29140925903320314

Epoch  2
Training loss:  0.11287364498078824
Training MSE:   0.11287364498078824
Training MAE:   0.22013930320739747
Testing loss:   0.08142820696830749
Testing MSE:    0.08142820696830749
Testing MAE:    0.1935881820678711

Epoch  3
Training loss:  0.09538442391455174
Training MSE:   0.09538442391455174
Training MAE:   0.21547959823608398
Testing loss:   0.07009365962147712
Testing MSE:    0.07009365962147712
Testing MAE:    0.19564605255126954

Epoch  4
Training loss:  0.08959447898566723
Training MSE:   0.08959447898566723
Training MAE:   0.2151274055480957
Testing loss:   0.07637391246557236
Testing MSE:    0.07637391246557236
Testing MAE:    0.2004779769897461

Epoch  5
Training loss:  0.08687565257996321
Training MSE:   0.08687565257996321
Training MAE:   0.21577466316223146
Testing loss:   0.11020923990011215
Testing MSE:    0.11020923990011215
Testing MAE:    0.2750835754394531

Epoch  6
Training loss:  0.11960371489077806
Training MSE:   0.11960371489077806
Training MAE:   0.2618922992706299
Testing loss:   0.05195368413925171
Testing MSE:    0.05195368413925171
Testing MAE:    0.16661213684082032

Epoch  7
Training loss:  0.15210380555763842
Training MSE:   0.15210380555763842
Training MAE:   0.3022537540435791
Testing loss:   0.04447613871395588
Testing MSE:    0.04447613871395588
Testing MAE:    0.14814503021240236

Epoch  8
Training loss:  0.1558412123583257
Training MSE:   0.1558412123583257
Training MAE:   0.3064732494354248
Testing loss:   0.1501474258184433
Testing MSE:    0.1501474258184433
Testing MAE:    0.33804790344238284

Epoch  9
Training loss:  0.1632710800051689
Training MSE:   0.1632710800051689
Training MAE:   0.31471385269165036
Testing loss:   0.202409525680542
Testing MSE:    0.202409525680542
Testing MAE:    0.41165054931640627

Training loss:  0.2050990324616432
Training MSE:   0.2050990324616432
Training MAE:   0.4129435287475586

Testing loss:  0.202409525680542
Testing MSE:   0.202409525680542
Testing MAE:   0.41165054931640627

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 20, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  3026.605698905456
Training MSE:   3026.605698905456
Training MAE:   13.973278302383424
Testing loss:   0.28170268688201905
Testing MSE:    0.28170268688201905
Testing MAE:    0.3416716827392578

Epoch  1
Training loss:  0.1675734994828701
Training MSE:   0.1675734994828701
Training MAE:   0.2643958404541016
Testing loss:   0.12006340346336365
Testing MSE:    0.12006340346336365
Testing MAE:    0.21168499298095703

Epoch  2
Training loss:  0.09618471000194549
Training MSE:   0.09618471000194549
Training MAE:   0.2093225383758545
Testing loss:   0.09779728723168372
Testing MSE:    0.09779728723168372
Testing MAE:    0.19494504699707033

Epoch  3
Training loss:  0.08599918511062861
Training MSE:   0.08599918511062861
Training MAE:   0.20698351516723631
Testing loss:   0.07268489573001861
Testing MSE:    0.07268489573001861
Testing MAE:    0.17985625305175781

Epoch  4
Training loss:  0.08012076911330224
Training MSE:   0.08012076911330224
Training MAE:   0.20421177558898926
Testing loss:   0.06505559335947037
Testing MSE:    0.06505559335947037
Testing MAE:    0.16865775756835938

Epoch  5
Training loss:  0.09042918912917376
Training MSE:   0.09042918912917376
Training MAE:   0.2224136951446533
Testing loss:   0.18778155789375306
Testing MSE:    0.18778155789375306
Testing MAE:    0.3665545852661133

Epoch  6
Training loss:  0.10565372927337885
Training MSE:   0.10565372927337885
Training MAE:   0.24642023277282715
Testing loss:   0.05513605052232742
Testing MSE:    0.05513605052232742
Testing MAE:    0.1602214370727539

Epoch  7
Training loss:  0.1263353300794959
Training MSE:   0.1263353300794959
Training MAE:   0.27442279624938964
Testing loss:   0.049592117872834206
Testing MSE:    0.049592117872834206
Testing MAE:    0.15255589904785155

Epoch  8
Training loss:  0.13345340886563062
Training MSE:   0.13345340886563062
Training MAE:   0.28284094276428223
Testing loss:   0.09683299763202667
Testing MSE:    0.09683299763202667
Testing MAE:    0.2540051040649414

Epoch  9
Training loss:  0.12822279467433692
Training MSE:   0.12822279467433692
Training MAE:   0.27809746322631834
Testing loss:   0.5815656316757202
Testing MSE:    0.5815656316757202
Testing MAE:    0.7328512481689453

Training loss:  0.5763806518793106
Training MSE:   0.5763806518793106
Training MAE:   0.7311866828918457

Testing loss:  0.5815656316757202
Testing MSE:   0.5815656316757202
Testing MAE:   0.7328512481689453

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 20, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7772.589741086936
Training MSE:   7772.589741086936
Training MAE:   26.23807683067322
Testing loss:   0.6678994203567505
Testing MSE:    0.6678994203567505
Testing MAE:    0.6373742111206054

Epoch  1
Training loss:  0.2749468344181776
Training MSE:   0.2749468344181776
Training MAE:   0.36515841255187986
Testing loss:   0.10814926332235336
Testing MSE:    0.10814926332235336
Testing MAE:    0.2296377182006836

Epoch  2
Training loss:  0.10240983059704303
Training MSE:   0.10240983059704303
Training MAE:   0.2240349235534668
Testing loss:   0.1452434553861618
Testing MSE:    0.1452434553861618
Testing MAE:    0.3154571319580078

Epoch  3
Training loss:  0.0855272708773613
Training MSE:   0.0855272708773613
Training MAE:   0.2102813491821289
Testing loss:   0.06379306147098542
Testing MSE:    0.06379306147098542
Testing MAE:    0.17848861083984374

Epoch  4
Training loss:  0.07935523476451635
Training MSE:   0.07935523476451635
Training MAE:   0.20594974517822265
Testing loss:   0.08262060484886169
Testing MSE:    0.08262060484886169
Testing MAE:    0.2268171340942383

Epoch  5
Training loss:  0.08201774015203118
Training MSE:   0.08201774015203118
Training MAE:   0.21207917556762695
Testing loss:   0.05250457937717438
Testing MSE:    0.05250457937717438
Testing MAE:    0.16103245239257813

Epoch  6
Training loss:  0.0838400141954422
Training MSE:   0.0838400141954422
Training MAE:   0.2155136257171631
Testing loss:   0.04881344112157822
Testing MSE:    0.04881344112157822
Testing MAE:    0.15606936187744141

Epoch  7
Training loss:  0.10600581556260585
Training MSE:   0.10600581556260585
Training MAE:   0.250002779006958
Testing loss:   0.08705485527515411
Testing MSE:    0.08705485527515411
Testing MAE:    0.23575704193115235

Epoch  8
Training loss:  0.11360630309134721
Training MSE:   0.11360630309134721
Training MAE:   0.2567386764526367
Testing loss:   0.046789141181111334
Testing MSE:    0.046789141181111334
Testing MAE:    0.15506709442138672

Epoch  9
Training loss:  0.11366044112443924
Training MSE:   0.11366044112443924
Training MAE:   0.26094684562683107
Testing loss:   0.036956755566596985
Testing MSE:    0.036956755566596985
Testing MAE:    0.13859732971191407

Training loss:  0.03813794805407524
Training MSE:   0.03813794805407524
Training MAE:   0.13878861808776854

Testing loss:  0.036956755566596985
Testing MSE:   0.036956755566596985
Testing MAE:   0.13859732971191407

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 20, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  12925.516702622223
Training MSE:   12925.516702622223
Training MAE:   40.288736151885985
Testing loss:   7.357391738891602
Testing MSE:    7.357391738891602
Testing MAE:    2.0652684616088868

Epoch  1
Training loss:  6.026531450557709
Training MSE:   6.026531450557709
Training MAE:   1.8739624626159668
Testing loss:   4.764014362335205
Testing MSE:    4.764014362335205
Testing MAE:    1.6662264083862304

Epoch  2
Training loss:  3.491032189655304
Training MSE:   3.491032189655304
Training MAE:   1.416215470123291
Testing loss:   2.3872094591140747
Testing MSE:    2.3872094591140747
Testing MAE:    1.163773924255371

Epoch  3
Training loss:  1.5826446791172029
Training MSE:   1.5826446791172029
Training MAE:   0.9410897483825683
Testing loss:   0.9578567358970642
Testing MSE:    0.9578567358970642
Testing MAE:    0.7348761138916016

Epoch  4
Training loss:  0.6587878305196762
Training MSE:   0.6587878305196762
Training MAE:   0.6016605827331543
Testing loss:   0.46804272918701173
Testing MSE:    0.46804272918701173
Testing MAE:    0.5305555603027344

Epoch  5
Training loss:  0.2986083305180073
Training MSE:   0.2986083305180073
Training MAE:   0.3956640132904053
Testing loss:   0.17812593377828598
Testing MSE:    0.17812593377828598
Testing MAE:    0.2917297348022461

Epoch  6
Training loss:  0.15436158023774624
Training MSE:   0.15436158023774624
Training MAE:   0.27147028350830077
Testing loss:   0.10969517481923104
Testing MSE:    0.10969517481923104
Testing MAE:    0.23146343841552736

Epoch  7
Training loss:  0.11284623749777675
Training MSE:   0.11284623749777675
Training MAE:   0.23321415328979492
Testing loss:   0.07574993733167648
Testing MSE:    0.07574993733167648
Testing MAE:    0.18893342742919922

Epoch  8
Training loss:  0.10789350023269653
Training MSE:   0.10789350023269653
Training MAE:   0.23836638488769532
Testing loss:   0.07865436554551125
Testing MSE:    0.07865436554551125
Testing MAE:    0.20901332397460937

Epoch  9
Training loss:  0.10090743225365877
Training MSE:   0.10090743225365877
Training MAE:   0.23520566902160644
Testing loss:   0.14169130729436874
Testing MSE:    0.14169130729436874
Testing MAE:    0.3078634826660156

Training loss:  0.1425789376437664
Training MSE:   0.1425789376437664
Training MAE:   0.30743745613098145

Testing loss:  0.14169130729436874
Testing MSE:   0.14169130729436874
Testing MAE:   0.3078634826660156

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 15, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  6272.358704778671
Training MSE:   6272.358704778671
Training MAE:   25.691068297195436
Testing loss:   13.770513414001465
Testing MSE:    13.770513414001465
Testing MAE:    2.8576962509155273

Epoch  1
Training loss:  5.7659053366661075
Training MSE:   5.7659053366661075
Training MAE:   1.7913749355316162
Testing loss:   1.3874084636688233
Testing MSE:    1.3874084636688233
Testing MAE:    0.9266538345336914

Epoch  2
Training loss:  0.5255390731185675
Training MSE:   0.5255390731185675
Training MAE:   0.5177735836029053
Testing loss:   0.21551419657468796
Testing MSE:    0.21551419657468796
Testing MAE:    0.3427936248779297

Epoch  3
Training loss:  0.13416607389748098
Training MSE:   0.13416607389748098
Training MAE:   0.2353715404510498
Testing loss:   0.09813034335374832
Testing MSE:    0.09813034335374832
Testing MAE:    0.19211629791259766

Epoch  4
Training loss:  0.11044034546986223
Training MSE:   0.11044034546986223
Training MAE:   0.21932734832763673
Testing loss:   0.18915841562747956
Testing MSE:    0.18915841562747956
Testing MAE:    0.3482877014160156

Epoch  5
Training loss:  0.11465491658225656
Training MSE:   0.11465491658225656
Training MAE:   0.24263184432983398
Testing loss:   0.0705818287730217
Testing MSE:    0.0705818287730217
Testing MAE:    0.1728357177734375

Epoch  6
Training loss:  0.1338202487140894
Training MSE:   0.1338202487140894
Training MAE:   0.27148584632873535
Testing loss:   0.1172561079621315
Testing MSE:    0.1172561079621315
Testing MAE:    0.27106856536865237

Epoch  7
Training loss:  0.13131034678667783
Training MSE:   0.13131034678667783
Training MAE:   0.2719181129455566
Testing loss:   0.10677446835041046
Testing MSE:    0.10677446835041046
Testing MAE:    0.2634226333618164

Epoch  8
Training loss:  0.15113679711446165
Training MSE:   0.15113679711446165
Training MAE:   0.29913905181884765
Testing loss:   0.06583039212822914
Testing MSE:    0.06583039212822914
Testing MAE:    0.19035165405273438

Epoch  9
Training loss:  0.13307316179126502
Training MSE:   0.13307316179126502
Training MAE:   0.28375242691040037
Testing loss:   0.045133663073182104
Testing MSE:    0.045133663073182104
Testing MAE:    0.14451873626708983

Training loss:  0.04448430853784084
Training MSE:   0.04448430853784084
Training MAE:   0.14303039512634277

Testing loss:  0.045133663073182104
Testing MSE:   0.045133663073182104
Testing MAE:   0.14451873626708983

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 15, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7341.089620131684
Training MSE:   7341.089620131684
Training MAE:   27.450204400634764
Testing loss:   22.541084358215333
Testing MSE:    22.541084358215333
Testing MAE:    3.7635409591674804

Epoch  1
Training loss:  10.839127754974365
Training MSE:   10.839127754974365
Training MAE:   2.501279675292969
Testing loss:   3.2743675613403322
Testing MSE:    3.2743675613403322
Testing MAE:    1.4055141815185548

Epoch  2
Training loss:  1.5270363157987594
Training MSE:   1.5270363157987594
Training MAE:   0.9182366241455078
Testing loss:   0.579271799325943
Testing MSE:    0.579271799325943
Testing MAE:    0.5850266235351562

Epoch  3
Training loss:  0.32361824868619443
Training MSE:   0.32361824868619443
Training MAE:   0.405189741897583
Testing loss:   0.15044331678152084
Testing MSE:    0.15044331678152084
Testing MAE:    0.26379269561767577

Epoch  4
Training loss:  0.13300034398287536
Training MSE:   0.13300034398287536
Training MAE:   0.24116297302246092
Testing loss:   0.11201535012125968
Testing MSE:    0.11201535012125968
Testing MAE:    0.22030382385253905

Epoch  5
Training loss:  0.10816879912465811
Training MSE:   0.10816879912465811
Training MAE:   0.2240733238220215
Testing loss:   0.08535117403268815
Testing MSE:    0.08535117403268815
Testing MAE:    0.19238192596435547

Epoch  6
Training loss:  0.11226074918061495
Training MSE:   0.11226074918061495
Training MAE:   0.24099644088745117
Testing loss:   0.06411712514162064
Testing MSE:    0.06411712514162064
Testing MAE:    0.1754768829345703

Epoch  7
Training loss:  0.13944929256737232
Training MSE:   0.13944929256737232
Training MAE:   0.28398786544799803
Testing loss:   0.05787352860569954
Testing MSE:    0.05787352860569954
Testing MAE:    0.17084198608398438

Epoch  8
Training loss:  0.14596145845353603
Training MSE:   0.14596145845353603
Training MAE:   0.2961431728363037
Testing loss:   0.05033371295928955
Testing MSE:    0.05033371295928955
Testing MAE:    0.15339446716308594

Epoch  9
Training loss:  0.13961590600013732
Training MSE:   0.13961590600013732
Training MAE:   0.2851778942108154
Testing loss:   0.10159899173974991
Testing MSE:    0.10159899173974991
Testing MAE:    0.2591623565673828

Training loss:  0.10320677506327629
Training MSE:   0.10320677506327629
Training MAE:   0.2595309200286865

Testing loss:  0.10159899173974991
Testing MSE:   0.10159899173974991
Testing MAE:   0.2591623565673828

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 15, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  5713.723213566589
Training MSE:   5713.723213566589
Training MAE:   25.78265174636841
Testing loss:   32.56163028259277
Testing MSE:    32.56163028259277
Testing MAE:    4.4035544723510744

Epoch  1
Training loss:  5.5350110193729405
Training MSE:   5.5350110193729405
Training MAE:   1.4565319061279296
Testing loss:   0.5602898496150971
Testing MSE:    0.5602898496150971
Testing MAE:    0.5629072891235352

Epoch  2
Training loss:  0.2797994511783123
Training MSE:   0.2797994511783123
Training MAE:   0.34868543663024903
Testing loss:   0.12167300170063973
Testing MSE:    0.12167300170063973
Testing MAE:    0.22955330200195312

Epoch  3
Training loss:  0.13196830897778272
Training MSE:   0.13196830897778272
Training MAE:   0.22484291534423828
Testing loss:   0.10040451382994652
Testing MSE:    0.10040451382994652
Testing MAE:    0.2187348648071289

Epoch  4
Training loss:  0.11133121141344308
Training MSE:   0.11133121141344308
Training MAE:   0.2221528148651123
Testing loss:   0.07647662155032157
Testing MSE:    0.07647662155032157
Testing MAE:    0.20234371490478514

Epoch  5
Training loss:  0.10884316689819097
Training MSE:   0.10884316689819097
Training MAE:   0.23359915466308595
Testing loss:   0.06986879929304123
Testing MSE:    0.06986879929304123
Testing MAE:    0.18733156585693359

Epoch  6
Training loss:  0.12115819080770016
Training MSE:   0.12115819080770016
Training MAE:   0.2588351119995117
Testing loss:   0.21514015769958497
Testing MSE:    0.21514015769958497
Testing MAE:    0.40875520782470703

Epoch  7
Training loss:  0.14746405634731055
Training MSE:   0.14746405634731055
Training MAE:   0.2897308116912842
Testing loss:   0.20632399926185607
Testing MSE:    0.20632399926185607
Testing MAE:    0.40508373107910156

Epoch  8
Training loss:  0.17200712996721268
Training MSE:   0.17200712996721268
Training MAE:   0.3201032539367676
Testing loss:   0.10533388662338257
Testing MSE:    0.10533388662338257
Testing MAE:    0.26331907653808595

Epoch  9
Training loss:  0.14578672761023045
Training MSE:   0.14578672761023045
Training MAE:   0.2940498882293701
Testing loss:   0.1939829264640808
Testing MSE:    0.1939829264640808
Testing MAE:    0.3952304809570312

Training loss:  0.197961640304327
Training MSE:   0.197961640304327
Training MAE:   0.3961857452392578

Testing loss:  0.1939829264640808
Testing MSE:   0.1939829264640808
Testing MAE:   0.3952304809570312

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 15, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9544.014437415313
Training MSE:   9544.014437415313
Training MAE:   31.57395005111694
Testing loss:   7.840977582550049
Testing MSE:    7.840977582550049
Testing MAE:    2.217254838562012

Epoch  1
Training loss:  3.4693358747959135
Training MSE:   3.4693358747959135
Training MAE:   1.4026282512664794
Testing loss:   0.9510747223854065
Testing MSE:    0.9510747223854065
Testing MAE:    0.7675517578125

Epoch  2
Training loss:  0.46510975575447083
Training MSE:   0.46510975575447083
Training MAE:   0.5061509994506836
Testing loss:   0.18249064695835113
Testing MSE:    0.18249064695835113
Testing MAE:    0.3181555267333984

Epoch  3
Training loss:  0.129079840297997
Training MSE:   0.129079840297997
Training MAE:   0.24555339431762696
Testing loss:   0.0813034736931324
Testing MSE:    0.0813034736931324
Testing MAE:    0.18941660003662109

Epoch  4
Training loss:  0.08357976326942444
Training MSE:   0.08357976326942444
Training MAE:   0.19448689041137696
Testing loss:   0.06385025722980499
Testing MSE:    0.06385025722980499
Testing MAE:    0.17514601287841797

Epoch  5
Training loss:  0.07542785141170025
Training MSE:   0.07542785141170025
Training MAE:   0.1936646026611328
Testing loss:   0.09602683802843094
Testing MSE:    0.09602683802843094
Testing MAE:    0.2342032028198242

Epoch  6
Training loss:  0.08479300929680467
Training MSE:   0.08479300929680467
Training MAE:   0.21359668807983398
Testing loss:   0.052093777158856394
Testing MSE:    0.052093777158856394
Testing MAE:    0.16251941375732423

Epoch  7
Training loss:  0.09965013559758663
Training MSE:   0.09965013559758663
Training MAE:   0.23842632751464843
Testing loss:   0.0482611669421196
Testing MSE:    0.0482611669421196
Testing MAE:    0.15208729705810548

Epoch  8
Training loss:  0.10828338210731744
Training MSE:   0.10828338210731744
Training MAE:   0.2525226673126221
Testing loss:   0.05069663001894951
Testing MSE:    0.05069663001894951
Testing MAE:    0.15768438873291016

Epoch  9
Training loss:  0.10969622960612178
Training MSE:   0.10969622960612178
Training MAE:   0.25352804946899415
Testing loss:   0.04880909339785576
Testing MSE:    0.04880909339785576
Testing MAE:    0.16300619201660158

Training loss:  0.04895491771548986
Training MSE:   0.04895491771548986
Training MAE:   0.16237265129089357

Testing loss:  0.04880909339785576
Testing MSE:   0.04880909339785576
Testing MAE:   0.16300619201660158

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 15, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  170298.0463
Training MSE:   170298.0463
Training MAE:   410.71962329101564
Testing loss:   169947.17125
Testing MSE:    169947.17125
Testing MAE:    410.3074407226562

Epoch  1
Training loss:  169184.9045875
Training MSE:   169184.9045875
Training MAE:   409.36390458984374
Testing loss:   168924.71235
Testing MSE:    168924.71235
Testing MAE:    409.0595787109375

Epoch  2
Training loss:  168165.0081625
Training MSE:   168165.0081625
Training MAE:   408.116512109375
Testing loss:   167905.81715
Testing MSE:    167905.81715
Testing MAE:    407.8122642578125

Epoch  3
Training loss:  167148.6446375
Training MSE:   167148.6446375
Training MAE:   406.86927426757813
Testing loss:   166890.32945
Testing MSE:    166890.32945
Testing MAE:    406.56531640625

Epoch  4
Training loss:  166135.6431
Training MSE:   166135.6431
Training MAE:   405.62222211914064
Testing loss:   165878.225
Testing MSE:    165878.225
Testing MAE:    405.3187048339844

Epoch  5
Training loss:  165125.7761125
Training MSE:   165125.7761125
Training MAE:   404.37578061523436
Testing loss:   164868.98155
Testing MSE:    164868.98155
Testing MAE:    404.071785546875

Epoch  6
Training loss:  164118.8816
Training MSE:   164118.8816
Training MAE:   403.1288853027344
Testing loss:   163862.9217
Testing MSE:    163862.9217
Testing MAE:    402.8249595703125

Epoch  7
Training loss:  163115.1523875
Training MSE:   163115.1523875
Training MAE:   401.88219821777346
Testing loss:   162859.9465
Testing MSE:    162859.9465
Testing MAE:    401.57810385742187

Epoch  8
Training loss:  162114.6248875
Training MSE:   162114.6248875
Training MAE:   400.6353920654297
Testing loss:   161860.20255
Testing MSE:    161860.20255
Testing MAE:    400.3314001464844

Epoch  9
Training loss:  161117.384425
Training MSE:   161117.384425
Training MAE:   399.3883916015625
Testing loss:   160863.73715
Testing MSE:    160863.73715
Testing MAE:    399.08490712890625

Training loss:  160619.356975
Training MSE:   160619.356975
Training MAE:   398.7648595703125

Testing loss:  160863.73715
Testing MSE:   160863.73715
Testing MAE:   399.08490712890625

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 10, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  5512.972963170814
Training MSE:   5512.972963170814
Training MAE:   23.593865589904784
Testing loss:   3.2671219184875486
Testing MSE:    3.2671219184875486
Testing MAE:    1.4221415969848632

Epoch  1
Training loss:  1.316812790942192
Training MSE:   1.316812790942192
Training MAE:   0.8608092834472656
Testing loss:   0.42331747102737427
Testing MSE:    0.42331747102737427
Testing MAE:    0.5015126434326171

Epoch  2
Training loss:  0.24349548315405845
Training MSE:   0.24349548315405845
Training MAE:   0.35822678871154784
Testing loss:   0.11513044711351395
Testing MSE:    0.11513044711351395
Testing MAE:    0.23519268798828125

Epoch  3
Training loss:  0.11305703888088464
Training MSE:   0.11305703888088464
Training MAE:   0.23626266365051268
Testing loss:   0.07866984205842018
Testing MSE:    0.07866984205842018
Testing MAE:    0.19550437774658203

Epoch  4
Training loss:  0.09396440220326185
Training MSE:   0.09396440220326185
Training MAE:   0.21830940055847167
Testing loss:   0.0683235923588276
Testing MSE:    0.0683235923588276
Testing MAE:    0.18162130279541017

Epoch  5
Training loss:  0.09482631705701351
Training MSE:   0.09482631705701351
Training MAE:   0.22582722320556642
Testing loss:   0.07061300147175789
Testing MSE:    0.07061300147175789
Testing MAE:    0.18628223419189452

Epoch  6
Training loss:  0.10516396541595459
Training MSE:   0.10516396541595459
Training MAE:   0.24256266441345214
Testing loss:   0.0673013605773449
Testing MSE:    0.0673013605773449
Testing MAE:    0.19550458679199217

Epoch  7
Training loss:  0.1399048327073455
Training MSE:   0.1399048327073455
Training MAE:   0.2852746047973633
Testing loss:   0.13080568803548812
Testing MSE:    0.13080568803548812
Testing MAE:    0.2946909698486328

Epoch  8
Training loss:  0.1214506085306406
Training MSE:   0.1214506085306406
Training MAE:   0.2662317386627197
Testing loss:   0.08492747738361359
Testing MSE:    0.08492747738361359
Testing MAE:    0.22404837646484374

Epoch  9
Training loss:  0.12941544527560472
Training MSE:   0.12941544527560472
Training MAE:   0.2750237232208252
Testing loss:   0.04986814561486244
Testing MSE:    0.04986814561486244
Testing MAE:    0.1579483932495117

Training loss:  0.052302856159955266
Training MSE:   0.052302856159955266
Training MAE:   0.15779162178039552

Testing loss:  0.04986814561486244
Testing MSE:   0.04986814561486244
Testing MAE:   0.1579483932495117

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 10, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  14620.8999962265
Training MSE:   14620.8999962265
Training MAE:   43.371877223587035
Testing loss:   11.931198233795167
Testing MSE:    11.931198233795167
Testing MAE:    2.6318119400024416

Epoch  1
Training loss:  2.4161088454723356
Training MSE:   2.4161088454723356
Training MAE:   0.972290567779541
Testing loss:   0.3863194693565369
Testing MSE:    0.3863194693565369
Testing MAE:    0.4442851364135742

Epoch  2
Training loss:  0.23308529361486435
Training MSE:   0.23308529361486435
Training MAE:   0.33126187591552736
Testing loss:   0.13154341799020766
Testing MSE:    0.13154341799020766
Testing MAE:    0.24245789184570313

Epoch  3
Training loss:  0.11407937862128019
Training MSE:   0.11407937862128019
Training MAE:   0.21849676818847658
Testing loss:   0.08035523363947869
Testing MSE:    0.08035523363947869
Testing MAE:    0.18460360717773439

Epoch  4
Training loss:  0.084442494982481
Training MSE:   0.084442494982481
Training MAE:   0.19155824241638184
Testing loss:   0.08009630485773087
Testing MSE:    0.08009630485773087
Testing MAE:    0.20439705657958984

Epoch  5
Training loss:  0.08170363880693912
Training MSE:   0.08170363880693912
Training MAE:   0.19864151611328126
Testing loss:   0.08048145293593406
Testing MSE:    0.08048145293593406
Testing MAE:    0.21366455688476563

Epoch  6
Training loss:  0.08234386292695998
Training MSE:   0.08234386292695998
Training MAE:   0.2068458999633789
Testing loss:   0.05695032384395599
Testing MSE:    0.05695032384395599
Testing MAE:    0.16799472045898436

Epoch  7
Training loss:  0.10136456138789654
Training MSE:   0.10136456138789654
Training MAE:   0.2384750877380371
Testing loss:   0.4590785583019257
Testing MSE:    0.4590785583019257
Testing MAE:    0.6415502792358398

Epoch  8
Training loss:  0.11654735585153103
Training MSE:   0.11654735585153103
Training MAE:   0.2605215003967285
Testing loss:   0.08991348068714142
Testing MSE:    0.08991348068714142
Testing MAE:    0.23519340057373048

Epoch  9
Training loss:  0.10808006173074246
Training MSE:   0.10808006173074246
Training MAE:   0.25064165687561035
Testing loss:   0.059586511993408205
Testing MSE:    0.059586511993408205
Testing MAE:    0.17737763671875

Training loss:  0.061576405361294745
Training MSE:   0.061576405361294745
Training MAE:   0.177854195022583

Testing loss:  0.059586511993408205
Testing MSE:   0.059586511993408205
Testing MAE:   0.17737763671875

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 10, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9525.732206886887
Training MSE:   9525.732206886887
Training MAE:   31.195092374038698
Testing loss:   0.5695100382328033
Testing MSE:    0.5695100382328033
Testing MAE:    0.5710776260375977

Epoch  1
Training loss:  0.23856804645061494
Training MSE:   0.23856804645061494
Training MAE:   0.3420031673431396
Testing loss:   0.12097393312454223
Testing MSE:    0.12097393312454223
Testing MAE:    0.24267814178466796

Epoch  2
Training loss:  0.08745973369479179
Training MSE:   0.08745973369479179
Training MAE:   0.20062167205810547
Testing loss:   0.06922130435109139
Testing MSE:    0.06922130435109139
Testing MAE:    0.17394937591552734

Epoch  3
Training loss:  0.07083248254209756
Training MSE:   0.07083248254209756
Training MAE:   0.1842976547241211
Testing loss:   0.07997455511689186
Testing MSE:    0.07997455511689186
Testing MAE:    0.20016260070800782

Epoch  4
Training loss:  0.06916816624850035
Training MSE:   0.06916816624850035
Training MAE:   0.1873023250579834
Testing loss:   0.07729345383644104
Testing MSE:    0.07729345383644104
Testing MAE:    0.21519279022216797

Epoch  5
Training loss:  0.06798715841472149
Training MSE:   0.06798715841472149
Training MAE:   0.1879223518371582
Testing loss:   0.057146853941679
Testing MSE:    0.057146853941679
Testing MAE:    0.16650656433105468

Epoch  6
Training loss:  0.07070890960544347
Training MSE:   0.07070890960544347
Training MAE:   0.19457826042175294
Testing loss:   0.09420963463783265
Testing MSE:    0.09420963463783265
Testing MAE:    0.25009422454833985

Epoch  7
Training loss:  0.08694566057473421
Training MSE:   0.08694566057473421
Training MAE:   0.2206309024810791
Testing loss:   0.061260153990983966
Testing MSE:    0.061260153990983966
Testing MAE:    0.19015601501464843

Epoch  8
Training loss:  0.08827517761662602
Training MSE:   0.08827517761662602
Training MAE:   0.22430451545715333
Testing loss:   0.10790233368873596
Testing MSE:    0.10790233368873596
Testing MAE:    0.2767492950439453

Epoch  9
Training loss:  0.10374849111139774
Training MSE:   0.10374849111139774
Training MAE:   0.24822892723083495
Testing loss:   0.07584499113559723
Testing MSE:    0.07584499113559723
Testing MAE:    0.22396951751708985

Training loss:  0.07596649763584137
Training MSE:   0.07596649763584137
Training MAE:   0.22387347450256348

Testing loss:  0.07584499113559723
Testing MSE:   0.07584499113559723
Testing MAE:   0.22396951751708985

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 10, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  8713.75488551483
Training MSE:   8713.75488551483
Training MAE:   31.858611124038696
Testing loss:   45.865853033447266
Testing MSE:    45.865853033447266
Testing MAE:    5.434791896057129

Epoch  1
Training loss:  7.7210670791268345
Training MSE:   7.7210670791268345
Training MAE:   1.6987882068634033
Testing loss:   0.3602094624757767
Testing MSE:    0.3602094624757767
Testing MAE:    0.42891231689453124

Epoch  2
Training loss:  0.2056246175765991
Training MSE:   0.2056246175765991
Training MAE:   0.3039413227081299
Testing loss:   0.14900577875375748
Testing MSE:    0.14900577875375748
Testing MAE:    0.2850174026489258

Epoch  3
Training loss:  0.11515483711212873
Training MSE:   0.11515483711212873
Training MAE:   0.22592343521118163
Testing loss:   0.1716825871706009
Testing MSE:    0.1716825871706009
Testing MAE:    0.3246091857910156

Epoch  4
Training loss:  0.09456816614270211
Training MSE:   0.09456816614270211
Training MAE:   0.21381022644042968
Testing loss:   0.07536459482312202
Testing MSE:    0.07536459482312202
Testing MAE:    0.1900788619995117

Epoch  5
Training loss:  0.08311984218060971
Training MSE:   0.08311984218060971
Training MAE:   0.2055534149169922
Testing loss:   0.0623566371768713
Testing MSE:    0.0623566371768713
Testing MAE:    0.17549451141357422

Epoch  6
Training loss:  0.08649334124177695
Training MSE:   0.08649334124177695
Training MAE:   0.21691456680297852
Testing loss:   0.1578018409013748
Testing MSE:    0.1578018409013748
Testing MAE:    0.3414798110961914

Epoch  7
Training loss:  0.10061237465888262
Training MSE:   0.10061237465888262
Training MAE:   0.23963321266174317
Testing loss:   0.16434008092880248
Testing MSE:    0.16434008092880248
Testing MAE:    0.3485239822387695

Epoch  8
Training loss:  0.10964846258834005
Training MSE:   0.10964846258834005
Training MAE:   0.2535032623291016
Testing loss:   0.04852569313645363
Testing MSE:    0.04852569313645363
Testing MAE:    0.15450010070800782

Epoch  9
Training loss:  0.10677258260920644
Training MSE:   0.10677258260920644
Training MAE:   0.24916557998657227
Testing loss:   0.04994540864229202
Testing MSE:    0.04994540864229202
Testing MAE:    0.16605408782958983

Training loss:  0.050154715682566166
Training MSE:   0.050154715682566166
Training MAE:   0.16523206634521484

Testing loss:  0.04994540864229202
Testing MSE:   0.04994540864229202
Testing MAE:   0.16605408782958983

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 10, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  5015.405884182739
Training MSE:   5015.405884182739
Training MAE:   23.634158616638185
Testing loss:   42.55313790893555
Testing MSE:    42.55313790893555
Testing MAE:    5.0574155609130855

Epoch  1
Training loss:  31.721733917999266
Training MSE:   31.721733917999266
Training MAE:   4.348932046127319
Testing loss:   20.341460015869142
Testing MSE:    20.341460015869142
Testing MAE:    3.512545962524414

Epoch  2
Training loss:  12.372028715133666
Training MSE:   12.372028715133666
Training MAE:   2.6698671310424804
Testing loss:   5.535508148956299
Testing MSE:    5.535508148956299
Testing MAE:    1.7995950073242188

Epoch  3
Training loss:  2.581535713887215
Training MSE:   2.581535713887215
Training MAE:   1.1640346160888673
Testing loss:   1.0701615224838257
Testing MSE:    1.0701615224838257
Testing MAE:    0.8229324081420898

Epoch  4
Training loss:  0.41702708203196526
Training MSE:   0.41702708203196526
Training MAE:   0.4649590320587158
Testing loss:   0.18877993552684785
Testing MSE:    0.18877993552684785
Testing MAE:    0.3178677322387695

Epoch  5
Training loss:  0.15024826226383448
Training MSE:   0.15024826226383448
Training MAE:   0.2702543598175049
Testing loss:   0.19526191794872283
Testing MSE:    0.19526191794872283
Testing MAE:    0.36207857208251953

Epoch  6
Training loss:  0.1139578037261963
Training MSE:   0.1139578037261963
Training MAE:   0.23490198974609375
Testing loss:   0.1180552764415741
Testing MSE:    0.1180552764415741
Testing MAE:    0.24466197662353514

Epoch  7
Training loss:  0.12136029615998269
Training MSE:   0.12136029615998269
Training MAE:   0.2529616523742676
Testing loss:   0.07802846779823303
Testing MSE:    0.07802846779823303
Testing MAE:    0.1927553497314453

Epoch  8
Training loss:  0.1254054173618555
Training MSE:   0.1254054173618555
Training MAE:   0.26468087692260744
Testing loss:   0.11777352861166
Testing MSE:    0.11777352861166
Testing MAE:    0.26075113983154297

Epoch  9
Training loss:  0.12327739529311657
Training MSE:   0.12327739529311657
Training MAE:   0.26568273849487306
Testing loss:   0.13213350664377213
Testing MSE:    0.13213350664377213
Testing MAE:    0.29196107330322263

Training loss:  0.13334555996358394
Training MSE:   0.13334555996358394
Training MAE:   0.29134716987609866

Testing loss:  0.13213350664377213
Testing MSE:   0.13213350664377213
Testing MAE:   0.29196107330322263

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 5, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  11235.042049051666
Training MSE:   11235.042049051666
Training MAE:   38.024997375106814
Testing loss:   7.846123970031738
Testing MSE:    7.846123970031738
Testing MAE:    2.227399774169922

Epoch  1
Training loss:  4.881784630680084
Training MSE:   4.881784630680084
Training MAE:   1.7389064743041993
Testing loss:   2.7987057525634764
Testing MSE:    2.7987057525634764
Testing MAE:    1.3326651824951172

Epoch  2
Training loss:  1.7597806635856628
Training MSE:   1.7597806635856628
Training MAE:   1.0313495796203613
Testing loss:   1.019314347743988
Testing MSE:    1.019314347743988
Testing MAE:    0.8037435455322266

Epoch  3
Training loss:  0.5135264236032963
Training MSE:   0.5135264236032963
Training MAE:   0.5386719207763672
Testing loss:   0.25834048006534577
Testing MSE:    0.25834048006534577
Testing MAE:    0.37866593780517577

Epoch  4
Training loss:  0.1911625183582306
Training MSE:   0.1911625183582306
Training MAE:   0.3116675220489502
Testing loss:   0.11304424855709076
Testing MSE:    0.11304424855709076
Testing MAE:    0.2242220748901367

Epoch  5
Training loss:  0.12038760380893945
Training MSE:   0.12038760380893945
Training MAE:   0.24308103637695314
Testing loss:   0.10326738225221634
Testing MSE:    0.10326738225221634
Testing MAE:    0.22135049591064454

Epoch  6
Training loss:  0.11944906864315272
Training MSE:   0.11944906864315272
Training MAE:   0.2514744632720947
Testing loss:   0.06543754148483276
Testing MSE:    0.06543754148483276
Testing MAE:    0.17305198974609376

Epoch  7
Training loss:  0.12736482216417788
Training MSE:   0.12736482216417788
Training MAE:   0.26874448738098144
Testing loss:   0.11671358091831208
Testing MSE:    0.11671358091831208
Testing MAE:    0.2774917465209961

Epoch  8
Training loss:  0.12626525975540281
Training MSE:   0.12626525975540281
Training MAE:   0.2700224708557129
Testing loss:   0.40759363703727725
Testing MSE:    0.40759363703727725
Testing MAE:    0.5990934921264648

Epoch  9
Training loss:  0.13504139425382017
Training MSE:   0.13504139425382017
Training MAE:   0.2804536273956299
Testing loss:   0.0807490794301033
Testing MSE:    0.0807490794301033
Testing MAE:    0.2125044006347656

Training loss:  0.08184816671907902
Training MSE:   0.08184816671907902
Training MAE:   0.21228665466308594

Testing loss:  0.0807490794301033
Testing MSE:   0.0807490794301033
Testing MAE:   0.2125044006347656

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 5, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  8928.715780777908
Training MSE:   8928.715780777908
Training MAE:   29.72029792098999
Testing loss:   0.9458776056289673
Testing MSE:    0.9458776056289673
Testing MAE:    0.7469724624633789

Epoch  1
Training loss:  0.4147659999787807
Training MSE:   0.4147659999787807
Training MAE:   0.4670881683349609
Testing loss:   0.16830138108730316
Testing MSE:    0.16830138108730316
Testing MAE:    0.2909443756103516

Epoch  2
Training loss:  0.12070867635905742
Training MSE:   0.12070867635905742
Training MAE:   0.235870654296875
Testing loss:   0.09046139607429504
Testing MSE:    0.09046139607429504
Testing MAE:    0.21429278259277343

Epoch  3
Training loss:  0.07724778420850635
Training MSE:   0.07724778420850635
Training MAE:   0.18793774375915528
Testing loss:   0.06889591940045357
Testing MSE:    0.06889591940045357
Testing MAE:    0.17755363311767577

Epoch  4
Training loss:  0.07117216172590851
Training MSE:   0.07117216172590851
Training MAE:   0.18816276473999025
Testing loss:   0.07523128591775895
Testing MSE:    0.07523128591775895
Testing MAE:    0.21151090087890626

Epoch  5
Training loss:  0.0682743159405887
Training MSE:   0.0682743159405887
Training MAE:   0.18867106819152832
Testing loss:   0.07414159079790116
Testing MSE:    0.07414159079790116
Testing MAE:    0.213210009765625

Epoch  6
Training loss:  0.07946012387871743
Training MSE:   0.07946012387871743
Training MAE:   0.21041009025573731
Testing loss:   0.05461778367161751
Testing MSE:    0.05461778367161751
Testing MAE:    0.16613835906982422

Epoch  7
Training loss:  0.08624474190026522
Training MSE:   0.08624474190026522
Training MAE:   0.22256615371704103
Testing loss:   0.05644557668566704
Testing MSE:    0.05644557668566704
Testing MAE:    0.1709956787109375

Epoch  8
Training loss:  0.09807292194664478
Training MSE:   0.09807292194664478
Training MAE:   0.2372413375854492
Testing loss:   0.04869678272604942
Testing MSE:    0.04869678272604942
Testing MAE:    0.1649336212158203

Epoch  9
Training loss:  0.10588533378615976
Training MSE:   0.10588533378615976
Training MAE:   0.2467167407989502
Testing loss:   0.05708850280642509
Testing MSE:    0.05708850280642509
Testing MAE:    0.17846221160888673

Training loss:  0.05838605290651321
Training MSE:   0.05838605290651321
Training MAE:   0.1785784397125244

Testing loss:  0.05708850280642509
Testing MSE:   0.05708850280642509
Testing MAE:   0.17846221160888673

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 5, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7571.2652924530985
Training MSE:   7571.2652924530985
Training MAE:   25.538345095825196
Testing loss:   4.457216466140747
Testing MSE:    4.457216466140747
Testing MAE:    1.49443221282959

Epoch  1
Training loss:  2.445535624027252
Training MSE:   2.445535624027252
Training MAE:   1.0966665424346924
Testing loss:   1.3943273468017578
Testing MSE:    1.3943273468017578
Testing MAE:    0.8510032775878906

Epoch  2
Training loss:  1.0959851313352584
Training MSE:   1.0959851313352584
Training MAE:   0.7651355522155762
Testing loss:   0.7376511496067047
Testing MSE:    0.7376511496067047
Testing MAE:    0.628258544921875

Epoch  3
Training loss:  0.5824863778233528
Training MSE:   0.5824863778233528
Training MAE:   0.5618980613708496
Testing loss:   0.43852383031845094
Testing MSE:    0.43852383031845094
Testing MAE:    0.5077080093383789

Epoch  4
Training loss:  0.28979699462354186
Training MSE:   0.28979699462354186
Training MAE:   0.387081929397583
Testing loss:   0.1936837321996689
Testing MSE:    0.1936837321996689
Testing MAE:    0.2966009475708008

Epoch  5
Training loss:  0.16302427948862314
Training MSE:   0.16302427948862314
Training MAE:   0.28369204254150393
Testing loss:   0.10039596217274666
Testing MSE:    0.10039596217274666
Testing MAE:    0.2023572998046875

Epoch  6
Training loss:  0.12350043610781432
Training MSE:   0.12350043610781432
Training MAE:   0.2512145378112793
Testing loss:   0.3535161355495453
Testing MSE:    0.3535161355495453
Testing MAE:    0.5360720581054688

Epoch  7
Training loss:  0.14154714266657828
Training MSE:   0.14154714266657828
Training MAE:   0.2825427108764648
Testing loss:   0.08089798713922501
Testing MSE:    0.08089798713922501
Testing MAE:    0.21008064880371094

Epoch  8
Training loss:  0.1568729129821062
Training MSE:   0.1568729129821062
Training MAE:   0.30473651313781736
Testing loss:   0.08247687406539916
Testing MSE:    0.08247687406539916
Testing MAE:    0.22164085540771483

Epoch  9
Training loss:  0.1456196490764618
Training MSE:   0.1456196490764618
Training MAE:   0.2924030124664307
Testing loss:   0.05259467869400978
Testing MSE:    0.05259467869400978
Testing MAE:    0.16032664642333985

Training loss:  0.05165061863064766
Training MSE:   0.05165061863064766
Training MAE:   0.1592448699951172

Testing loss:  0.05259467869400978
Testing MSE:   0.05259467869400978
Testing MAE:   0.16032664642333985

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 5, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9741.269547312164
Training MSE:   9741.269547312164
Training MAE:   33.85237158660889
Testing loss:   14.2976126663208
Testing MSE:    14.2976126663208
Testing MAE:    2.9236236434936522

Epoch  1
Training loss:  2.8537153618454933
Training MSE:   2.8537153618454933
Training MAE:   1.0798253810882568
Testing loss:   0.3064732085943222
Testing MSE:    0.3064732085943222
Testing MAE:    0.3832936599731445

Epoch  2
Training loss:  0.1909687982842326
Training MSE:   0.1909687982842326
Training MAE:   0.27410439147949217
Testing loss:   0.11695886098742485
Testing MSE:    0.11695886098742485
Testing MAE:    0.21173307342529296

Epoch  3
Training loss:  0.10859061302021146
Training MSE:   0.10859061302021146
Training MAE:   0.20593238677978515
Testing loss:   0.0788672967672348
Testing MSE:    0.0788672967672348
Testing MAE:    0.1769460205078125

Epoch  4
Training loss:  0.09006157649308442
Training MSE:   0.09006157649308442
Training MAE:   0.20216713638305664
Testing loss:   0.08967914545536042
Testing MSE:    0.08967914545536042
Testing MAE:    0.22363715057373046

Epoch  5
Training loss:  0.08467500910311937
Training MSE:   0.08467500910311937
Training MAE:   0.20449798393249513
Testing loss:   0.08277048418521882
Testing MSE:    0.08277048418521882
Testing MAE:    0.20576031341552734

Epoch  6
Training loss:  0.08539420760422944
Training MSE:   0.08539420760422944
Training MAE:   0.2133676742553711
Testing loss:   0.13010039494037628
Testing MSE:    0.13010039494037628
Testing MAE:    0.28878819732666017

Epoch  7
Training loss:  0.10568589999079704
Training MSE:   0.10568589999079704
Training MAE:   0.24690153198242187
Testing loss:   0.5255161374092102
Testing MSE:    0.5255161374092102
Testing MAE:    0.6912088439941406

Epoch  8
Training loss:  0.12820317978560924
Training MSE:   0.12820317978560924
Training MAE:   0.27593957328796387
Testing loss:   0.09640640242099761
Testing MSE:    0.09640640242099761
Testing MAE:    0.24527388763427735

Epoch  9
Training loss:  0.11896410967558622
Training MSE:   0.11896410967558622
Training MAE:   0.26130796012878416
Testing loss:   0.0977037140250206
Testing MSE:    0.0977037140250206
Testing MAE:    0.2610785675048828

Training loss:  0.09741458161175251
Training MSE:   0.09741458161175251
Training MAE:   0.26133857498168944

Testing loss:  0.0977037140250206
Testing MSE:   0.0977037140250206
Testing MAE:   0.2610785675048828

Number of layers:  3
Number of units in layer [1,2,3]:  [25, 5, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9609.588278591036
Training MSE:   9609.588278591036
Training MAE:   34.02945502624512
Testing loss:   0.8344588357925415
Testing MSE:    0.8344588357925415
Testing MAE:    0.691262483215332

Epoch  1
Training loss:  0.2838132743149996
Training MSE:   0.2838132743149996
Training MAE:   0.37436691169738767
Testing loss:   0.12734426902532578
Testing MSE:    0.12734426902532578
Testing MAE:    0.22813219451904296

Epoch  2
Training loss:  0.09361487607061864
Training MSE:   0.09361487607061864
Training MAE:   0.20526940269470215
Testing loss:   0.0850995690047741
Testing MSE:    0.0850995690047741
Testing MAE:    0.19918436737060546

Epoch  3
Training loss:  0.075871641869843
Training MSE:   0.075871641869843
Training MAE:   0.19119584465026856
Testing loss:   0.06811191315054893
Testing MSE:    0.06811191315054893
Testing MAE:    0.176972412109375

Epoch  4
Training loss:  0.06862039086967707
Training MSE:   0.06862039086967707
Training MAE:   0.1851062957763672
Testing loss:   0.06791639954447747
Testing MSE:    0.06791639954447747
Testing MAE:    0.19058526763916014

Epoch  5
Training loss:  0.06577518577873707
Training MSE:   0.06577518577873707
Training MAE:   0.1845020637512207
Testing loss:   0.06738276139497756
Testing MSE:    0.06738276139497756
Testing MAE:    0.18180761260986328

Epoch  6
Training loss:  0.06420676048398018
Training MSE:   0.06420676048398018
Training MAE:   0.1835490837097168
Testing loss:   0.051722874811291696
Testing MSE:    0.051722874811291696
Testing MAE:    0.15712304534912108

Epoch  7
Training loss:  0.0696122516900301
Training MSE:   0.0696122516900301
Training MAE:   0.19389742469787596
Testing loss:   0.08027256893515587
Testing MSE:    0.08027256893515587
Testing MAE:    0.21536000061035157

Epoch  8
Training loss:  0.07815413137078285
Training MSE:   0.07815413137078285
Training MAE:   0.20930903549194335
Testing loss:   0.06230700553655624
Testing MSE:    0.06230700553655624
Testing MAE:    0.1928603988647461

Epoch  9
Training loss:  0.07936911832541227
Training MSE:   0.07936911832541227
Training MAE:   0.2113795093536377
Testing loss:   0.05527086183428764
Testing MSE:    0.05527086183428764
Testing MAE:    0.1714432647705078

Training loss:  0.055214440394937996
Training MSE:   0.055214440394937996
Training MAE:   0.17086747589111329

Testing loss:  0.05527086183428764
Testing MSE:   0.05527086183428764
Testing MAE:   0.1714432647705078

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 25, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7357.031421723938
Training MSE:   7357.031421723938
Training MAE:   27.731272254180908
Testing loss:   31.818852160644532
Testing MSE:    31.818852160644532
Testing MAE:    4.358567083740234

Epoch  1
Training loss:  13.254225214290619
Training MSE:   13.254225214290619
Training MAE:   2.6341495723724364
Testing loss:   3.142227696609497
Testing MSE:    3.142227696609497
Testing MAE:    1.346205125427246

Epoch  2
Training loss:  1.6190745997190474
Training MSE:   1.6190745997190474
Training MAE:   0.9352668128967285
Testing loss:   0.7145630373001098
Testing MSE:    0.7145630373001098
Testing MAE:    0.6566276214599609

Epoch  3
Training loss:  0.37630901894271374
Training MSE:   0.37630901894271374
Training MAE:   0.4385225116729736
Testing loss:   0.19867277550697326
Testing MSE:    0.19867277550697326
Testing MAE:    0.2966234680175781

Epoch  4
Training loss:  0.15946868168115616
Training MSE:   0.15946868168115616
Training MAE:   0.27059002380371094
Testing loss:   0.11141989357471466
Testing MSE:    0.11141989357471466
Testing MAE:    0.21221333770751954

Epoch  5
Training loss:  0.12331667027175426
Training MSE:   0.12331667027175426
Training MAE:   0.24699961013793945
Testing loss:   0.0776833929002285
Testing MSE:    0.0776833929002285
Testing MAE:    0.18205855712890626

Epoch  6
Training loss:  0.1333041072279215
Training MSE:   0.1333041072279215
Training MAE:   0.2712638401031494
Testing loss:   0.14541300694942474
Testing MSE:    0.14541300694942474
Testing MAE:    0.3211854888916016

Epoch  7
Training loss:  0.17105230223983525
Training MSE:   0.17105230223983525
Training MAE:   0.3182929416656494
Testing loss:   0.19762463691234589
Testing MSE:    0.19762463691234589
Testing MAE:    0.3884850051879883

Epoch  8
Training loss:  0.14809855190366505
Training MSE:   0.14809855190366505
Training MAE:   0.29750548667907717
Testing loss:   0.09225613189935684
Testing MSE:    0.09225613189935684
Testing MAE:    0.25035365295410156

Epoch  9
Training loss:  0.14986438774317504
Training MSE:   0.14986438774317504
Training MAE:   0.30397707633972165
Testing loss:   0.2371504653453827
Testing MSE:    0.2371504653453827
Testing MAE:    0.44953292694091795

Training loss:  0.23845407729148865
Training MSE:   0.23845407729148865
Training MAE:   0.45092222023010253

Testing loss:  0.2371504653453827
Testing MSE:   0.2371504653453827
Testing MAE:   0.44953292694091795

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 25, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7182.819684120941
Training MSE:   7182.819684120941
Training MAE:   26.899987295913697
Testing loss:   38.52526103820801
Testing MSE:    38.52526103820801
Testing MAE:    4.491594242858887

Epoch  1
Training loss:  21.26674061126709
Training MSE:   21.26674061126709
Training MAE:   3.2770101249694825
Testing loss:   8.375319961547852
Testing MSE:    8.375319961547852
Testing MAE:    2.123659971618652

Epoch  2
Training loss:  3.9492750045776366
Training MSE:   3.9492750045776366
Training MAE:   1.4219168472290038
Testing loss:   1.4982508354187012
Testing MSE:    1.4982508354187012
Testing MAE:    0.9346721252441407

Epoch  3
Training loss:  0.9626723730921746
Training MSE:   0.9626723730921746
Training MAE:   0.7370109420776367
Testing loss:   0.5547947326183319
Testing MSE:    0.5547947326183319
Testing MAE:    0.5655194152832032

Epoch  4
Training loss:  0.4061975359022617
Training MSE:   0.4061975359022617
Training MAE:   0.4769941436767578
Testing loss:   0.26657361459732054
Testing MSE:    0.26657361459732054
Testing MAE:    0.40646190948486327

Epoch  5
Training loss:  0.19270490246713162
Training MSE:   0.19270490246713162
Training MAE:   0.32003956451416016
Testing loss:   0.10622624518871307
Testing MSE:    0.10622624518871307
Testing MAE:    0.22358816986083985

Epoch  6
Training loss:  0.1527550320714712
Training MSE:   0.1527550320714712
Training MAE:   0.28992938270568847
Testing loss:   0.1005195842385292
Testing MSE:    0.1005195842385292
Testing MAE:    0.24165377502441407

Epoch  7
Training loss:  0.15572244801074267
Training MSE:   0.15572244801074267
Training MAE:   0.2987724754333496
Testing loss:   0.332589289188385
Testing MSE:    0.332589289188385
Testing MAE:    0.5244804321289063

Epoch  8
Training loss:  0.1813977296024561
Training MSE:   0.1813977296024561
Training MAE:   0.33043608322143553
Testing loss:   0.07747403600811958
Testing MSE:    0.07747403600811958
Testing MAE:    0.20201785125732422

Epoch  9
Training loss:  0.16905005950927735
Training MSE:   0.16905005950927735
Training MAE:   0.3183504940032959
Testing loss:   0.09148079999685288
Testing MSE:    0.09148079999685288
Testing MAE:    0.23825992126464843

Training loss:  0.09201700613200664
Training MSE:   0.09201700613200664
Training MAE:   0.23855673904418945

Testing loss:  0.09148079999685288
Testing MSE:   0.09148079999685288
Testing MAE:   0.23825992126464843

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 25, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10786.169073223877
Training MSE:   10786.169073223877
Training MAE:   36.43423690032959
Testing loss:   36.95653218841553
Testing MSE:    36.95653218841553
Testing MAE:    4.803169453430176

Epoch  1
Training loss:  13.507379082345963
Training MSE:   13.507379082345963
Training MAE:   2.393958123397827
Testing loss:   0.5014602335929871
Testing MSE:    0.5014602335929871
Testing MAE:    0.5477137680053711

Epoch  2
Training loss:  0.17158521834164858
Training MSE:   0.17158521834164858
Training MAE:   0.28656339836120603
Testing loss:   0.136905659198761
Testing MSE:    0.136905659198761
Testing MAE:    0.26473748016357423

Epoch  3
Training loss:  0.08619259318560361
Training MSE:   0.08619259318560361
Training MAE:   0.19637794456481933
Testing loss:   0.07711560307741165
Testing MSE:    0.07711560307741165
Testing MAE:    0.1874773712158203

Epoch  4
Training loss:  0.07892822530269623
Training MSE:   0.07892822530269623
Training MAE:   0.1953460247039795
Testing loss:   0.06618024205565452
Testing MSE:    0.06618024205565452
Testing MAE:    0.16294995422363281

Epoch  5
Training loss:  0.07664411255642772
Training MSE:   0.07664411255642772
Training MAE:   0.19737565422058106
Testing loss:   0.06859310318827629
Testing MSE:    0.06859310318827629
Testing MAE:    0.19087541656494142

Epoch  6
Training loss:  0.08645239184647799
Training MSE:   0.08645239184647799
Training MAE:   0.21775635871887208
Testing loss:   0.08103179401159287
Testing MSE:    0.08103179401159287
Testing MAE:    0.22134335174560546

Epoch  7
Training loss:  0.11962717552781105
Training MSE:   0.11962717552781105
Training MAE:   0.26284481048583985
Testing loss:   0.04728788626492023
Testing MSE:    0.04728788626492023
Testing MAE:    0.1431726104736328

Epoch  8
Training loss:  0.11453619938716292
Training MSE:   0.11453619938716292
Training MAE:   0.25762973976135256
Testing loss:   0.04697502445876598
Testing MSE:    0.04697502445876598
Testing MAE:    0.15069529724121095

Epoch  9
Training loss:  0.1282024177543819
Training MSE:   0.1282024177543819
Training MAE:   0.27139055557250974
Testing loss:   0.09938263119459152
Testing MSE:    0.09938263119459152
Testing MAE:    0.25673182373046877

Training loss:  0.09898602701425552
Training MSE:   0.09898602701425552
Training MAE:   0.2562745658874512

Testing loss:  0.09938263119459152
Testing MSE:   0.09938263119459152
Testing MAE:   0.25673182373046877

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 25, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  11467.170032546424
Training MSE:   11467.170032546424
Training MAE:   36.64018520050049
Testing loss:   4.857225568771362
Testing MSE:    4.857225568771362
Testing MAE:    1.735152133178711

Epoch  1
Training loss:  1.3090878202319145
Training MSE:   1.3090878202319145
Training MAE:   0.8058883396148682
Testing loss:   0.22359377262592317
Testing MSE:    0.22359377262592317
Testing MAE:    0.33203975524902346

Epoch  2
Training loss:  0.14171946247071027
Training MSE:   0.14171946247071027
Training MAE:   0.24725313987731934
Testing loss:   0.17425366978645324
Testing MSE:    0.17425366978645324
Testing MAE:    0.31875142822265623

Epoch  3
Training loss:  0.10146445760577917
Training MSE:   0.10146445760577917
Training MAE:   0.20584061965942382
Testing loss:   0.10095352492332459
Testing MSE:    0.10095352492332459
Testing MAE:    0.20061485900878906

Epoch  4
Training loss:  0.08878530700355768
Training MSE:   0.08878530700355768
Training MAE:   0.19970833625793458
Testing loss:   0.13522170898914337
Testing MSE:    0.13522170898914337
Testing MAE:    0.2880367630004883

Epoch  5
Training loss:  0.08744969056770205
Training MSE:   0.08744969056770205
Training MAE:   0.20554556312561034
Testing loss:   0.13309702513217925
Testing MSE:    0.13309702513217925
Testing MAE:    0.29322568969726565

Epoch  6
Training loss:  0.09357227208167314
Training MSE:   0.09357227208167314
Training MAE:   0.22195173797607423
Testing loss:   0.06372145859599114
Testing MSE:    0.06372145859599114
Testing MAE:    0.16975646209716796

Epoch  7
Training loss:  0.1030431030318141
Training MSE:   0.1030431030318141
Training MAE:   0.23936958045959472
Testing loss:   0.16618251717090607
Testing MSE:    0.16618251717090607
Testing MAE:    0.3510085861206055

Epoch  8
Training loss:  0.12283982884585858
Training MSE:   0.12283982884585858
Training MAE:   0.2660720966339111
Testing loss:   0.15195858407020568
Testing MSE:    0.15195858407020568
Testing MAE:    0.33300028228759765

Epoch  9
Training loss:  0.100857844581455
Training MSE:   0.100857844581455
Training MAE:   0.24000137557983398
Testing loss:   0.06657417076230049
Testing MSE:    0.06657417076230049
Testing MAE:    0.1942037628173828

Training loss:  0.06429943817704915
Training MSE:   0.06429943817704915
Training MAE:   0.1929038459777832

Testing loss:  0.06657417076230049
Testing MSE:   0.06657417076230049
Testing MAE:   0.1942037628173828

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 25, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  5251.217270794296
Training MSE:   5251.217270794296
Training MAE:   22.41981012802124
Testing loss:   9.84453893737793
Testing MSE:    9.84453893737793
Testing MAE:    2.3865047256469727

Epoch  1
Training loss:  2.0939961582779882
Training MSE:   2.0939961582779882
Training MAE:   0.9542370147705078
Testing loss:   0.4451787708044052
Testing MSE:    0.4451787708044052
Testing MAE:    0.4714977630615234

Epoch  2
Training loss:  0.2677266189783812
Training MSE:   0.2677266189783812
Training MAE:   0.34847603225708007
Testing loss:   0.1588126299738884
Testing MSE:    0.1588126299738884
Testing MAE:    0.25504888763427735

Epoch  3
Training loss:  0.12879992145448924
Training MSE:   0.12879992145448924
Training MAE:   0.2335756130218506
Testing loss:   0.10034484427571297
Testing MSE:    0.10034484427571297
Testing MAE:    0.19766999206542968

Epoch  4
Training loss:  0.10940326599925757
Training MSE:   0.10940326599925757
Training MAE:   0.2293903835296631
Testing loss:   0.08456314361691475
Testing MSE:    0.08456314361691475
Testing MAE:    0.18644842834472655

Epoch  5
Training loss:  0.12886217832416297
Training MSE:   0.12886217832416297
Training MAE:   0.2615524677276611
Testing loss:   0.09316874933838844
Testing MSE:    0.09316874933838844
Testing MAE:    0.21488108825683594

Epoch  6
Training loss:  0.13395690492838622
Training MSE:   0.13395690492838622
Training MAE:   0.27431701164245603
Testing loss:   0.1540507522344589
Testing MSE:    0.1540507522344589
Testing MAE:    0.32897041625976564

Epoch  7
Training loss:  0.15138342984318734
Training MSE:   0.15138342984318734
Training MAE:   0.2952099826812744
Testing loss:   0.5174407576560974
Testing MSE:    0.5174407576560974
Testing MAE:    0.6842070663452149

Epoch  8
Training loss:  0.1579020425185561
Training MSE:   0.1579020425185561
Training MAE:   0.3082636116027832
Testing loss:   0.06945892695188523
Testing MSE:    0.06945892695188523
Testing MAE:    0.19899230651855468

Epoch  9
Training loss:  0.15895284712314606
Training MSE:   0.15895284712314606
Training MAE:   0.3094425151824951
Testing loss:   0.18952944304943084
Testing MSE:    0.18952944304943084
Testing MAE:    0.3880431396484375

Training loss:  0.18731431777477264
Training MSE:   0.18731431777477264
Training MAE:   0.38929967384338376

Testing loss:  0.18952944304943084
Testing MSE:   0.18952944304943084
Testing MAE:   0.3880431396484375

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 20, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9260.441500813698
Training MSE:   9260.441500813698
Training MAE:   29.642196433258057
Testing loss:   0.7110580541610718
Testing MSE:    0.7110580541610718
Testing MAE:    0.6400280364990234

Epoch  1
Training loss:  0.3327116385817528
Training MSE:   0.3327116385817528
Training MAE:   0.40214526481628415
Testing loss:   0.18234233951568604
Testing MSE:    0.18234233951568604
Testing MAE:    0.2806318634033203

Epoch  2
Training loss:  0.14073166516423224
Training MSE:   0.14073166516423224
Training MAE:   0.24664325981140137
Testing loss:   0.1397314829826355
Testing MSE:    0.1397314829826355
Testing MAE:    0.29532457885742186

Epoch  3
Training loss:  0.10047431409657001
Training MSE:   0.10047431409657001
Training MAE:   0.2153103698730469
Testing loss:   0.07814876803159714
Testing MSE:    0.07814876803159714
Testing MAE:    0.18999442749023437

Epoch  4
Training loss:  0.08582456338703633
Training MSE:   0.08582456338703633
Training MAE:   0.20725656585693358
Testing loss:   0.09565108754634857
Testing MSE:    0.09565108754634857
Testing MAE:    0.22906716003417968

Epoch  5
Training loss:  0.08352248992919922
Training MSE:   0.08352248992919922
Training MAE:   0.20984346618652344
Testing loss:   0.0569877903521061
Testing MSE:    0.0569877903521061
Testing MAE:    0.16911658172607422

Epoch  6
Training loss:  0.09106239639818668
Training MSE:   0.09106239639818668
Training MAE:   0.22374349212646485
Testing loss:   0.20533832430839538
Testing MSE:    0.20533832430839538
Testing MAE:    0.40292583618164063

Epoch  7
Training loss:  0.10603729467540979
Training MSE:   0.10603729467540979
Training MAE:   0.24672423400878907
Testing loss:   0.05320406920313835
Testing MSE:    0.05320406920313835
Testing MAE:    0.17051280212402345

Epoch  8
Training loss:  0.11783266433253885
Training MSE:   0.11783266433253885
Training MAE:   0.25973379783630374
Testing loss:   0.10535953294038773
Testing MSE:    0.10535953294038773
Testing MAE:    0.263057585144043

Epoch  9
Training loss:  0.13002046502381565
Training MSE:   0.13002046502381565
Training MAE:   0.27786386680603026
Testing loss:   0.04817082816362381
Testing MSE:    0.04817082816362381
Testing MAE:    0.16229053649902345

Training loss:  0.04857189148515463
Training MSE:   0.04857189148515463
Training MAE:   0.1612515110015869

Testing loss:  0.04817082816362381
Testing MSE:   0.04817082816362381
Testing MAE:   0.16229053649902345

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 20, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9829.192522826004
Training MSE:   9829.192522826004
Training MAE:   32.25860488548279
Testing loss:   20.648604763793944
Testing MSE:    20.648604763793944
Testing MAE:    3.5426386169433592

Epoch  1
Training loss:  6.861693887424469
Training MSE:   6.861693887424469
Training MAE:   1.9162851524353028
Testing loss:   1.1461135238647462
Testing MSE:    1.1461135238647462
Testing MAE:    0.8641587509155273

Epoch  2
Training loss:  0.5016994140714407
Training MSE:   0.5016994140714407
Training MAE:   0.528529581451416
Testing loss:   0.3090432351589203
Testing MSE:    0.3090432351589203
Testing MAE:    0.44229897613525393

Epoch  3
Training loss:  0.2801038887068629
Training MSE:   0.2801038887068629
Training MAE:   0.3939403347015381
Testing loss:   0.1700062263250351
Testing MSE:    0.1700062263250351
Testing MAE:    0.30952537078857423

Epoch  4
Training loss:  0.26143700314462187
Training MSE:   0.26143700314462187
Training MAE:   0.3876879081726074
Testing loss:   0.14337824560403825
Testing MSE:    0.14337824560403825
Testing MAE:    0.2695063690185547

Epoch  5
Training loss:  0.2723389220520854
Training MSE:   0.2723389220520854
Training MAE:   0.39211921463012694
Testing loss:   0.1209164843082428
Testing MSE:    0.1209164843082428
Testing MAE:    0.26403273620605466

Epoch  6
Training loss:  0.2898324852824211
Training MSE:   0.2898324852824211
Training MAE:   0.420989351272583
Testing loss:   0.5265210318565369
Testing MSE:    0.5265210318565369
Testing MAE:    0.6787893814086914

Epoch  7
Training loss:  0.2786803585499525
Training MSE:   0.2786803585499525
Training MAE:   0.4095626644134521
Testing loss:   0.4958148265838623
Testing MSE:    0.4958148265838623
Testing MAE:    0.6602310470581054

Epoch  8
Training loss:  0.29033857726603746
Training MSE:   0.29033857726603746
Training MAE:   0.42057783241271973
Testing loss:   0.15956719698905944
Testing MSE:    0.15956719698905944
Testing MAE:    0.32208276977539063

Epoch  9
Training loss:  0.24184199228137732
Training MSE:   0.24184199228137732
Training MAE:   0.37605109329223635
Testing loss:   0.267829523396492
Testing MSE:    0.267829523396492
Testing MAE:    0.4643102600097656

Training loss:  0.2684318377733231
Training MSE:   0.2684318377733231
Training MAE:   0.4643394344329834

Testing loss:  0.267829523396492
Testing MSE:   0.267829523396492
Testing MAE:   0.4643102600097656

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 20, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  4073.791211888504
Training MSE:   4073.791211888504
Training MAE:   17.50503524131775
Testing loss:   11.203376614379883
Testing MSE:    11.203376614379883
Testing MAE:    2.4171623931884767

Epoch  1
Training loss:  3.3505336895227433
Training MSE:   3.3505336895227433
Training MAE:   1.2817002716064454
Testing loss:   0.6138238164424896
Testing MSE:    0.6138238164424896
Testing MAE:    0.6055345657348633

Epoch  2
Training loss:  0.29179590299725533
Training MSE:   0.29179590299725533
Training MAE:   0.3839113254547119
Testing loss:   0.16838453642129897
Testing MSE:    0.16838453642129897
Testing MAE:    0.2858734390258789

Epoch  3
Training loss:  0.11137401377260685
Training MSE:   0.11137401377260685
Training MAE:   0.22307779579162598
Testing loss:   0.08834910670518875
Testing MSE:    0.08834910670518875
Testing MAE:    0.1924826141357422

Epoch  4
Training loss:  0.09149460973590612
Training MSE:   0.09149460973590612
Training MAE:   0.20704709968566895
Testing loss:   0.08808832235336304
Testing MSE:    0.08808832235336304
Testing MAE:    0.21508531188964844

Epoch  5
Training loss:  0.09855405748337508
Training MSE:   0.09855405748337508
Training MAE:   0.2264170211791992
Testing loss:   0.06958192685246467
Testing MSE:    0.06958192685246467
Testing MAE:    0.1739546630859375

Epoch  6
Training loss:  0.1053145128712058
Training MSE:   0.1053145128712058
Training MAE:   0.239806050491333
Testing loss:   0.156927002453804
Testing MSE:    0.156927002453804
Testing MAE:    0.3223112396240234

Epoch  7
Training loss:  0.14714870883375405
Training MSE:   0.14714870883375405
Training MAE:   0.2943493980407715
Testing loss:   0.5219205242156982
Testing MSE:    0.5219205242156982
Testing MAE:    0.6869929809570312

Epoch  8
Training loss:  0.16416768404096366
Training MSE:   0.16416768404096366
Training MAE:   0.31053755950927736
Testing loss:   0.05719405820965767
Testing MSE:    0.05719405820965767
Testing MAE:    0.1657897201538086

Epoch  9
Training loss:  0.15592471088320017
Training MSE:   0.15592471088320017
Training MAE:   0.3023700035095215
Testing loss:   0.16043927128314972
Testing MSE:    0.16043927128314972
Testing MAE:    0.35102707672119143

Training loss:  0.15936743277311324
Training MSE:   0.15936743277311324
Training MAE:   0.3519190700531006

Testing loss:  0.16043927128314972
Testing MSE:   0.16043927128314972
Testing MAE:   0.35102707672119143

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 20, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7891.5572717556
Training MSE:   7891.5572717556
Training MAE:   27.43644315109253
Testing loss:   18.96162085571289
Testing MSE:    18.96162085571289
Testing MAE:    3.1583950485229493

Epoch  1
Training loss:  13.34238441734314
Training MSE:   13.34238441734314
Training MAE:   2.651650996017456
Testing loss:   8.168954024505615
Testing MSE:    8.168954024505615
Testing MAE:    2.1146411361694337

Epoch  2
Training loss:  5.313219631862641
Training MSE:   5.313219631862641
Training MAE:   1.6795379386901856
Testing loss:   2.7537206912994385
Testing MSE:    2.7537206912994385
Testing MAE:    1.2259949508666992

Epoch  3
Training loss:  1.626231348490715
Training MSE:   1.626231348490715
Training MAE:   0.9195014190673828
Testing loss:   0.7370305213928222
Testing MSE:    0.7370305213928222
Testing MAE:    0.6273859588623046

Epoch  4
Training loss:  0.46168110953569413
Training MSE:   0.46168110953569413
Training MAE:   0.4914233341217041
Testing loss:   0.23547685520648956
Testing MSE:    0.23547685520648956
Testing MAE:    0.33851149139404296

Epoch  5
Training loss:  0.1828672840103507
Training MSE:   0.1828672840103507
Training MAE:   0.2940608486175537
Testing loss:   0.1741028395652771
Testing MSE:    0.1741028395652771
Testing MAE:    0.29514905853271484

Epoch  6
Training loss:  0.1204341364338994
Training MSE:   0.1204341364338994
Training MAE:   0.23451493644714355
Testing loss:   0.08674057329893112
Testing MSE:    0.08674057329893112
Testing MAE:    0.1911511947631836

Epoch  7
Training loss:  0.13115296276807786
Training MSE:   0.13115296276807786
Training MAE:   0.2620521556854248
Testing loss:   0.0969022371172905
Testing MSE:    0.0969022371172905
Testing MAE:    0.23061533660888672

Epoch  8
Training loss:  0.13176976609230043
Training MSE:   0.13176976609230043
Training MAE:   0.2698798683166504
Testing loss:   0.08313135973215104
Testing MSE:    0.08313135973215104
Testing MAE:    0.21477680053710937

Epoch  9
Training loss:  0.1398114313438535
Training MSE:   0.1398114313438535
Training MAE:   0.2851641170501709
Testing loss:   0.05824117167592049
Testing MSE:    0.05824117167592049
Testing MAE:    0.1650080337524414

Training loss:  0.05728862875252962
Training MSE:   0.05728862875252962
Training MAE:   0.1632863536834717

Testing loss:  0.05824117167592049
Testing MSE:   0.05824117167592049
Testing MAE:   0.1650080337524414

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 20, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  4032.1631475837708
Training MSE:   4032.1631475837708
Training MAE:   18.00688246498108
Testing loss:   16.038499451446533
Testing MSE:    16.038499451446533
Testing MAE:    3.197994107055664

Epoch  1
Training loss:  11.168670537376403
Training MSE:   11.168670537376403
Training MAE:   2.6354963165283203
Testing loss:   6.991642228698731
Testing MSE:    6.991642228698731
Testing MAE:    2.102380549621582

Epoch  2
Training loss:  4.552914513587952
Training MSE:   4.552914513587952
Training MAE:   1.6621781303405763
Testing loss:   2.27663094367981
Testing MSE:    2.27663094367981
Testing MAE:    1.19508836517334

Epoch  3
Training loss:  1.2678350701451302
Training MSE:   1.2678350701451302
Training MAE:   0.8499294189453125
Testing loss:   0.5189711771011353
Testing MSE:    0.5189711771011353
Testing MAE:    0.5495356231689453

Epoch  4
Training loss:  0.3124338633328676
Training MSE:   0.3124338633328676
Training MAE:   0.39870564880371095
Testing loss:   0.16474909205436705
Testing MSE:    0.16474909205436705
Testing MAE:    0.2728062469482422

Epoch  5
Training loss:  0.13499385911971332
Training MSE:   0.13499385911971332
Training MAE:   0.2502755165100098
Testing loss:   0.11014183987379074
Testing MSE:    0.11014183987379074
Testing MAE:    0.24675550079345704

Epoch  6
Training loss:  0.10841062761247158
Training MSE:   0.10841062761247158
Training MAE:   0.23402866287231444
Testing loss:   0.17633959035873414
Testing MSE:    0.17633959035873414
Testing MAE:    0.36319340057373045

Epoch  7
Training loss:  0.11820089925974607
Training MSE:   0.11820089925974607
Training MAE:   0.2569471839904785
Testing loss:   0.07105196285247803
Testing MSE:    0.07105196285247803
Testing MAE:    0.2020384323120117

Epoch  8
Training loss:  0.12787442894503473
Training MSE:   0.12787442894503473
Training MAE:   0.27251353721618654
Testing loss:   0.07106008780598641
Testing MSE:    0.07106008780598641
Testing MAE:    0.19815221710205078

Epoch  9
Training loss:  0.12157487811148167
Training MSE:   0.12157487811148167
Training MAE:   0.2677526470184326
Testing loss:   0.14255511970520018
Testing MSE:    0.14255511970520018
Testing MAE:    0.33018527069091796

Training loss:  0.14255078929066659
Training MSE:   0.14255078929066659
Training MAE:   0.33110320854187014

Testing loss:  0.14255511970520018
Testing MSE:   0.14255511970520018
Testing MAE:   0.33018527069091796

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 15, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  6985.169446511841
Training MSE:   6985.169446511841
Training MAE:   27.07191385192871
Testing loss:   40.0872580078125
Testing MSE:    40.0872580078125
Testing MAE:    5.071834434509277

Epoch  1
Training loss:  26.634263860702514
Training MSE:   26.634263860702514
Training MAE:   4.10034322013855
Testing loss:   14.799416358947754
Testing MSE:    14.799416358947754
Testing MAE:    3.0779748413085937

Epoch  2
Training loss:  6.859761004447937
Training MSE:   6.859761004447937
Training MAE:   1.9891061855316161
Testing loss:   1.6163472213745118
Testing MSE:    1.6163472213745118
Testing MAE:    0.978517416381836

Epoch  3
Training loss:  0.6523290466070175
Training MSE:   0.6523290466070175
Training MAE:   0.5968096008300782
Testing loss:   0.2559583340167999
Testing MSE:    0.2559583340167999
Testing MAE:    0.38010212097167967

Epoch  4
Training loss:  0.20465886181294918
Training MSE:   0.20465886181294918
Training MAE:   0.33143725090026854
Testing loss:   0.137523056858778
Testing MSE:    0.137523056858778
Testing MAE:    0.26384808197021486

Epoch  5
Training loss:  0.1231462678924203
Training MSE:   0.1231462678924203
Training MAE:   0.2517911186218262
Testing loss:   0.08547612282037735
Testing MSE:    0.08547612282037735
Testing MAE:    0.2089097183227539

Epoch  6
Training loss:  0.11519703931957483
Training MSE:   0.11519703931957483
Training MAE:   0.2512273548126221
Testing loss:   0.10408528738021851
Testing MSE:    0.10408528738021851
Testing MAE:    0.23855208435058595

Epoch  7
Training loss:  0.14081914208382368
Training MSE:   0.14081914208382368
Training MAE:   0.2855062084197998
Testing loss:   0.0891105375289917
Testing MSE:    0.0891105375289917
Testing MAE:    0.2192811492919922

Epoch  8
Training loss:  0.1332380474001169
Training MSE:   0.1332380474001169
Training MAE:   0.27912457618713377
Testing loss:   0.06904883010983467
Testing MSE:    0.06904883010983467
Testing MAE:    0.19924307708740235

Epoch  9
Training loss:  0.14205219811052083
Training MSE:   0.14205219811052083
Training MAE:   0.28731367149353026
Testing loss:   0.24959123349189757
Testing MSE:    0.24959123349189757
Testing MAE:    0.4522427688598633

Training loss:  0.2488074003815651
Training MSE:   0.2488074003815651
Training MAE:   0.45341247520446776

Testing loss:  0.24959123349189757
Testing MSE:   0.24959123349189757
Testing MAE:   0.4522427688598633

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 15, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7402.972881801224
Training MSE:   7402.972881801224
Training MAE:   26.088709564590456
Testing loss:   21.459890182495116
Testing MSE:    21.459890182495116
Testing MAE:    3.518376774597168

Epoch  1
Training loss:  13.000010087013244
Training MSE:   13.000010087013244
Training MAE:   2.6898545196533203
Testing loss:   5.9966103485107425
Testing MSE:    5.9966103485107425
Testing MAE:    1.8608131988525392

Epoch  2
Training loss:  3.0910231180667878
Training MSE:   3.0910231180667878
Training MAE:   1.287691527557373
Testing loss:   1.09330570602417
Testing MSE:    1.09330570602417
Testing MAE:    0.8027875991821289

Epoch  3
Training loss:  0.5417982973873615
Training MSE:   0.5417982973873615
Training MAE:   0.5279957397460937
Testing loss:   0.24820574684143065
Testing MSE:    0.24820574684143065
Testing MAE:    0.36957523498535155

Epoch  4
Training loss:  0.1763354999691248
Training MSE:   0.1763354999691248
Training MAE:   0.29029903259277345
Testing loss:   0.15541726717948914
Testing MSE:    0.15541726717948914
Testing MAE:    0.2992844741821289

Epoch  5
Training loss:  0.12237884351313114
Training MSE:   0.12237884351313114
Training MAE:   0.24259053535461425
Testing loss:   0.18169260342121124
Testing MSE:    0.18169260342121124
Testing MAE:    0.34230179443359376

Epoch  6
Training loss:  0.11394885738044977
Training MSE:   0.11394885738044977
Training MAE:   0.24308854522705078
Testing loss:   0.08781379599571228
Testing MSE:    0.08781379599571228
Testing MAE:    0.2116173568725586

Epoch  7
Training loss:  0.12898222248703242
Training MSE:   0.12898222248703242
Training MAE:   0.26769564590454104
Testing loss:   0.10361940248012542
Testing MSE:    0.10361940248012542
Testing MAE:    0.2428544219970703

Epoch  8
Training loss:  0.15810559111088515
Training MSE:   0.15810559111088515
Training MAE:   0.3037416374206543
Testing loss:   0.08027963796257973
Testing MSE:    0.08027963796257973
Testing MAE:    0.2058383514404297

Epoch  9
Training loss:  0.14671111762076616
Training MSE:   0.14671111762076616
Training MAE:   0.2942620235443115
Testing loss:   0.10102241290807724
Testing MSE:    0.10102241290807724
Testing MAE:    0.2533681259155273

Training loss:  0.10232099579870702
Training MSE:   0.10232099579870702
Training MAE:   0.2531792079925537

Testing loss:  0.10102241290807724
Testing MSE:   0.10102241290807724
Testing MAE:   0.2533681259155273

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 15, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7839.8894540054325
Training MSE:   7839.8894540054325
Training MAE:   29.547855999374388
Testing loss:   43.1981489074707
Testing MSE:    43.1981489074707
Testing MAE:    4.932204496765137

Epoch  1
Training loss:  29.973760746765137
Training MSE:   29.973760746765137
Training MAE:   4.086315937423706
Testing loss:   17.005360820007326
Testing MSE:    17.005360820007326
Testing MAE:    3.117158694458008

Epoch  2
Training loss:  8.98120552330017
Training MSE:   8.98120552330017
Training MAE:   2.179012720108032
Testing loss:   3.188017469406128
Testing MSE:    3.188017469406128
Testing MAE:    1.3264723846435547

Epoch  3
Training loss:  1.4955816327929496
Training MSE:   1.4955816327929496
Training MAE:   0.8838244171142579
Testing loss:   0.564522027015686
Testing MSE:    0.564522027015686
Testing MAE:    0.570010987854004

Epoch  4
Training loss:  0.3497621003806591
Training MSE:   0.3497621003806591
Training MAE:   0.43463144149780275
Testing loss:   0.17838776714801788
Testing MSE:    0.17838776714801788
Testing MAE:    0.31216895904541014

Epoch  5
Training loss:  0.1501010081410408
Training MSE:   0.1501010081410408
Training MAE:   0.27216384048461917
Testing loss:   0.10346021350622177
Testing MSE:    0.10346021350622177
Testing MAE:    0.23165382537841797

Epoch  6
Training loss:  0.10758645983189344
Training MSE:   0.10758645983189344
Training MAE:   0.23083330039978028
Testing loss:   0.14212251623868943
Testing MSE:    0.14212251623868943
Testing MAE:    0.3098035842895508

Epoch  7
Training loss:  0.12703432075828314
Training MSE:   0.12703432075828314
Training MAE:   0.26186447830200194
Testing loss:   0.07740426172018051
Testing MSE:    0.07740426172018051
Testing MAE:    0.21014902648925782

Epoch  8
Training loss:  0.13443950645178557
Training MSE:   0.13443950645178557
Training MAE:   0.27868794174194333
Testing loss:   0.07164768155813217
Testing MSE:    0.07164768155813217
Testing MAE:    0.20384171905517579

Epoch  9
Training loss:  0.13353192439824343
Training MSE:   0.13353192439824343
Training MAE:   0.2811441375732422
Testing loss:   0.06998340902328491
Testing MSE:    0.06998340902328491
Testing MAE:    0.20312020874023437

Training loss:  0.07146012470275163
Training MSE:   0.07146012470275163
Training MAE:   0.20290542221069335

Testing loss:  0.06998340902328491
Testing MSE:   0.06998340902328491
Testing MAE:   0.20312020874023437

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 15, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9122.21733028121
Training MSE:   9122.21733028121
Training MAE:   31.91019533843994
Testing loss:   1.5277810564041139
Testing MSE:    1.5277810564041139
Testing MAE:    0.9526660690307617

Epoch  1
Training loss:  0.4995877078086138
Training MSE:   0.4995877078086138
Training MAE:   0.49785991592407225
Testing loss:   0.19661184655427932
Testing MSE:    0.19661184655427932
Testing MAE:    0.29922095642089847

Epoch  2
Training loss:  0.13311779631525278
Training MSE:   0.13311779631525278
Training MAE:   0.23980493278503417
Testing loss:   0.0963981877207756
Testing MSE:    0.0963981877207756
Testing MAE:    0.21528207702636717

Epoch  3
Training loss:  0.08967490084320306
Training MSE:   0.08967490084320306
Training MAE:   0.2018040512084961
Testing loss:   0.14233755662441253
Testing MSE:    0.14233755662441253
Testing MAE:    0.3135112319946289

Epoch  4
Training loss:  0.07501111382320523
Training MSE:   0.07501111382320523
Training MAE:   0.19023588409423828
Testing loss:   0.10381652423143387
Testing MSE:    0.10381652423143387
Testing MAE:    0.2594116516113281

Epoch  5
Training loss:  0.07608196604251861
Training MSE:   0.07608196604251861
Training MAE:   0.1993924545288086
Testing loss:   0.059667668545246126
Testing MSE:    0.059667668545246126
Testing MAE:    0.16897696990966796

Epoch  6
Training loss:  0.07375505263060331
Training MSE:   0.07375505263060331
Training MAE:   0.19907609367370604
Testing loss:   0.050913134914636615
Testing MSE:    0.050913134914636615
Testing MAE:    0.15993632354736328

Epoch  7
Training loss:  0.08242989898100496
Training MSE:   0.08242989898100496
Training MAE:   0.2142655475616455
Testing loss:   0.06628940129876137
Testing MSE:    0.06628940129876137
Testing MAE:    0.18834131927490233

Epoch  8
Training loss:  0.09469390515610576
Training MSE:   0.09469390515610576
Training MAE:   0.23264053153991698
Testing loss:   0.09478326549530029
Testing MSE:    0.09478326549530029
Testing MAE:    0.24853053131103517

Epoch  9
Training loss:  0.08744623581171036
Training MSE:   0.08744623581171036
Training MAE:   0.22490158576965333
Testing loss:   0.1725929765701294
Testing MSE:    0.1725929765701294
Testing MAE:    0.3737078826904297

Training loss:  0.1742029101550579
Training MSE:   0.1742029101550579
Training MAE:   0.3740674411773682

Testing loss:  0.1725929765701294
Testing MSE:   0.1725929765701294
Testing MAE:   0.3737078826904297

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 15, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10243.805429063892
Training MSE:   10243.805429063892
Training MAE:   36.18638172531128
Testing loss:   4.621897721862793
Testing MSE:    4.621897721862793
Testing MAE:    1.7028844635009766

Epoch  1
Training loss:  0.8229977062106133
Training MSE:   0.8229977062106133
Training MAE:   0.5605724529266357
Testing loss:   0.17219116322994232
Testing MSE:    0.17219116322994232
Testing MAE:    0.2918000411987305

Epoch  2
Training loss:  0.11523870850652457
Training MSE:   0.11523870850652457
Training MAE:   0.22220102844238282
Testing loss:   0.07406933243274688
Testing MSE:    0.07406933243274688
Testing MAE:    0.18532452087402343

Epoch  3
Training loss:  0.08410170322209597
Training MSE:   0.08410170322209597
Training MAE:   0.19881847953796386
Testing loss:   0.06988706515431405
Testing MSE:    0.06988706515431405
Testing MAE:    0.18159744567871094

Epoch  4
Training loss:  0.08078910087645054
Training MSE:   0.08078910087645054
Training MAE:   0.20353964767456054
Testing loss:   0.0729096529841423
Testing MSE:    0.0729096529841423
Testing MAE:    0.19660491333007812

Epoch  5
Training loss:  0.07171732668429613
Training MSE:   0.07171732668429613
Training MAE:   0.1927066879272461
Testing loss:   0.05808727107644081
Testing MSE:    0.05808727107644081
Testing MAE:    0.1765825393676758

Epoch  6
Training loss:  0.07518313024565577
Training MSE:   0.07518313024565577
Training MAE:   0.2010216609954834
Testing loss:   0.05095917711853981
Testing MSE:    0.05095917711853981
Testing MAE:    0.15630584716796875

Epoch  7
Training loss:  0.07341929072663188
Training MSE:   0.07341929072663188
Training MAE:   0.19973713188171388
Testing loss:   0.048550965410470964
Testing MSE:    0.048550965410470964
Testing MAE:    0.15734459533691406

Epoch  8
Training loss:  0.08930689562410116
Training MSE:   0.08930689562410116
Training MAE:   0.22637582778930665
Testing loss:   0.07738816766142845
Testing MSE:    0.07738816766142845
Testing MAE:    0.22377763519287108

Epoch  9
Training loss:  0.08581070694848895
Training MSE:   0.08581070694848895
Training MAE:   0.22172390785217286
Testing loss:   0.04331094045639038
Testing MSE:    0.04331094045639038
Testing MAE:    0.14961029205322265

Training loss:  0.04349420885294676
Training MSE:   0.04349420885294676
Training MAE:   0.14853798675537108

Testing loss:  0.04331094045639038
Testing MSE:   0.04331094045639038
Testing MAE:   0.14961029205322265

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 10, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  8807.822381048583
Training MSE:   8807.822381048583
Training MAE:   31.04871480102539
Testing loss:   42.43487302856445
Testing MSE:    42.43487302856445
Testing MAE:    5.141913322448731

Epoch  1
Training loss:  20.673061926460267
Training MSE:   20.673061926460267
Training MAE:   3.38925440826416
Testing loss:   5.4659354110717775
Testing MSE:    5.4659354110717775
Testing MAE:    1.8087198028564453

Epoch  2
Training loss:  2.2312066039562226
Training MSE:   2.2312066039562226
Training MAE:   1.0785877464294433
Testing loss:   0.7397671736240387
Testing MSE:    0.7397671736240387
Testing MAE:    0.6310109115600586

Epoch  3
Training loss:  0.5099631141901017
Training MSE:   0.5099631141901017
Training MAE:   0.5159110710144043
Testing loss:   0.2882908949613571
Testing MSE:    0.2882908949613571
Testing MAE:    0.3696766937255859

Epoch  4
Training loss:  0.20402165322750807
Training MSE:   0.20402165322750807
Training MAE:   0.30600181999206544
Testing loss:   0.12622269719839097
Testing MSE:    0.12622269719839097
Testing MAE:    0.2209825958251953

Epoch  5
Training loss:  0.12095157795995475
Training MSE:   0.12095157795995475
Training MAE:   0.2313375171661377
Testing loss:   0.11716213818788529
Testing MSE:    0.11716213818788529
Testing MAE:    0.24744473571777345

Epoch  6
Training loss:  0.11196813537031412
Training MSE:   0.11196813537031412
Training MAE:   0.2352105312347412
Testing loss:   0.07345647981762886
Testing MSE:    0.07345647981762886
Testing MAE:    0.17559073028564454

Epoch  7
Training loss:  0.1366177224740386
Training MSE:   0.1366177224740386
Training MAE:   0.2748654327392578
Testing loss:   0.06312085946202278
Testing MSE:    0.06312085946202278
Testing MAE:    0.1650005111694336

Epoch  8
Training loss:  0.14612994923517109
Training MSE:   0.14612994923517109
Training MAE:   0.28822188034057616
Testing loss:   0.06351518402695656
Testing MSE:    0.06351518402695656
Testing MAE:    0.1816781463623047

Epoch  9
Training loss:  0.12831857300251723
Training MSE:   0.12831857300251723
Training MAE:   0.2712959857940674
Testing loss:   0.050713657167553904
Testing MSE:    0.050713657167553904
Testing MAE:    0.14938893280029297

Training loss:  0.05040635369196534
Training MSE:   0.05040635369196534
Training MAE:   0.14876672134399413

Testing loss:  0.050713657167553904
Testing MSE:   0.050713657167553904
Testing MAE:   0.14938893280029297

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 10, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  17135.518390928126
Training MSE:   17135.518390928126
Training MAE:   51.64718159942627
Testing loss:   1.4758053298950196
Testing MSE:    1.4758053298950196
Testing MAE:    0.8718854415893554

Epoch  1
Training loss:  0.46228466812968255
Training MSE:   0.46228466812968255
Training MAE:   0.422554504776001
Testing loss:   0.12533044931292533
Testing MSE:    0.12533044931292533
Testing MAE:    0.20337006225585938

Epoch  2
Training loss:  0.10347199693024159
Training MSE:   0.10347199693024159
Training MAE:   0.19463624267578125
Testing loss:   0.0767730629146099
Testing MSE:    0.0767730629146099
Testing MAE:    0.17117363739013672

Epoch  3
Training loss:  0.07804132865667343
Training MSE:   0.07804132865667343
Training MAE:   0.18656305274963378
Testing loss:   0.06400602288246154
Testing MSE:    0.06400602288246154
Testing MAE:    0.16623145446777343

Epoch  4
Training loss:  0.07206976055651904
Training MSE:   0.07206976055651904
Training MAE:   0.18931874504089355
Testing loss:   0.05645401950478554
Testing MSE:    0.05645401950478554
Testing MAE:    0.1657546371459961

Epoch  5
Training loss:  0.06501437572985888
Training MSE:   0.06501437572985888
Training MAE:   0.18137265815734863
Testing loss:   0.07278100380301475
Testing MSE:    0.07278100380301475
Testing MAE:    0.20887908172607422

Epoch  6
Training loss:  0.06492143254205585
Training MSE:   0.06492143254205585
Training MAE:   0.18474570083618164
Testing loss:   0.055147859045863154
Testing MSE:    0.055147859045863154
Testing MAE:    0.1660102813720703

Epoch  7
Training loss:  0.07629279726594687
Training MSE:   0.07629279726594687
Training MAE:   0.20678038177490235
Testing loss:   0.05416235639452934
Testing MSE:    0.05416235639452934
Testing MAE:    0.17532812805175782

Epoch  8
Training loss:  0.08301551807150245
Training MSE:   0.08301551807150245
Training MAE:   0.2186937728881836
Testing loss:   0.04295474365055561
Testing MSE:    0.04295474365055561
Testing MAE:    0.1483007034301758

Epoch  9
Training loss:  0.08322623712792993
Training MSE:   0.08322623712792993
Training MAE:   0.21913093566894531
Testing loss:   0.06349517512321472
Testing MSE:    0.06349517512321472
Testing MAE:    0.19197592010498046

Training loss:  0.06493829740136862
Training MSE:   0.06493829740136862
Training MAE:   0.19234986305236818

Testing loss:  0.06349517512321472
Testing MSE:   0.06349517512321472
Testing MAE:   0.19197592010498046

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 10, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  6073.744606245423
Training MSE:   6073.744606245423
Training MAE:   24.508341709899902
Testing loss:   18.689219573974608
Testing MSE:    18.689219573974608
Testing MAE:    3.363036541748047

Epoch  1
Training loss:  9.433095867395402
Training MSE:   9.433095867395402
Training MAE:   2.2832558601379396
Testing loss:   2.3908457733154296
Testing MSE:    2.3908457733154296
Testing MAE:    1.1743553756713867

Epoch  2
Training loss:  0.886498760175705
Training MSE:   0.886498760175705
Training MAE:   0.6801163398742676
Testing loss:   0.37650238080024717
Testing MSE:    0.37650238080024717
Testing MAE:    0.4800427032470703

Epoch  3
Training loss:  0.19162568591237067
Training MSE:   0.19162568591237067
Training MAE:   0.30558980751037595
Testing loss:   0.10947511103153229
Testing MSE:    0.10947511103153229
Testing MAE:    0.2142481460571289

Epoch  4
Training loss:  0.10471745328754187
Training MSE:   0.10471745328754187
Training MAE:   0.21414421577453613
Testing loss:   0.07591292316913605
Testing MSE:    0.07591292316913605
Testing MAE:    0.17961533203125

Epoch  5
Training loss:  0.08902613141834737
Training MSE:   0.08902613141834737
Training MAE:   0.20506895332336425
Testing loss:   0.07560408737659455
Testing MSE:    0.07560408737659455
Testing MAE:    0.19756057586669923

Epoch  6
Training loss:  0.08830741828680039
Training MSE:   0.08830741828680039
Training MAE:   0.21339372444152832
Testing loss:   0.0580542917907238
Testing MSE:    0.0580542917907238
Testing MAE:    0.164932568359375

Epoch  7
Training loss:  0.10786837995350361
Training MSE:   0.10786837995350361
Training MAE:   0.24616966133117676
Testing loss:   0.07254932673573494
Testing MSE:    0.07254932673573494
Testing MAE:    0.2052470428466797

Epoch  8
Training loss:  0.11443735959231853
Training MSE:   0.11443735959231853
Training MAE:   0.2570429180145264
Testing loss:   0.08503630728721619
Testing MSE:    0.08503630728721619
Testing MAE:    0.2335062484741211

Epoch  9
Training loss:  0.14069591203778983
Training MSE:   0.14069591203778983
Training MAE:   0.28898292274475096
Testing loss:   0.04412606151103973
Testing MSE:    0.04412606151103973
Testing MAE:    0.14725861968994142

Training loss:  0.044854820067435505
Training MSE:   0.044854820067435505
Training MAE:   0.14652269439697266

Testing loss:  0.04412606151103973
Testing MSE:   0.04412606151103973
Testing MAE:   0.14725861968994142

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 10, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  13316.799928623199
Training MSE:   13316.799928623199
Training MAE:   42.40897957763672
Testing loss:   19.38080066833496
Testing MSE:    19.38080066833496
Testing MAE:    3.426934722900391

Epoch  1
Training loss:  14.689348683929444
Training MSE:   14.689348683929444
Training MAE:   2.9779801387786864
Testing loss:   9.885546949768067
Testing MSE:    9.885546949768067
Testing MAE:    2.458893835449219

Epoch  2
Training loss:  5.8232542352676395
Training MSE:   5.8232542352676395
Training MAE:   1.8377204391479491
Testing loss:   2.3465344352722166
Testing MSE:    2.3465344352722166
Testing MAE:    1.1756648071289062

Epoch  3
Training loss:  0.9433081393003464
Training MSE:   0.9433081393003464
Training MAE:   0.6766895732879639
Testing loss:   0.2472893324136734
Testing MSE:    0.2472893324136734
Testing MAE:    0.3290609664916992

Epoch  4
Training loss:  0.16796062944978477
Training MSE:   0.16796062944978477
Training MAE:   0.2673976058959961
Testing loss:   0.11864136303067208
Testing MSE:    0.11864136303067208
Testing MAE:    0.21941457061767577

Epoch  5
Training loss:  0.1156158550158143
Training MSE:   0.1156158550158143
Training MAE:   0.22034050216674805
Testing loss:   0.0870006962776184
Testing MSE:    0.0870006962776184
Testing MAE:    0.18119371948242188

Epoch  6
Training loss:  0.09607688573226332
Training MSE:   0.09607688573226332
Training MAE:   0.20600629463195802
Testing loss:   0.10963154300451279
Testing MSE:    0.10963154300451279
Testing MAE:    0.25393038635253906

Epoch  7
Training loss:  0.09457236091196537
Training MSE:   0.09457236091196537
Training MAE:   0.21847889518737793
Testing loss:   0.17731742441654205
Testing MSE:    0.17731742441654205
Testing MAE:    0.3617057098388672

Epoch  8
Training loss:  0.10817863400876522
Training MSE:   0.10817863400876522
Training MAE:   0.24525594787597657
Testing loss:   0.21702195036411284
Testing MSE:    0.21702195036411284
Testing MAE:    0.41271511840820313

Epoch  9
Training loss:  0.10853212459385395
Training MSE:   0.10853212459385395
Training MAE:   0.24553096199035646
Testing loss:   0.1167992149591446
Testing MSE:    0.1167992149591446
Testing MAE:    0.28244872283935546

Training loss:  0.11668050961196423
Training MSE:   0.11668050961196423
Training MAE:   0.2833759834289551

Testing loss:  0.1167992149591446
Testing MSE:   0.1167992149591446
Testing MAE:   0.28244872283935546

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 10, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  11066.322282466126
Training MSE:   11066.322282466126
Training MAE:   37.097295771789554
Testing loss:   25.682072790527343
Testing MSE:    25.682072790527343
Testing MAE:    4.1032247039794925

Epoch  1
Training loss:  16.803235836410522
Training MSE:   16.803235836410522
Training MAE:   3.260468783569336
Testing loss:   8.374169058227539
Testing MSE:    8.374169058227539
Testing MAE:    2.3333568939208984

Epoch  2
Training loss:  4.468612439107895
Training MSE:   4.468612439107895
Training MAE:   1.646558289718628
Testing loss:   1.596764811706543
Testing MSE:    1.596764811706543
Testing MAE:    1.0040096740722657

Epoch  3
Training loss:  0.7898716966509819
Training MSE:   0.7898716966509819
Training MAE:   0.6822417041778565
Testing loss:   0.338554631280899
Testing MSE:    0.338554631280899
Testing MAE:    0.46712378234863283

Epoch  4
Training loss:  0.17527727239131927
Training MSE:   0.17527727239131927
Training MAE:   0.31613521423339846
Testing loss:   0.10079165543317795
Testing MSE:    0.10079165543317795
Testing MAE:    0.23419432220458986

Epoch  5
Training loss:  0.09858776085078716
Training MSE:   0.09858776085078716
Training MAE:   0.22826597023010253
Testing loss:   0.07409492681622505
Testing MSE:    0.07409492681622505
Testing MAE:    0.19585597076416017

Epoch  6
Training loss:  0.09678161690980196
Training MSE:   0.09678161690980196
Training MAE:   0.22807118682861327
Testing loss:   0.10271420638561249
Testing MSE:    0.10271420638561249
Testing MAE:    0.24351236114501953

Epoch  7
Training loss:  0.11457070058509708
Training MSE:   0.11457070058509708
Training MAE:   0.2550674850463867
Testing loss:   0.08523873343467712
Testing MSE:    0.08523873343467712
Testing MAE:    0.21431841125488282

Epoch  8
Training loss:  0.12705008253455163
Training MSE:   0.12705008253455163
Training MAE:   0.2724509860992432
Testing loss:   0.10383062592744827
Testing MSE:    0.10383062592744827
Testing MAE:    0.2542885498046875

Epoch  9
Training loss:  0.1203300296112895
Training MSE:   0.1203300296112895
Training MAE:   0.26612380638122557
Testing loss:   0.17939523651599884
Testing MSE:    0.17939523651599884
Testing MAE:    0.37529354553222655

Training loss:  0.17821737909913063
Training MSE:   0.17821737909913063
Training MAE:   0.375910746383667

Testing loss:  0.17939523651599884
Testing MSE:   0.17939523651599884
Testing MAE:   0.37529354553222655

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 5, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  11226.211333423114
Training MSE:   11226.211333423114
Training MAE:   35.11940929107666
Testing loss:   0.9376491579532623
Testing MSE:    0.9376491579532623
Testing MAE:    0.725220263671875

Epoch  1
Training loss:  0.46411297398507595
Training MSE:   0.46411297398507595
Training MAE:   0.4723156871795654
Testing loss:   0.21263203358650207
Testing MSE:    0.21263203358650207
Testing MAE:    0.30949300384521483

Epoch  2
Training loss:  0.15678257868587972
Training MSE:   0.15678257868587972
Training MAE:   0.2541576885223389
Testing loss:   0.1092766350030899
Testing MSE:    0.1092766350030899
Testing MAE:    0.21544271240234375

Epoch  3
Training loss:  0.0927445533208549
Training MSE:   0.0927445533208549
Training MAE:   0.1905892421722412
Testing loss:   0.07080853356122971
Testing MSE:    0.07080853356122971
Testing MAE:    0.16603746948242187

Epoch  4
Training loss:  0.07321328487098216
Training MSE:   0.07321328487098216
Training MAE:   0.17766646041870118
Testing loss:   0.06083361033499241
Testing MSE:    0.06083361033499241
Testing MAE:    0.15818690490722656

Epoch  5
Training loss:  0.06893055605590344
Training MSE:   0.06893055605590344
Training MAE:   0.18030870704650878
Testing loss:   0.07293113681077958
Testing MSE:    0.07293113681077958
Testing MAE:    0.1905052932739258

Epoch  6
Training loss:  0.07151671388968825
Training MSE:   0.07151671388968825
Training MAE:   0.19122124519348144
Testing loss:   0.0813325575351715
Testing MSE:    0.0813325575351715
Testing MAE:    0.22508939056396485

Epoch  7
Training loss:  0.07774665062278509
Training MSE:   0.07774665062278509
Training MAE:   0.20509452743530274
Testing loss:   0.04962730386853218
Testing MSE:    0.04962730386853218
Testing MAE:    0.15591223297119142

Epoch  8
Training loss:  0.09306964191496372
Training MSE:   0.09306964191496372
Training MAE:   0.23090674133300781
Testing loss:   0.04883462962508202
Testing MSE:    0.04883462962508202
Testing MAE:    0.15411580810546874

Epoch  9
Training loss:  0.09259992865398527
Training MSE:   0.09259992865398527
Training MAE:   0.23218576927185058
Testing loss:   0.10357927782535553
Testing MSE:    0.10357927782535553
Testing MAE:    0.2729195587158203

Training loss:  0.10327078158259392
Training MSE:   0.10327078158259392
Training MAE:   0.2730189468383789

Testing loss:  0.10357927782535553
Testing MSE:   0.10357927782535553
Testing MAE:   0.2729195587158203

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 5, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10176.339464459228
Training MSE:   10176.339464459228
Training MAE:   36.672521773529056
Testing loss:   71.77980069580079
Testing MSE:    71.77980069580079
Testing MAE:    6.714220616149903

Epoch  1
Training loss:  45.35030623474121
Training MSE:   45.35030623474121
Training MAE:   5.199650645828247
Testing loss:   23.62293366546631
Testing MSE:    23.62293366546631
Testing MAE:    3.7638402282714845

Epoch  2
Training loss:  12.852535543060302
Training MSE:   12.852535543060302
Training MAE:   2.727185546875
Testing loss:   6.88688129272461
Testing MSE:    6.88688129272461
Testing MAE:    2.0401616516113283

Epoch  3
Training loss:  2.8166749839782717
Training MSE:   2.8166749839782717
Training MAE:   1.2758847045898438
Testing loss:   0.9528369841575622
Testing MSE:    0.9528369841575622
Testing MAE:    0.7578840011596679

Epoch  4
Training loss:  0.44927210069298745
Training MSE:   0.44927210069298745
Training MAE:   0.48396805114746094
Testing loss:   0.15003814656734465
Testing MSE:    0.15003814656734465
Testing MAE:    0.25910696258544924

Epoch  5
Training loss:  0.1274932508111
Training MSE:   0.1274932508111
Training MAE:   0.2406154327392578
Testing loss:   0.08841449307203293
Testing MSE:    0.08841449307203293
Testing MAE:    0.18989817504882814

Epoch  6
Training loss:  0.10418772892653942
Training MSE:   0.10418772892653942
Training MAE:   0.22380804100036622
Testing loss:   0.14736955108642577
Testing MSE:    0.14736955108642577
Testing MAE:    0.2998303268432617

Epoch  7
Training loss:  0.11962047763019801
Training MSE:   0.11962047763019801
Training MAE:   0.2532951240539551
Testing loss:   0.10307279832363128
Testing MSE:    0.10307279832363128
Testing MAE:    0.25347056884765623

Epoch  8
Training loss:  0.12666723505705596
Training MSE:   0.12666723505705596
Training MAE:   0.26799281692504884
Testing loss:   0.05594417325556278
Testing MSE:    0.05594417325556278
Testing MAE:    0.1591585403442383

Epoch  9
Training loss:  0.1233226491957903
Training MSE:   0.1233226491957903
Training MAE:   0.2652105724334717
Testing loss:   0.04822159655690193
Testing MSE:    0.04822159655690193
Testing MAE:    0.14969186401367188

Training loss:  0.048617378839105364
Training MSE:   0.048617378839105364
Training MAE:   0.14878044624328612

Testing loss:  0.04822159655690193
Testing MSE:   0.04822159655690193
Testing MAE:   0.14969186401367188

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 5, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9563.276856098939
Training MSE:   9563.276856098939
Training MAE:   34.6970999256134
Testing loss:   34.01967706604004
Testing MSE:    34.01967706604004
Testing MAE:    4.616873031616211

Epoch  1
Training loss:  20.500773346328735
Training MSE:   20.500773346328735
Training MAE:   3.524614771652222
Testing loss:   11.35771167602539
Testing MSE:    11.35771167602539
Testing MAE:    2.6547156768798827

Epoch  2
Training loss:  6.385796839094162
Training MSE:   6.385796839094162
Training MAE:   1.9200836681365967
Testing loss:   2.6137148658752443
Testing MSE:    2.6137148658752443
Testing MAE:    1.2439067794799805

Epoch  3
Training loss:  1.3956713888406753
Training MSE:   1.3956713888406753
Training MAE:   0.8810279685974121
Testing loss:   0.566508748793602
Testing MSE:    0.566508748793602
Testing MAE:    0.5725651779174805

Epoch  4
Training loss:  0.344054149055481
Training MSE:   0.344054149055481
Training MAE:   0.4256514991760254
Testing loss:   0.19677202591896056
Testing MSE:    0.19677202591896056
Testing MAE:    0.30744446868896486

Epoch  5
Training loss:  0.13586642010062933
Training MSE:   0.13586642010062933
Training MAE:   0.2464690715789795
Testing loss:   0.08682958118319511
Testing MSE:    0.08682958118319511
Testing MAE:    0.19320696411132812

Epoch  6
Training loss:  0.09652390709668397
Training MSE:   0.09652390709668397
Training MAE:   0.21082289695739745
Testing loss:   0.07541128601431847
Testing MSE:    0.07541128601431847
Testing MAE:    0.1971020538330078

Epoch  7
Training loss:  0.09401256210133434
Training MSE:   0.09401256210133434
Training MAE:   0.221655558013916
Testing loss:   0.0831222393155098
Testing MSE:    0.0831222393155098
Testing MAE:    0.22339305725097655

Epoch  8
Training loss:  0.09103032662272453
Training MSE:   0.09103032662272453
Training MAE:   0.22362993049621582
Testing loss:   0.13632981359958649
Testing MSE:    0.13632981359958649
Testing MAE:    0.31158434295654297

Epoch  9
Training loss:  0.09513823442235589
Training MSE:   0.09513823442235589
Training MAE:   0.23219155654907225
Testing loss:   0.09911196582317353
Testing MSE:    0.09911196582317353
Testing MAE:    0.2546380920410156

Training loss:  0.10100285389125348
Training MSE:   0.10100285389125348
Training MAE:   0.2543487606048584

Testing loss:  0.09911196582317353
Testing MSE:   0.09911196582317353
Testing MAE:   0.2546380920410156

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 5, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  169776.2352875
Training MSE:   169776.2352875
Training MAE:   410.0321190185547
Testing loss:   166600.29835
Testing MSE:    166600.29835
Testing MAE:    406.20847495117187

Epoch  1
Training loss:  162137.369225
Training MSE:   162137.369225
Training MAE:   400.64875854492186
Testing loss:   157552.67505
Testing MSE:    157552.67505
Testing MAE:    394.9148033203125

Epoch  2
Training loss:  151493.4885875
Training MSE:   151493.4885875
Training MAE:   387.1267931640625
Testing loss:   145486.866725
Testing MSE:    145486.866725
Testing MAE:    379.3308500488281

Epoch  3
Training loss:  138333.16293125
Training MSE:   138333.16293125
Training MAE:   369.7302520996094
Testing loss:   131331.575825
Testing MSE:    131331.575825
Testing MAE:    360.1896755371094

Epoch  4
Training loss:  123428.934675
Training MSE:   123428.934675
Training MAE:   348.97299436035155
Testing loss:   115749.5662625
Testing MSE:    115749.5662625
Testing MAE:    337.8677151855469

Epoch  5
Training loss:  107387.100575
Training MSE:   107387.100575
Training MAE:   325.15776193847654
Testing loss:   99308.3717125
Testing MSE:    99308.3717125
Testing MAE:    312.59142451171874

Epoch  6
Training loss:  90768.74083125
Training MSE:   90768.74083125
Training MAE:   298.4882430175781
Testing loss:   82573.38285
Testing MSE:    82573.38285
Testing MAE:    284.5670564941406

Epoch  7
Training loss:  74139.505840625
Training MSE:   74139.505840625
Training MAE:   269.1869174072266
Testing loss:   66111.528
Testing MSE:    66111.528
Testing MAE:    254.0010923828125

Epoch  8
Training loss:  58083.15858125
Training MSE:   58083.15858125
Training MAE:   237.47005572509767
Testing loss:   50520.682725
Testing MSE:    50520.682725
Testing MAE:    221.19203330078125

Epoch  9
Training loss:  43187.803078125
Training MSE:   43187.803078125
Training MAE:   203.6966475830078
Testing loss:   36371.2163
Testing MSE:    36371.2163
Testing MAE:    186.5085140625

Training loss:  36262.921228125
Training MSE:   36262.921228125
Training MAE:   186.2056349609375

Testing loss:  36371.2163
Testing MSE:   36371.2163
Testing MAE:   186.5085140625

Number of layers:  3
Number of units in layer [1,2,3]:  [20, 5, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7585.960743789673
Training MSE:   7585.960743789673
Training MAE:   31.047309925079347
Testing loss:   30.71339515991211
Testing MSE:    30.71339515991211
Testing MAE:    4.316878750610352

Epoch  1
Training loss:  14.204927621126174
Training MSE:   14.204927621126174
Training MAE:   2.6049111709594728
Testing loss:   0.6003696449756623
Testing MSE:    0.6003696449756623
Testing MAE:    0.5618818634033204

Epoch  2
Training loss:  0.2286108742952347
Training MSE:   0.2286108742952347
Training MAE:   0.3121265651702881
Testing loss:   0.12211246265769005
Testing MSE:    0.12211246265769005
Testing MAE:    0.2160083526611328

Epoch  3
Training loss:  0.09716451589167117
Training MSE:   0.09716451589167117
Training MAE:   0.20677258987426758
Testing loss:   0.09119536193609237
Testing MSE:    0.09119536193609237
Testing MAE:    0.21359115600585937

Epoch  4
Training loss:  0.08020400356799363
Training MSE:   0.08020400356799363
Training MAE:   0.19507331390380858
Testing loss:   0.07801733534932137
Testing MSE:    0.07801733534932137
Testing MAE:    0.20241338653564453

Epoch  5
Training loss:  0.07406187508925796
Training MSE:   0.07406187508925796
Training MAE:   0.1919166316986084
Testing loss:   0.05986608776450157
Testing MSE:    0.05986608776450157
Testing MAE:    0.16658512115478516

Epoch  6
Training loss:  0.07318928522020578
Training MSE:   0.07318928522020578
Training MAE:   0.1953699405670166
Testing loss:   0.08031782915592194
Testing MSE:    0.08031782915592194
Testing MAE:    0.2200636245727539

Epoch  7
Training loss:  0.07401687117666006
Training MSE:   0.07401687117666006
Training MAE:   0.19978797912597657
Testing loss:   0.050790908095240596
Testing MSE:    0.050790908095240596
Testing MAE:    0.15420444641113282

Epoch  8
Training loss:  0.0880442322447896
Training MSE:   0.0880442322447896
Training MAE:   0.22417698783874512
Testing loss:   0.05124554866552353
Testing MSE:    0.05124554866552353
Testing MAE:    0.16543338012695313

Epoch  9
Training loss:  0.09175835226848722
Training MSE:   0.09175835226848722
Training MAE:   0.22977655448913575
Testing loss:   0.1337410694360733
Testing MSE:    0.1337410694360733
Testing MAE:    0.31341060791015624

Training loss:  0.13452211673259734
Training MSE:   0.13452211673259734
Training MAE:   0.3133361042022705

Testing loss:  0.1337410694360733
Testing MSE:   0.1337410694360733
Testing MAE:   0.31341060791015624

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 25, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10883.697186586953
Training MSE:   10883.697186586953
Training MAE:   35.96593699035645
Testing loss:   2.3537171184539796
Testing MSE:    2.3537171184539796
Testing MAE:    1.185731037902832

Epoch  1
Training loss:  0.7194946210235357
Training MSE:   0.7194946210235357
Training MAE:   0.5812382579803467
Testing loss:   0.16779123071432114
Testing MSE:    0.16779123071432114
Testing MAE:    0.2690318374633789

Epoch  2
Training loss:  0.1301136119440198
Training MSE:   0.1301136119440198
Training MAE:   0.23429159088134766
Testing loss:   0.09077946188449859
Testing MSE:    0.09077946188449859
Testing MAE:    0.19300278778076171

Epoch  3
Training loss:  0.088237476465106
Training MSE:   0.088237476465106
Training MAE:   0.20298211326599122
Testing loss:   0.06998478543758392
Testing MSE:    0.06998478543758392
Testing MAE:    0.17526815032958984

Epoch  4
Training loss:  0.08505899575427174
Training MSE:   0.08505899575427174
Training MAE:   0.21113952713012696
Testing loss:   0.07704833099246025
Testing MSE:    0.07704833099246025
Testing MAE:    0.2104221954345703

Epoch  5
Training loss:  0.09892661341130733
Training MSE:   0.09892661341130733
Training MAE:   0.23559882278442382
Testing loss:   0.07034948120117188
Testing MSE:    0.07034948120117188
Testing MAE:    0.1932208023071289

Epoch  6
Training loss:  0.10983191756308079
Training MSE:   0.10983191756308079
Training MAE:   0.2502131694793701
Testing loss:   0.08581921800374985
Testing MSE:    0.08581921800374985
Testing MAE:    0.23433318939208983

Epoch  7
Training loss:  0.15018331662267448
Training MSE:   0.15018331662267448
Training MAE:   0.29638416175842286
Testing loss:   0.4571420745849609
Testing MSE:    0.4571420745849609
Testing MAE:    0.6451766342163086

Epoch  8
Training loss:  0.14155245074778794
Training MSE:   0.14155245074778794
Training MAE:   0.29088687248229983
Testing loss:   0.06696286869645118
Testing MSE:    0.06696286869645118
Testing MAE:    0.20290673828125

Epoch  9
Training loss:  0.13552829950600861
Training MSE:   0.13552829950600861
Training MAE:   0.2901973403930664
Testing loss:   0.08989457877874374
Testing MSE:    0.08989457877874374
Testing MAE:    0.2469556625366211

Training loss:  0.08902781737744808
Training MSE:   0.08902781737744808
Training MAE:   0.2472129207611084

Testing loss:  0.08989457877874374
Testing MSE:   0.08989457877874374
Testing MAE:   0.2469556625366211

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 25, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7961.936262204361
Training MSE:   7961.936262204361
Training MAE:   27.818267337417602
Testing loss:   8.400664636230468
Testing MSE:    8.400664636230468
Testing MAE:    2.2006674713134764

Epoch  1
Training loss:  2.091683993792534
Training MSE:   2.091683993792534
Training MAE:   1.0052417572021484
Testing loss:   0.6458076069355011
Testing MSE:    0.6458076069355011
Testing MAE:    0.5654047805786133

Epoch  2
Training loss:  0.383863925755024
Training MSE:   0.383863925755024
Training MAE:   0.4436787605285645
Testing loss:   0.3056887736797333
Testing MSE:    0.3056887736797333
Testing MAE:    0.38328252716064454

Epoch  3
Training loss:  0.2029109212189913
Training MSE:   0.2029109212189913
Training MAE:   0.31640357437133787
Testing loss:   0.19151241706609726
Testing MSE:    0.19151241706609726
Testing MAE:    0.30087396087646484

Epoch  4
Training loss:  0.1292171280503273
Training MSE:   0.1292171280503273
Training MAE:   0.2514826457977295
Testing loss:   0.15670336394309997
Testing MSE:    0.15670336394309997
Testing MAE:    0.2824599136352539

Epoch  5
Training loss:  0.09711614584177733
Training MSE:   0.09711614584177733
Training MAE:   0.22200934371948242
Testing loss:   0.0776402105152607
Testing MSE:    0.0776402105152607
Testing MAE:    0.17357738342285156

Epoch  6
Training loss:  0.10360730196684599
Training MSE:   0.10360730196684599
Training MAE:   0.23866743545532226
Testing loss:   0.09107354073524475
Testing MSE:    0.09107354073524475
Testing MAE:    0.21193430938720703

Epoch  7
Training loss:  0.10441216523051262
Training MSE:   0.10441216523051262
Training MAE:   0.24319399032592773
Testing loss:   0.05785315616130829
Testing MSE:    0.05785315616130829
Testing MAE:    0.15330654754638673

Epoch  8
Training loss:  0.12095331324785948
Training MSE:   0.12095331324785948
Training MAE:   0.2645094139099121
Testing loss:   0.06247369586825371
Testing MSE:    0.06247369586825371
Testing MAE:    0.17436156158447266

Epoch  9
Training loss:  0.12286590388268233
Training MSE:   0.12286590388268233
Training MAE:   0.27007115783691404
Testing loss:   0.25526437249183653
Testing MSE:    0.25526437249183653
Testing MAE:    0.45881859893798826

Training loss:  0.24671118048429488
Training MSE:   0.24671118048429488
Training MAE:   0.45730628509521487

Testing loss:  0.25526437249183653
Testing MSE:   0.25526437249183653
Testing MAE:   0.45881859893798826

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 25, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  15681.184593257523
Training MSE:   15681.184593257523
Training MAE:   48.63088230514526
Testing loss:   8.043199571228028
Testing MSE:    8.043199571228028
Testing MAE:    2.2251140533447265

Epoch  1
Training loss:  3.8694276646614076
Training MSE:   3.8694276646614076
Training MAE:   1.4797395725250244
Testing loss:   1.7465125349998474
Testing MSE:    1.7465125349998474
Testing MAE:    0.9848030944824219

Epoch  2
Training loss:  0.9248598832368851
Training MSE:   0.9248598832368851
Training MAE:   0.6978198425292969
Testing loss:   0.39517902393341064
Testing MSE:    0.39517902393341064
Testing MAE:    0.45188454284667967

Epoch  3
Training loss:  0.2760045213848352
Training MSE:   0.2760045213848352
Training MAE:   0.36598999557495115
Testing loss:   0.15298681807518005
Testing MSE:    0.15298681807518005
Testing MAE:    0.2693562789916992

Epoch  4
Training loss:  0.12668768579810857
Training MSE:   0.12668768579810857
Training MAE:   0.2417698917388916
Testing loss:   0.07416573008298874
Testing MSE:    0.07416573008298874
Testing MAE:    0.18496307525634767

Epoch  5
Training loss:  0.07828998097926378
Training MSE:   0.07828998097926378
Training MAE:   0.19637377891540528
Testing loss:   0.05908598523139954
Testing MSE:    0.05908598523139954
Testing MAE:    0.1770516159057617

Epoch  6
Training loss:  0.07494210142046213
Training MSE:   0.07494210142046213
Training MAE:   0.20240615768432618
Testing loss:   0.046345620813965795
Testing MSE:    0.046345620813965795
Testing MAE:    0.14969632415771483

Epoch  7
Training loss:  0.07279403593838214
Training MSE:   0.07279403593838214
Training MAE:   0.20110965270996095
Testing loss:   0.07244386912584305
Testing MSE:    0.07244386912584305
Testing MAE:    0.20578493041992188

Epoch  8
Training loss:  0.08316823080480099
Training MSE:   0.08316823080480099
Training MAE:   0.21924345588684083
Testing loss:   0.048505402565002444
Testing MSE:    0.048505402565002444
Testing MAE:    0.1617187530517578

Epoch  9
Training loss:  0.0787430025331676
Training MSE:   0.0787430025331676
Training MAE:   0.21435094985961914
Testing loss:   0.04817556087374687
Testing MSE:    0.04817556087374687
Testing MAE:    0.1652583251953125

Training loss:  0.04738234191909432
Training MSE:   0.04738234191909432
Training MAE:   0.1645967586517334

Testing loss:  0.04817556087374687
Testing MSE:   0.04817556087374687
Testing MAE:   0.1652583251953125

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 25, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  4752.529226740647
Training MSE:   4752.529226740647
Training MAE:   21.375806844711303
Testing loss:   14.732368751525879
Testing MSE:    14.732368751525879
Testing MAE:    2.7738659240722656

Epoch  1
Training loss:  6.13385997247696
Training MSE:   6.13385997247696
Training MAE:   1.8022936252593995
Testing loss:   2.831106480407715
Testing MSE:    2.831106480407715
Testing MAE:    1.2912596832275391

Epoch  2
Training loss:  1.913717637705803
Training MSE:   1.913717637705803
Training MAE:   1.0436613986968994
Testing loss:   1.0264691968917847
Testing MSE:    1.0264691968917847
Testing MAE:    0.7547044494628906

Epoch  3
Training loss:  0.6719306555628777
Training MSE:   0.6719306555628777
Training MAE:   0.5982844451904297
Testing loss:   0.4051103264808655
Testing MSE:    0.4051103264808655
Testing MAE:    0.4415414794921875

Epoch  4
Training loss:  0.27581343614459036
Training MSE:   0.27581343614459036
Training MAE:   0.3584803153991699
Testing loss:   0.1732155876517296
Testing MSE:    0.1732155876517296
Testing MAE:    0.2662882293701172

Epoch  5
Training loss:  0.18324288454204798
Training MSE:   0.18324288454204798
Training MAE:   0.2960097270965576
Testing loss:   0.1118792254447937
Testing MSE:    0.1118792254447937
Testing MAE:    0.20620852661132813

Epoch  6
Training loss:  0.14887880892902614
Training MSE:   0.14887880892902614
Training MAE:   0.2773444869995117
Testing loss:   0.09650693427324294
Testing MSE:    0.09650693427324294
Testing MAE:    0.21213116302490234

Epoch  7
Training loss:  0.1680724491506815
Training MSE:   0.1680724491506815
Training MAE:   0.30542836112976074
Testing loss:   0.23938971800804137
Testing MSE:    0.23938971800804137
Testing MAE:    0.4245679840087891

Epoch  8
Training loss:  0.17217782959192993
Training MSE:   0.17217782959192993
Training MAE:   0.3157513542175293
Testing loss:   0.26203266806602477
Testing MSE:    0.26203266806602477
Testing MAE:    0.4568298812866211

Epoch  9
Training loss:  0.1529846066683531
Training MSE:   0.1529846066683531
Training MAE:   0.299017760848999
Testing loss:   0.05910865620374679
Testing MSE:    0.05910865620374679
Testing MAE:    0.1658681335449219

Training loss:  0.058782959543913604
Training MSE:   0.058782959543913604
Training MAE:   0.16428055572509764

Testing loss:  0.05910865620374679
Testing MSE:   0.05910865620374679
Testing MAE:   0.1658681335449219

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 25, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  5436.823583947754
Training MSE:   5436.823583947754
Training MAE:   23.77359865913391
Testing loss:   36.87805282592773
Testing MSE:    36.87805282592773
Testing MAE:    4.608849768066406

Epoch  1
Training loss:  22.490898777008056
Training MSE:   22.490898777008056
Training MAE:   3.5063290966033938
Testing loss:   11.389939930725097
Testing MSE:    11.389939930725097
Testing MAE:    2.5193560287475587

Epoch  2
Training loss:  6.818724093818664
Training MSE:   6.818724093818664
Training MAE:   1.9087742820739746
Testing loss:   3.28893985748291
Testing MSE:    3.28893985748291
Testing MAE:    1.3621598373413086

Epoch  3
Training loss:  2.0107919149160387
Training MSE:   2.0107919149160387
Training MAE:   1.0408883220672607
Testing loss:   0.9317902634620666
Testing MSE:    0.9317902634620666
Testing MAE:    0.7213544052124023

Epoch  4
Training loss:  0.6023308224618434
Training MSE:   0.6023308224618434
Training MAE:   0.5624202518463135
Testing loss:   0.29611859307289123
Testing MSE:    0.29611859307289123
Testing MAE:    0.3860271759033203

Epoch  5
Training loss:  0.23083539331555367
Training MSE:   0.23083539331555367
Training MAE:   0.33231276626586914
Testing loss:   0.20535085651874543
Testing MSE:    0.20535085651874543
Testing MAE:    0.3245427978515625

Epoch  6
Training loss:  0.14369173210561276
Training MSE:   0.14369173210561276
Training MAE:   0.26234697227478027
Testing loss:   0.09779513692259789
Testing MSE:    0.09779513692259789
Testing MAE:    0.21774310607910155

Epoch  7
Training loss:  0.12945922182649375
Training MSE:   0.12945922182649375
Training MAE:   0.2623338459014893
Testing loss:   0.16598685507774352
Testing MSE:    0.16598685507774352
Testing MAE:    0.34286705169677734

Epoch  8
Training loss:  0.14241044794917107
Training MSE:   0.14241044794917107
Training MAE:   0.28430881843566896
Testing loss:   0.49927406759262083
Testing MSE:    0.49927406759262083
Testing MAE:    0.6652228073120117

Epoch  9
Training loss:  0.13799901396483183
Training MSE:   0.13799901396483183
Training MAE:   0.2823237880706787
Testing loss:   0.3563662099838257
Testing MSE:    0.3563662099838257
Testing MAE:    0.5550260192871094

Training loss:  0.3581982150197029
Training MSE:   0.3581982150197029
Training MAE:   0.5557051788330079

Testing loss:  0.3563662099838257
Testing MSE:   0.3563662099838257
Testing MAE:   0.5550260192871094

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 20, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  11490.115907632828
Training MSE:   11490.115907632828
Training MAE:   36.187144860839844
Testing loss:   15.327297839355468
Testing MSE:    15.327297839355468
Testing MAE:    3.120820960998535

Epoch  1
Training loss:  11.620574608802796
Training MSE:   11.620574608802796
Training MAE:   2.708268542480469
Testing loss:   7.900021472930908
Testing MSE:    7.900021472930908
Testing MAE:    2.236355242919922

Epoch  2
Training loss:  4.986459628963471
Training MSE:   4.986459628963471
Training MAE:   1.7374849452972412
Testing loss:   2.6485749307632447
Testing MSE:    2.6485749307632447
Testing MAE:    1.277476190185547

Epoch  3
Training loss:  1.4564578342199326
Training MSE:   1.4564578342199326
Training MAE:   0.9119374042510986
Testing loss:   0.7121888569831848
Testing MSE:    0.7121888569831848
Testing MAE:    0.646814875793457

Epoch  4
Training loss:  0.4324918276965618
Training MSE:   0.4324918276965618
Training MAE:   0.4908469566345215
Testing loss:   0.2501505826473236
Testing MSE:    0.2501505826473236
Testing MAE:    0.3640993850708008

Epoch  5
Training loss:  0.1816490305095911
Training MSE:   0.1816490305095911
Training MAE:   0.30542542724609373
Testing loss:   0.11008998011350632
Testing MSE:    0.11008998011350632
Testing MAE:    0.2327376480102539

Epoch  6
Training loss:  0.10578858567923308
Training MSE:   0.10578858567923308
Training MAE:   0.22624305572509765
Testing loss:   0.17965499541759491
Testing MSE:    0.17965499541759491
Testing MAE:    0.3441531692504883

Epoch  7
Training loss:  0.10182171226143837
Training MSE:   0.10182171226143837
Training MAE:   0.23172859573364257
Testing loss:   0.1348579601764679
Testing MSE:    0.1348579601764679
Testing MAE:    0.29485941619873046

Epoch  8
Training loss:  0.10927437593191862
Training MSE:   0.10927437593191862
Training MAE:   0.24930274658203125
Testing loss:   0.08947968462705612
Testing MSE:    0.08947968462705612
Testing MAE:    0.22437940216064453

Epoch  9
Training loss:  0.11317446631938219
Training MSE:   0.11317446631938219
Training MAE:   0.25531371726989743
Testing loss:   0.05847825758457184
Testing MSE:    0.05847825758457184
Testing MAE:    0.17942535552978517

Training loss:  0.059319016894698144
Training MSE:   0.059319016894698144
Training MAE:   0.1786676685333252

Testing loss:  0.05847825758457184
Testing MSE:   0.05847825758457184
Testing MAE:   0.17942535552978517

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 20, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  11977.424847686005
Training MSE:   11977.424847686005
Training MAE:   39.87725937347412
Testing loss:   30.349097625732423
Testing MSE:    30.349097625732423
Testing MAE:    4.273713993835449

Epoch  1
Training loss:  14.15880492324829
Training MSE:   14.15880492324829
Training MAE:   2.843360506439209
Testing loss:   6.27184514465332
Testing MSE:    6.27184514465332
Testing MAE:    1.9455183883666993

Epoch  2
Training loss:  3.557653032779694
Training MSE:   3.557653032779694
Training MAE:   1.4311836029052734
Testing loss:   1.4819444816589356
Testing MSE:    1.4819444816589356
Testing MAE:    0.9266672332763672

Epoch  3
Training loss:  0.852727530169487
Training MSE:   0.852727530169487
Training MAE:   0.6884618137359619
Testing loss:   0.4041911011695862
Testing MSE:    0.4041911011695862
Testing MAE:    0.4666257873535156

Epoch  4
Training loss:  0.3086215559929609
Training MSE:   0.3086215559929609
Training MAE:   0.4075994422912598
Testing loss:   0.2869260495185852
Testing MSE:    0.2869260495185852
Testing MAE:    0.39601095581054685

Epoch  5
Training loss:  0.20122835924327373
Training MSE:   0.20122835924327373
Training MAE:   0.3221773868560791
Testing loss:   0.1519008302092552
Testing MSE:    0.1519008302092552
Testing MAE:    0.2700777053833008

Epoch  6
Training loss:  0.20516546440273523
Training MSE:   0.20516546440273523
Training MAE:   0.33536140060424807
Testing loss:   0.09651500061154365
Testing MSE:    0.09651500061154365
Testing MAE:    0.2109791976928711

Epoch  7
Training loss:  0.17630521671921015
Training MSE:   0.17630521671921015
Training MAE:   0.3133268142700195
Testing loss:   0.09840976125597954
Testing MSE:    0.09840976125597954
Testing MAE:    0.2137216781616211

Epoch  8
Training loss:  0.1872076883494854
Training MSE:   0.1872076883494854
Training MAE:   0.33136188163757324
Testing loss:   0.20803147897720337
Testing MSE:    0.20803147897720337
Testing MAE:    0.39113314208984373

Epoch  9
Training loss:  0.1727040975421667
Training MSE:   0.1727040975421667
Training MAE:   0.3178596706390381
Testing loss:   0.3445210631847382
Testing MSE:    0.3445210631847382
Testing MAE:    0.5365546203613282

Training loss:  0.3422343351483345
Training MSE:   0.3422343351483345
Training MAE:   0.5375220695495605

Testing loss:  0.3445210631847382
Testing MSE:   0.3445210631847382
Testing MAE:   0.5365546203613282

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 20, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  14187.71150871048
Training MSE:   14187.71150871048
Training MAE:   43.47410470237732
Testing loss:   21.67627368927002
Testing MSE:    21.67627368927002
Testing MAE:    3.6677471466064455

Epoch  1
Training loss:  13.881531750106811
Training MSE:   13.881531750106811
Training MAE:   2.9005154235839843
Testing loss:   8.286844554901123
Testing MSE:    8.286844554901123
Testing MAE:    2.244346469116211

Epoch  2
Training loss:  4.649636535167694
Training MSE:   4.649636535167694
Training MAE:   1.6508188804626465
Testing loss:   2.059191456413269
Testing MSE:    2.059191456413269
Testing MAE:    1.1032370208740234

Epoch  3
Training loss:  1.0058111139535904
Training MSE:   1.0058111139535904
Training MAE:   0.7519560684204102
Testing loss:   0.4620980700969696
Testing MSE:    0.4620980700969696
Testing MAE:    0.5384006805419922

Epoch  4
Training loss:  0.2616962565422058
Training MSE:   0.2616962565422058
Training MAE:   0.37709390182495117
Testing loss:   0.1473740680336952
Testing MSE:    0.1473740680336952
Testing MAE:    0.2731116638183594

Epoch  5
Training loss:  0.13015564595162868
Training MSE:   0.13015564595162868
Training MAE:   0.2526816318511963
Testing loss:   0.08257931439876556
Testing MSE:    0.08257931439876556
Testing MAE:    0.19629689636230468

Epoch  6
Training loss:  0.09953845014423132
Training MSE:   0.09953845014423132
Training MAE:   0.22579295654296874
Testing loss:   0.11248008706569672
Testing MSE:    0.11248008706569672
Testing MAE:    0.2586372955322266

Epoch  7
Training loss:  0.09306897515058518
Training MSE:   0.09306897515058518
Training MAE:   0.22598508377075197
Testing loss:   0.2049891035079956
Testing MSE:    0.2049891035079956
Testing MAE:    0.40359663543701174

Epoch  8
Training loss:  0.11361521972566843
Training MSE:   0.11361521972566843
Training MAE:   0.2579988872528076
Testing loss:   0.05554164437055588
Testing MSE:    0.05554164437055588
Testing MAE:    0.1768767059326172

Epoch  9
Training loss:  0.10205831218063831
Training MSE:   0.10205831218063831
Training MAE:   0.2435302375793457
Testing loss:   0.04708202783465385
Testing MSE:    0.04708202783465385
Testing MAE:    0.15402919616699218

Training loss:  0.04911140973642469
Training MSE:   0.04911140973642469
Training MAE:   0.1542722225189209

Testing loss:  0.04708202783465385
Testing MSE:   0.04708202783465385
Testing MAE:   0.15402919616699218

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 20, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10762.616618257141
Training MSE:   10762.616618257141
Training MAE:   36.621756143951416
Testing loss:   3.520572799682617
Testing MSE:    3.520572799682617
Testing MAE:    1.4627592498779296

Epoch  1
Training loss:  1.0242988867521285
Training MSE:   1.0242988867521285
Training MAE:   0.7315448177337647
Testing loss:   0.26722304167747496
Testing MSE:    0.26722304167747496
Testing MAE:    0.38834923248291014

Epoch  2
Training loss:  0.16545467397272587
Training MSE:   0.16545467397272587
Training MAE:   0.2871087070465088
Testing loss:   0.09881443885564804
Testing MSE:    0.09881443885564804
Testing MAE:    0.2162012451171875

Epoch  3
Training loss:  0.09489512331485749
Training MSE:   0.09489512331485749
Training MAE:   0.21570640602111815
Testing loss:   0.08908090495467186
Testing MSE:    0.08908090495467186
Testing MAE:    0.20563675842285156

Epoch  4
Training loss:  0.08098977739810943
Training MSE:   0.08098977739810943
Training MAE:   0.2031931629180908
Testing loss:   0.060707433211803434
Testing MSE:    0.060707433211803434
Testing MAE:    0.16886559600830078

Epoch  5
Training loss:  0.07610439114421606
Training MSE:   0.07610439114421606
Training MAE:   0.1995763111114502
Testing loss:   0.10637132960557938
Testing MSE:    0.10637132960557938
Testing MAE:    0.26703184814453124

Epoch  6
Training loss:  0.08059722948595882
Training MSE:   0.08059722948595882
Training MAE:   0.20896464614868165
Testing loss:   0.10952081661224365
Testing MSE:    0.10952081661224365
Testing MAE:    0.27450361328125

Epoch  7
Training loss:  0.09400267717316747
Training MSE:   0.09400267717316747
Training MAE:   0.2333488124847412
Testing loss:   0.06274637873768807
Testing MSE:    0.06274637873768807
Testing MAE:    0.1807488998413086

Epoch  8
Training loss:  0.10312841468006373
Training MSE:   0.10312841468006373
Training MAE:   0.2469875331878662
Testing loss:   0.042006498005986215
Testing MSE:    0.042006498005986215
Testing MAE:    0.1453266616821289

Epoch  9
Training loss:  0.10270691497027874
Training MSE:   0.10270691497027874
Training MAE:   0.24638603591918945
Testing loss:   0.04534118391871452
Testing MSE:    0.04534118391871452
Testing MAE:    0.15330140380859375

Training loss:  0.046303893737494946
Training MSE:   0.046303893737494946
Training MAE:   0.15405296936035157

Testing loss:  0.04534118391871452
Testing MSE:   0.04534118391871452
Testing MAE:   0.15330140380859375

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 20, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  24606.89650446167
Training MSE:   24606.89650446167
Training MAE:   69.65920501632691
Testing loss:   40.21734733276367
Testing MSE:    40.21734733276367
Testing MAE:    4.835842417907715

Epoch  1
Training loss:  35.45314675598144
Training MSE:   35.45314675598144
Training MAE:   4.5184283908844
Testing loss:   27.915375299072267
Testing MSE:    27.915375299072267
Testing MAE:    4.003569807434082

Epoch  2
Training loss:  22.124890056228637
Training MSE:   22.124890056228637
Training MAE:   3.5451728778839113
Testing loss:   15.070984866333008
Testing MSE:    15.070984866333008
Testing MAE:    2.943300184631348

Epoch  3
Training loss:  10.465484685325622
Training MSE:   10.465484685325622
Training MAE:   2.407385026168823
Testing loss:   5.904128791046142
Testing MSE:    5.904128791046142
Testing MAE:    1.7917464233398437

Epoch  4
Training loss:  3.4384527916908265
Training MSE:   3.4384527916908265
Training MAE:   1.3590105754852295
Testing loss:   1.64635360622406
Testing MSE:    1.64635360622406
Testing MAE:    0.9399520538330078

Epoch  5
Training loss:  0.7754915048122406
Training MSE:   0.7754915048122406
Training MAE:   0.6310505588531494
Testing loss:   0.34574094331264493
Testing MSE:    0.34574094331264493
Testing MAE:    0.4110579025268555

Epoch  6
Training loss:  0.22218223449885846
Training MSE:   0.22218223449885846
Training MAE:   0.3238923454284668
Testing loss:   0.1476258046388626
Testing MSE:    0.1476258046388626
Testing MAE:    0.2671541275024414

Epoch  7
Training loss:  0.1358483602836728
Training MSE:   0.1358483602836728
Training MAE:   0.2495619873046875
Testing loss:   0.10163323752880096
Testing MSE:    0.10163323752880096
Testing MAE:    0.2061313446044922

Epoch  8
Training loss:  0.11531491517573594
Training MSE:   0.11531491517573594
Training MAE:   0.2400072696685791
Testing loss:   0.07970263745188713
Testing MSE:    0.07970263745188713
Testing MAE:    0.1879616455078125

Epoch  9
Training loss:  0.1087490136384964
Training MSE:   0.1087490136384964
Training MAE:   0.24128947257995606
Testing loss:   0.06408328425884247
Testing MSE:    0.06408328425884247
Testing MAE:    0.17481306915283204

Training loss:  0.06388235847502947
Training MSE:   0.06388235847502947
Training MAE:   0.17278839645385743

Testing loss:  0.06408328425884247
Testing MSE:   0.06408328425884247
Testing MAE:   0.17481306915283204

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 15, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  6210.599702904892
Training MSE:   6210.599702904892
Training MAE:   23.317901208114623
Testing loss:   9.946246099090576
Testing MSE:    9.946246099090576
Testing MAE:    2.4017623794555663

Epoch  1
Training loss:  2.9236007486343385
Training MSE:   2.9236007486343385
Training MAE:   1.1796097820281983
Testing loss:   1.1155712025642395
Testing MSE:    1.1155712025642395
Testing MAE:    0.7802422409057617

Epoch  2
Training loss:  0.8321782095789909
Training MSE:   0.8321782095789909
Training MAE:   0.6747281215667724
Testing loss:   0.5126551523685455
Testing MSE:    0.5126551523685455
Testing MAE:    0.5069955535888672

Epoch  3
Training loss:  0.38965945581793787
Training MSE:   0.38965945581793787
Training MAE:   0.4411091106414795
Testing loss:   0.23612491545677186
Testing MSE:    0.23612491545677186
Testing MAE:    0.31326560668945314

Epoch  4
Training loss:  0.21379029656648635
Training MSE:   0.21379029656648635
Training MAE:   0.3085368263244629
Testing loss:   0.15531730186939238
Testing MSE:    0.15531730186939238
Testing MAE:    0.261973828125

Epoch  5
Training loss:  0.16736314204335212
Training MSE:   0.16736314204335212
Training MAE:   0.2849204124450684
Testing loss:   0.11191928685903549
Testing MSE:    0.11191928685903549
Testing MAE:    0.20542198791503907

Epoch  6
Training loss:  0.1692792991682887
Training MSE:   0.1692792991682887
Training MAE:   0.30241795120239257
Testing loss:   0.3975577990055084
Testing MSE:    0.3975577990055084
Testing MAE:    0.5643653884887695

Epoch  7
Training loss:  0.19908610922694206
Training MSE:   0.19908610922694206
Training MAE:   0.33939293022155764
Testing loss:   0.0938621002972126
Testing MSE:    0.0938621002972126
Testing MAE:    0.21762909240722655

Epoch  8
Training loss:  0.19707223149091005
Training MSE:   0.19707223149091005
Training MAE:   0.33851779747009275
Testing loss:   0.2560897329807281
Testing MSE:    0.2560897329807281
Testing MAE:    0.4512214767456055

Epoch  9
Training loss:  0.160162242628634
Training MSE:   0.160162242628634
Training MAE:   0.3087824478149414
Testing loss:   0.06066779118180275
Testing MSE:    0.06066779118180275
Testing MAE:    0.16782021484375

Training loss:  0.05950274873375893
Training MSE:   0.05950274873375893
Training MAE:   0.16639826316833495

Testing loss:  0.06066779118180275
Testing MSE:   0.06066779118180275
Testing MAE:   0.16782021484375

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 15, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  5879.18868948822
Training MSE:   5879.18868948822
Training MAE:   25.40268992576599
Testing loss:   43.57481180419922
Testing MSE:    43.57481180419922
Testing MAE:    5.183805375671387

Epoch  1
Training loss:  30.473274101257324
Training MSE:   30.473274101257324
Training MAE:   4.293267647171021
Testing loss:   16.84224337463379
Testing MSE:    16.84224337463379
Testing MAE:    3.2356075149536134

Epoch  2
Training loss:  9.619396310138702
Training MSE:   9.619396310138702
Training MAE:   2.376297978210449
Testing loss:   4.023593856430054
Testing MSE:    4.023593856430054
Testing MAE:    1.5582108993530273

Epoch  3
Training loss:  1.790089049434662
Training MSE:   1.790089049434662
Training MAE:   0.9968646400451661
Testing loss:   0.5776330373287201
Testing MSE:    0.5776330373287201
Testing MAE:    0.576313264465332

Epoch  4
Training loss:  0.3515661655336618
Training MSE:   0.3515661655336618
Training MAE:   0.4308504852294922
Testing loss:   0.1781864775300026
Testing MSE:    0.1781864775300026
Testing MAE:    0.29514012603759765

Epoch  5
Training loss:  0.14373257985711096
Training MSE:   0.14373257985711096
Training MAE:   0.26573608627319334
Testing loss:   0.13692517697811127
Testing MSE:    0.13692517697811127
Testing MAE:    0.2925570068359375

Epoch  6
Training loss:  0.12022116327881813
Training MSE:   0.12022116327881813
Training MAE:   0.25052357749938964
Testing loss:   0.06783884171247483
Testing MSE:    0.06783884171247483
Testing MAE:    0.17766672668457031

Epoch  7
Training loss:  0.10930298615843058
Training MSE:   0.10930298615843058
Training MAE:   0.2450221652984619
Testing loss:   0.12878209841251373
Testing MSE:    0.12878209841251373
Testing MAE:    0.28710853424072263

Epoch  8
Training loss:  0.13359026744365693
Training MSE:   0.13359026744365693
Training MAE:   0.2798824932098389
Testing loss:   0.43396713695526123
Testing MSE:    0.43396713695526123
Testing MAE:    0.6231081756591796

Epoch  9
Training loss:  0.12244297454878687
Training MSE:   0.12244297454878687
Training MAE:   0.26814220504760744
Testing loss:   0.13146099253892898
Testing MSE:    0.13146099253892898
Testing MAE:    0.3112249725341797

Training loss:  0.13259914047122
Training MSE:   0.13259914047122
Training MAE:   0.31244671211242675

Testing loss:  0.13146099253892898
Testing MSE:   0.13146099253892898
Testing MAE:   0.3112249725341797

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 15, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7601.731482805252
Training MSE:   7601.731482805252
Training MAE:   26.50163768043518
Testing loss:   3.3104502361297605
Testing MSE:    3.3104502361297605
Testing MAE:    1.3815026092529297

Epoch  1
Training loss:  2.4286109049797058
Training MSE:   2.4286109049797058
Training MAE:   1.1878334239959716
Testing loss:   1.3296644704818725
Testing MSE:    1.3296644704818725
Testing MAE:    0.8939485488891602

Epoch  2
Training loss:  0.8176285421252251
Training MSE:   0.8176285421252251
Training MAE:   0.690609041595459
Testing loss:   0.4496442612171173
Testing MSE:    0.4496442612171173
Testing MAE:    0.5143630447387695

Epoch  3
Training loss:  0.3020455174922943
Training MSE:   0.3020455174922943
Training MAE:   0.41587645797729494
Testing loss:   0.251393962931633
Testing MSE:    0.251393962931633
Testing MAE:    0.39960286865234373

Epoch  4
Training loss:  0.15663982615172864
Training MSE:   0.15663982615172864
Training MAE:   0.2874581272125244
Testing loss:   0.11190322633385658
Testing MSE:    0.11190322633385658
Testing MAE:    0.22674390411376952

Epoch  5
Training loss:  0.11519701599478721
Training MSE:   0.11519701599478721
Training MAE:   0.2400638572692871
Testing loss:   0.07851004063487053
Testing MSE:    0.07851004063487053
Testing MAE:    0.18689056854248046

Epoch  6
Training loss:  0.1129091825440526
Training MSE:   0.1129091825440526
Training MAE:   0.24247442283630372
Testing loss:   0.19248679566383362
Testing MSE:    0.19248679566383362
Testing MAE:    0.36226087036132815

Epoch  7
Training loss:  0.11507344205975532
Training MSE:   0.11507344205975532
Training MAE:   0.2521727893829346
Testing loss:   0.4413808143615723
Testing MSE:    0.4413808143615723
Testing MAE:    0.6211025985717773

Epoch  8
Training loss:  0.14024438314139842
Training MSE:   0.14024438314139842
Training MAE:   0.28501486167907714
Testing loss:   0.08636930470466614
Testing MSE:    0.08636930470466614
Testing MAE:    0.21864185638427736

Epoch  9
Training loss:  0.12983847862631082
Training MSE:   0.12983847862631082
Training MAE:   0.2768588359832764
Testing loss:   0.11687572696208953
Testing MSE:    0.11687572696208953
Testing MAE:    0.27316754150390626

Training loss:  0.11766920331120491
Training MSE:   0.11766920331120491
Training MAE:   0.27276519622802736

Testing loss:  0.11687572696208953
Testing MSE:   0.11687572696208953
Testing MAE:   0.27316754150390626

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 15, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  19322.082545646284
Training MSE:   19322.082545646284
Training MAE:   56.02059635925293
Testing loss:   4.030270710754395
Testing MSE:    4.030270710754395
Testing MAE:    1.5780960983276366

Epoch  1
Training loss:  2.778135813856125
Training MSE:   2.778135813856125
Training MAE:   1.311255082321167
Testing loss:   1.8888573905944823
Testing MSE:    1.8888573905944823
Testing MAE:    1.085636622619629

Epoch  2
Training loss:  1.4803923316955567
Training MSE:   1.4803923316955567
Training MAE:   0.9592534336090088
Testing loss:   1.006405359840393
Testing MSE:    1.006405359840393
Testing MAE:    0.794196369934082

Epoch  3
Training loss:  0.7737804039001465
Training MSE:   0.7737804039001465
Training MAE:   0.6864883438110352
Testing loss:   0.5545583309173584
Testing MSE:    0.5545583309173584
Testing MAE:    0.5746256759643554

Epoch  4
Training loss:  0.38257773166894915
Training MSE:   0.38257773166894915
Training MAE:   0.4675524242401123
Testing loss:   0.23612471551895142
Testing MSE:    0.23612471551895142
Testing MAE:    0.36152078704833984

Epoch  5
Training loss:  0.20483500741422175
Training MSE:   0.20483500741422175
Training MAE:   0.3231866718292236
Testing loss:   0.20174628443717957
Testing MSE:    0.20174628443717957
Testing MAE:    0.3605696166992188

Epoch  6
Training loss:  0.12557615435421468
Training MSE:   0.12557615435421468
Training MAE:   0.24337351989746095
Testing loss:   0.10200006424188614
Testing MSE:    0.10200006424188614
Testing MAE:    0.23120910034179687

Epoch  7
Training loss:  0.11284718491286039
Training MSE:   0.11284718491286039
Training MAE:   0.24036709213256835
Testing loss:   0.12621412252187728
Testing MSE:    0.12621412252187728
Testing MAE:    0.27233519744873047

Epoch  8
Training loss:  0.10464222034662962
Training MSE:   0.10464222034662962
Training MAE:   0.23730697898864747
Testing loss:   0.21646267442703246
Testing MSE:    0.21646267442703246
Testing MAE:    0.4141760528564453

Epoch  9
Training loss:  0.1115079688578844
Training MSE:   0.1115079688578844
Training MAE:   0.2502777317047119
Testing loss:   0.10070753260850906
Testing MSE:    0.10070753260850906
Testing MAE:    0.25893863067626954

Training loss:  0.10088634433448315
Training MSE:   0.10088634433448315
Training MAE:   0.2591403522491455

Testing loss:  0.10070753260850906
Testing MSE:   0.10070753260850906
Testing MAE:   0.25893863067626954

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 15, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  5506.14586065979
Training MSE:   5506.14586065979
Training MAE:   24.90066051712036
Testing loss:   26.82186342468262
Testing MSE:    26.82186342468262
Testing MAE:    4.116171072387695

Epoch  1
Training loss:  18.272502098083496
Training MSE:   18.272502098083496
Training MAE:   3.353476636505127
Testing loss:   12.390240730285644
Testing MSE:    12.390240730285644
Testing MAE:    2.7533603729248046

Epoch  2
Training loss:  7.872023578453064
Training MSE:   7.872023578453064
Training MAE:   2.1972410133361815
Testing loss:   4.362996183776856
Testing MSE:    4.362996183776856
Testing MAE:    1.6310152938842772

Epoch  3
Training loss:  2.4432071968078612
Training MSE:   2.4432071968078612
Training MAE:   1.2093151691436768
Testing loss:   1.0072694810867309
Testing MSE:    1.0072694810867309
Testing MAE:    0.7880631469726562

Epoch  4
Training loss:  0.6049858277320862
Training MSE:   0.6049858277320862
Training MAE:   0.590747386932373
Testing loss:   0.23791157462596893
Testing MSE:    0.23791157462596893
Testing MAE:    0.3693964126586914

Epoch  5
Training loss:  0.18229345794320106
Training MSE:   0.18229345794320106
Training MAE:   0.3177961711883545
Testing loss:   0.22506399023532866
Testing MSE:    0.22506399023532866
Testing MAE:    0.39660669708251955

Epoch  6
Training loss:  0.13653707920312882
Training MSE:   0.13653707920312882
Training MAE:   0.27462133827209473
Testing loss:   0.39422376294136047
Testing MSE:    0.39422376294136047
Testing MAE:    0.563698307800293

Epoch  7
Training loss:  0.13570041480064393
Training MSE:   0.13570041480064393
Training MAE:   0.27605924339294435
Testing loss:   0.15302021615505218
Testing MSE:    0.15302021615505218
Testing MAE:    0.3098806411743164

Epoch  8
Training loss:  0.1480476075708866
Training MSE:   0.1480476075708866
Training MAE:   0.29318746223449704
Testing loss:   0.14843221356868744
Testing MSE:    0.14843221356868744
Testing MAE:    0.3074660629272461

Epoch  9
Training loss:  0.13916230537593366
Training MSE:   0.13916230537593366
Training MAE:   0.28484192695617677
Testing loss:   0.06353137172460556
Testing MSE:    0.06353137172460556
Testing MAE:    0.1810268493652344

Training loss:  0.06521535972952842
Training MSE:   0.06521535972952842
Training MAE:   0.17981501693725585

Testing loss:  0.06353137172460556
Testing MSE:   0.06353137172460556
Testing MAE:   0.1810268493652344

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 10, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10666.022887814332
Training MSE:   10666.022887814332
Training MAE:   38.50469906730652
Testing loss:   79.42208400878906
Testing MSE:    79.42208400878906
Testing MAE:    6.956988078308106

Epoch  1
Training loss:  49.34400772628784
Training MSE:   49.34400772628784
Training MAE:   5.396085359954834
Testing loss:   16.955502194213867
Testing MSE:    16.955502194213867
Testing MAE:    3.2636551666259765

Epoch  2
Training loss:  4.867315933585167
Training MSE:   4.867315933585167
Training MAE:   1.5696943599700928
Testing loss:   0.5735418724536896
Testing MSE:    0.5735418724536896
Testing MAE:    0.5871858978271485

Epoch  3
Training loss:  0.22690675677657127
Training MSE:   0.22690675677657127
Training MAE:   0.3528149402618408
Testing loss:   0.13903172512054443
Testing MSE:    0.13903172512054443
Testing MAE:    0.28016680297851565

Epoch  4
Training loss:  0.09044363669604064
Training MSE:   0.09044363669604064
Training MAE:   0.22753772850036622
Testing loss:   0.07285892221927642
Testing MSE:    0.07285892221927642
Testing MAE:    0.19225200958251953

Epoch  5
Training loss:  0.08737667132765055
Training MSE:   0.08737667132765055
Training MAE:   0.22518606834411622
Testing loss:   0.06013414427638054
Testing MSE:    0.06013414427638054
Testing MAE:    0.17365975036621092

Epoch  6
Training loss:  0.0911787540793419
Training MSE:   0.0911787540793419
Training MAE:   0.23145654220581055
Testing loss:   0.32200820784568784
Testing MSE:    0.32200820784568784
Testing MAE:    0.5221539733886719

Epoch  7
Training loss:  0.09662558131814003
Training MSE:   0.09662558131814003
Training MAE:   0.23841930541992187
Testing loss:   0.14280043709278106
Testing MSE:    0.14280043709278106
Testing MAE:    0.3252098388671875

Epoch  8
Training loss:  0.11206149060577154
Training MSE:   0.11206149060577154
Training MAE:   0.2601571464538574
Testing loss:   0.32051555829048156
Testing MSE:    0.32051555829048156
Testing MAE:    0.529678662109375

Epoch  9
Training loss:  0.11411547030061484
Training MSE:   0.11411547030061484
Training MAE:   0.2615250675201416
Testing loss:   0.040448183122277256
Testing MSE:    0.040448183122277256
Testing MAE:    0.13850248718261718

Training loss:  0.037744371777027844
Training MSE:   0.037744371777027844
Training MAE:   0.13730129585266113

Testing loss:  0.040448183122277256
Testing MSE:   0.040448183122277256
Testing MAE:   0.13850248718261718

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 10, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10929.69068139801
Training MSE:   10929.69068139801
Training MAE:   38.29195746269226
Testing loss:   58.15577388305664
Testing MSE:    58.15577388305664
Testing MAE:    5.964152238464355

Epoch  1
Training loss:  15.353122107052803
Training MSE:   15.353122107052803
Training MAE:   2.498600249481201
Testing loss:   0.6019934180259705
Testing MSE:    0.6019934180259705
Testing MAE:    0.592418441772461

Epoch  2
Training loss:  0.27615584838986396
Training MSE:   0.27615584838986396
Training MAE:   0.36928276596069337
Testing loss:   0.12049565459489822
Testing MSE:    0.12049565459489822
Testing MAE:    0.2429474594116211

Epoch  3
Training loss:  0.11430060428529978
Training MSE:   0.11430060428529978
Training MAE:   0.23562115783691406
Testing loss:   0.09264792884588241
Testing MSE:    0.09264792884588241
Testing MAE:    0.21926671600341796

Epoch  4
Training loss:  0.08509910200685263
Training MSE:   0.08509910200685263
Training MAE:   0.20910504302978516
Testing loss:   0.06073486225605011
Testing MSE:    0.06073486225605011
Testing MAE:    0.1770255828857422

Epoch  5
Training loss:  0.07788379183560609
Training MSE:   0.07788379183560609
Training MAE:   0.20492754707336425
Testing loss:   0.09775058398246765
Testing MSE:    0.09775058398246765
Testing MAE:    0.25480774841308595

Epoch  6
Training loss:  0.08093731279447675
Training MSE:   0.08093731279447675
Training MAE:   0.21369516105651856
Testing loss:   0.1293911515235901
Testing MSE:    0.1293911515235901
Testing MAE:    0.29840064544677736

Epoch  7
Training loss:  0.08264038977175951
Training MSE:   0.08264038977175951
Training MAE:   0.21633697471618651
Testing loss:   0.09850142263174057
Testing MSE:    0.09850142263174057
Testing MAE:    0.25976961822509764

Epoch  8
Training loss:  0.09441388814374804
Training MSE:   0.09441388814374804
Training MAE:   0.2337690773010254
Testing loss:   0.13338519008159638
Testing MSE:    0.13338519008159638
Testing MAE:    0.3181155364990234

Epoch  9
Training loss:  0.09668287864029408
Training MSE:   0.09668287864029408
Training MAE:   0.2330816982269287
Testing loss:   0.047195925468206404
Testing MSE:    0.047195925468206404
Testing MAE:    0.1541615005493164

Training loss:  0.049107604171335695
Training MSE:   0.049107604171335695
Training MAE:   0.15462441444396974

Testing loss:  0.047195925468206404
Testing MSE:   0.047195925468206404
Testing MAE:   0.1541615005493164

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 10, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  6716.267927957535
Training MSE:   6716.267927957535
Training MAE:   25.844511165618897
Testing loss:   10.19679811553955
Testing MSE:    10.19679811553955
Testing MAE:    2.5392387603759765

Epoch  1
Training loss:  5.0309203669548035
Training MSE:   5.0309203669548035
Training MAE:   1.7220415992736817
Testing loss:   2.077080875968933
Testing MSE:    2.077080875968933
Testing MAE:    1.1304248168945312

Epoch  2
Training loss:  1.207232422375679
Training MSE:   1.207232422375679
Training MAE:   0.848510330581665
Testing loss:   0.43941367712020873
Testing MSE:    0.43941367712020873
Testing MAE:    0.517464599609375

Epoch  3
Training loss:  0.28494407172501085
Training MSE:   0.28494407172501085
Training MAE:   0.40140205268859863
Testing loss:   0.1521520240545273
Testing MSE:    0.1521520240545273
Testing MAE:    0.2873733413696289

Epoch  4
Training loss:  0.1391165070414543
Training MSE:   0.1391165070414543
Training MAE:   0.2703594360351563
Testing loss:   0.11782603925466538
Testing MSE:    0.11782603925466538
Testing MAE:    0.2499999710083008

Epoch  5
Training loss:  0.13970470373779537
Training MSE:   0.13970470373779537
Training MAE:   0.2774282375335693
Testing loss:   0.07581825949549675
Testing MSE:    0.07581825949549675
Testing MAE:    0.20166268615722657

Epoch  6
Training loss:  0.13605457821935416
Training MSE:   0.13605457821935416
Training MAE:   0.2811493186950684
Testing loss:   0.06585319746136666
Testing MSE:    0.06585319746136666
Testing MAE:    0.18781055908203126

Epoch  7
Training loss:  0.15824936878681184
Training MSE:   0.15824936878681184
Training MAE:   0.3011594669342041
Testing loss:   0.06041055927276611
Testing MSE:    0.06041055927276611
Testing MAE:    0.17179711303710937

Epoch  8
Training loss:  0.15071233802437783
Training MSE:   0.15071233802437783
Training MAE:   0.30031042060852053
Testing loss:   0.09400136923789978
Testing MSE:    0.09400136923789978
Testing MAE:    0.24892425231933593

Epoch  9
Training loss:  0.1301634027570486
Training MSE:   0.1301634027570486
Training MAE:   0.2761003349304199
Testing loss:   0.08520200572013854
Testing MSE:    0.08520200572013854
Testing MAE:    0.2382315979003906

Training loss:  0.08567130282223225
Training MSE:   0.08567130282223225
Training MAE:   0.23855825958251953

Testing loss:  0.08520200572013854
Testing MSE:   0.08520200572013854
Testing MAE:   0.2382315979003906

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 10, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  12138.503886276245
Training MSE:   12138.503886276245
Training MAE:   43.00159502105713
Testing loss:   75.87251688232422
Testing MSE:    75.87251688232422
Testing MAE:    6.989507653808594

Epoch  1
Training loss:  39.613683500289916
Training MSE:   39.613683500289916
Training MAE:   4.830006669616699
Testing loss:   10.345431416320801
Testing MSE:    10.345431416320801
Testing MAE:    2.5148811218261717

Epoch  2
Training loss:  3.702105909729004
Training MSE:   3.702105909729004
Training MAE:   1.4005659217834472
Testing loss:   0.6843196186065674
Testing MSE:    0.6843196186065674
Testing MAE:    0.6362926345825195

Epoch  3
Training loss:  0.27226175058186053
Training MSE:   0.27226175058186053
Training MAE:   0.37012914123535157
Testing loss:   0.10588629680871964
Testing MSE:    0.10588629680871964
Testing MAE:    0.23244279327392578

Epoch  4
Training loss:  0.09364129749685526
Training MSE:   0.09364129749685526
Training MAE:   0.21274086151123048
Testing loss:   0.09277078640460969
Testing MSE:    0.09277078640460969
Testing MAE:    0.21957235260009766

Epoch  5
Training loss:  0.07867650969922542
Training MSE:   0.07867650969922542
Training MAE:   0.2005541717529297
Testing loss:   0.06481375997662545
Testing MSE:    0.06481375997662545
Testing MAE:    0.18766007232666015

Epoch  6
Training loss:  0.08360725479573011
Training MSE:   0.08360725479573011
Training MAE:   0.21420226974487305
Testing loss:   0.06010843825936318
Testing MSE:    0.06010843825936318
Testing MAE:    0.17285774993896486

Epoch  7
Training loss:  0.0906059767305851
Training MSE:   0.0906059767305851
Training MAE:   0.22539489669799806
Testing loss:   0.05011369608640671
Testing MSE:    0.05011369608640671
Testing MAE:    0.15781877136230468

Epoch  8
Training loss:  0.1045544862061739
Training MSE:   0.1045544862061739
Training MAE:   0.24608753356933594
Testing loss:   0.054248072862625125
Testing MSE:    0.054248072862625125
Testing MAE:    0.16689175415039062

Epoch  9
Training loss:  0.09274637118428945
Training MSE:   0.09274637118428945
Training MAE:   0.23318829383850098
Testing loss:   0.05911121165752411
Testing MSE:    0.05911121165752411
Testing MAE:    0.18765682830810546

Training loss:  0.059171800690889356
Training MSE:   0.059171800690889356
Training MAE:   0.18711751403808594

Testing loss:  0.05911121165752411
Testing MSE:   0.05911121165752411
Testing MAE:   0.18765682830810546

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 10, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  170293.593125
Training MSE:   170293.593125
Training MAE:   410.71489526367185
Testing loss:   169946.3208
Testing MSE:    169946.3208
Testing MAE:    410.3064048828125

Epoch  1
Training loss:  169184.1757625
Training MSE:   169184.1757625
Training MAE:   409.36305625
Testing loss:   168924.138
Testing MSE:    168924.138
Testing MAE:    409.0588770996094

Epoch  2
Training loss:  168164.6628875
Training MSE:   168164.6628875
Training MAE:   408.11559040527345
Testing loss:   167905.56725
Testing MSE:    167905.56725
Testing MAE:    407.81195771484374

Epoch  3
Training loss:  167148.2147625
Training MSE:   167148.2147625
Training MAE:   406.8689024658203
Testing loss:   166889.8336
Testing MSE:    166889.8336
Testing MAE:    406.5647056640625

Epoch  4
Training loss:  166135.1182875
Training MSE:   166135.1182875
Training MAE:   405.6217789794922
Testing loss:   165877.581775
Testing MSE:    165877.581775
Testing MAE:    405.3179103027344

Epoch  5
Training loss:  165125.27
Training MSE:   165125.27
Training MAE:   404.3748545410156
Testing loss:   164868.6122
Testing MSE:    164868.6122
Testing MAE:    404.07132817382814

Epoch  6
Training loss:  164118.4863125
Training MSE:   164118.4863125
Training MAE:   403.1284730712891
Testing loss:   163862.45455
Testing MSE:    163862.45455
Testing MAE:    402.82438110351563

Epoch  7
Training loss:  163114.7568625
Training MSE:   163114.7568625
Training MAE:   401.88152939453124
Testing loss:   162859.504525
Testing MSE:    162859.504525
Testing MAE:    401.57755493164063

Epoch  8
Training loss:  162114.2364625
Training MSE:   162114.2364625
Training MAE:   400.63470627441404
Testing loss:   161859.787075
Testing MSE:    161859.787075
Testing MAE:    400.330878125

Epoch  9
Training loss:  161116.8576125
Training MSE:   161116.8576125
Training MAE:   399.38805183105467
Testing loss:   160863.1773
Testing MSE:    160863.1773
Testing MAE:    399.08420737304687

Training loss:  160618.797
Training MSE:   160618.797
Training MAE:   398.76415888671875

Testing loss:  160863.1773
Testing MSE:   160863.1773
Testing MAE:   399.08420737304687

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 5, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9469.244400575257
Training MSE:   9469.244400575257
Training MAE:   32.64266640357971
Testing loss:   8.928523989105225
Testing MSE:    8.928523989105225
Testing MAE:    2.3112884078979494

Epoch  1
Training loss:  2.1793576383709907
Training MSE:   2.1793576383709907
Training MAE:   1.0535838176727295
Testing loss:   0.5197576043128967
Testing MSE:    0.5197576043128967
Testing MAE:    0.5585324035644531

Epoch  2
Training loss:  0.3071057024896145
Training MSE:   0.3071057024896145
Training MAE:   0.40416686935424806
Testing loss:   0.14608974047899245
Testing MSE:    0.14608974047899245
Testing MAE:    0.2792654327392578

Epoch  3
Training loss:  0.11990671564638615
Training MSE:   0.11990671564638615
Training MAE:   0.2355758632659912
Testing loss:   0.08315410529971123
Testing MSE:    0.08315410529971123
Testing MAE:    0.19579283294677735

Epoch  4
Training loss:  0.09175179306417704
Training MSE:   0.09175179306417704
Training MAE:   0.20940398330688476
Testing loss:   0.20083307032585143
Testing MSE:    0.20083307032585143
Testing MAE:    0.3880444793701172

Epoch  5
Training loss:  0.0872321448341012
Training MSE:   0.0872321448341012
Training MAE:   0.2113513401031494
Testing loss:   0.06633616650700569
Testing MSE:    0.06633616650700569
Testing MAE:    0.18802820892333985

Epoch  6
Training loss:  0.09543122856914997
Training MSE:   0.09543122856914997
Training MAE:   0.2296647933959961
Testing loss:   0.05654763256311417
Testing MSE:    0.05654763256311417
Testing MAE:    0.16707116241455078

Epoch  7
Training loss:  0.10820126834362745
Training MSE:   0.10820126834362745
Training MAE:   0.24915941848754883
Testing loss:   0.11672948271036149
Testing MSE:    0.11672948271036149
Testing MAE:    0.2738618911743164

Epoch  8
Training loss:  0.1170818674467504
Training MSE:   0.1170818674467504
Training MAE:   0.261654439163208
Testing loss:   0.052263614946603774
Testing MSE:    0.052263614946603774
Testing MAE:    0.1598502914428711

Epoch  9
Training loss:  0.10964943299889565
Training MSE:   0.10964943299889565
Training MAE:   0.252446467590332
Testing loss:   0.05377892439961433
Testing MSE:    0.05377892439961433
Testing MAE:    0.1648341567993164

Training loss:  0.056022767052054404
Training MSE:   0.056022767052054404
Training MAE:   0.16520565795898437

Testing loss:  0.05377892439961433
Testing MSE:   0.05377892439961433
Testing MAE:   0.1648341567993164

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 5, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  8118.122014112472
Training MSE:   8118.122014112472
Training MAE:   28.198105192947388
Testing loss:   6.86280804977417
Testing MSE:    6.86280804977417
Testing MAE:    2.059379098510742

Epoch  1
Training loss:  1.2906639097094537
Training MSE:   1.2906639097094537
Training MAE:   0.7572713302612305
Testing loss:   0.23358691303730011
Testing MSE:    0.23358691303730011
Testing MAE:    0.37076763763427734

Epoch  2
Training loss:  0.1499133377686143
Training MSE:   0.1499133377686143
Training MAE:   0.2571195873260498
Testing loss:   0.12111985902786254
Testing MSE:    0.12111985902786254
Testing MAE:    0.24845214385986328

Epoch  3
Training loss:  0.10179020745009183
Training MSE:   0.10179020745009183
Training MAE:   0.20907310943603516
Testing loss:   0.0748915562748909
Testing MSE:    0.0748915562748909
Testing MAE:    0.1810319839477539

Epoch  4
Training loss:  0.08269229191318155
Training MSE:   0.08269229191318155
Training MAE:   0.19651857376098633
Testing loss:   0.06168438793420792
Testing MSE:    0.06168438793420792
Testing MAE:    0.16567799682617187

Epoch  5
Training loss:  0.07361733909249306
Training MSE:   0.07361733909249306
Training MAE:   0.19248250770568848
Testing loss:   0.15052995491027832
Testing MSE:    0.15052995491027832
Testing MAE:    0.33430237884521485

Epoch  6
Training loss:  0.07522048950791359
Training MSE:   0.07522048950791359
Training MAE:   0.20073491134643554
Testing loss:   0.049788542681932446
Testing MSE:    0.049788542681932446
Testing MAE:    0.15197977600097656

Epoch  7
Training loss:  0.09125848373174668
Training MSE:   0.09125848373174668
Training MAE:   0.22831928443908692
Testing loss:   0.08727172820568085
Testing MSE:    0.08727172820568085
Testing MAE:    0.24032451324462892

Epoch  8
Training loss:  0.1120526613175869
Training MSE:   0.1120526613175869
Training MAE:   0.25719054222106935
Testing loss:   0.04223810421824455
Testing MSE:    0.04223810421824455
Testing MAE:    0.14327538604736328

Epoch  9
Training loss:  0.10868666482195258
Training MSE:   0.10868666482195258
Training MAE:   0.25534004859924314
Testing loss:   0.060909155052900316
Testing MSE:    0.060909155052900316
Testing MAE:    0.19320254669189454

Training loss:  0.06084437336325645
Training MSE:   0.06084437336325645
Training MAE:   0.19294932746887208

Testing loss:  0.060909155052900316
Testing MSE:   0.060909155052900316
Testing MAE:   0.19320254669189454

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 5, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  13567.606336972045
Training MSE:   13567.606336972045
Training MAE:   42.42078446006775
Testing loss:   20.728036112976074
Testing MSE:    20.728036112976074
Testing MAE:    3.585297506713867

Epoch  1
Training loss:  15.333756033325196
Training MSE:   15.333756033325196
Training MAE:   3.0952790603637697
Testing loss:   10.664125016784668
Testing MSE:    10.664125016784668
Testing MAE:    2.575275004577637

Epoch  2
Training loss:  5.602951175022125
Training MSE:   5.602951175022125
Training MAE:   1.8422231758117675
Testing loss:   3.293054086494446
Testing MSE:    3.293054086494446
Testing MAE:    1.408168034362793

Epoch  3
Training loss:  2.0464914345979692
Training MSE:   2.0464914345979692
Training MAE:   1.1121065608978271
Testing loss:   1.128372523546219
Testing MSE:    1.128372523546219
Testing MAE:    0.8369515991210937

Epoch  4
Training loss:  0.659334019458294
Training MSE:   0.659334019458294
Training MAE:   0.623491837310791
Testing loss:   0.30973387928009033
Testing MSE:    0.30973387928009033
Testing MAE:    0.4292331115722656

Epoch  5
Training loss:  0.1872674045741558
Training MSE:   0.1872674045741558
Training MAE:   0.31037798347473144
Testing loss:   0.09782046349048615
Testing MSE:    0.09782046349048615
Testing MAE:    0.20593428039550782

Epoch  6
Training loss:  0.08998732412308455
Training MSE:   0.08998732412308455
Training MAE:   0.20112702026367188
Testing loss:   0.08722953385114669
Testing MSE:    0.08722953385114669
Testing MAE:    0.21062955932617186

Epoch  7
Training loss:  0.0816367660626769
Training MSE:   0.0816367660626769
Training MAE:   0.20273930892944336
Testing loss:   0.05287148950099945
Testing MSE:    0.05287148950099945
Testing MAE:    0.1568567352294922

Epoch  8
Training loss:  0.08281839894726872
Training MSE:   0.08281839894726872
Training MAE:   0.21092779083251953
Testing loss:   0.04751803646087646
Testing MSE:    0.04751803646087646
Testing MAE:    0.14992917938232422

Epoch  9
Training loss:  0.09221549177542329
Training MSE:   0.09221549177542329
Training MAE:   0.22979483146667481
Testing loss:   0.0438094933539629
Testing MSE:    0.0438094933539629
Testing MAE:    0.14300572204589843

Training loss:  0.04451688200309872
Training MSE:   0.04451688200309872
Training MAE:   0.14245706672668457

Testing loss:  0.0438094933539629
Testing MSE:   0.0438094933539629
Testing MAE:   0.14300572204589843

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 5, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  12397.01406642952
Training MSE:   12397.01406642952
Training MAE:   40.957031206512454
Testing loss:   19.073310583496095
Testing MSE:    19.073310583496095
Testing MAE:    3.3233054580688477

Epoch  1
Training loss:  15.266996497344971
Training MSE:   15.266996497344971
Training MAE:   2.9743620662689207
Testing loss:   11.82604729232788
Testing MSE:    11.82604729232788
Testing MAE:    2.5984075393676758

Epoch  2
Training loss:  8.15883762845993
Training MSE:   8.15883762845993
Training MAE:   2.146378484725952
Testing loss:   5.187606668090821
Testing MSE:    5.187606668090821
Testing MAE:    1.705909797668457

Epoch  3
Training loss:  3.1865913235664367
Training MSE:   3.1865913235664367
Training MAE:   1.3104466106414794
Testing loss:   1.6765347383499145
Testing MSE:    1.6765347383499145
Testing MAE:    0.9482449844360351

Epoch  4
Training loss:  0.9985484513282776
Training MSE:   0.9985484513282776
Training MAE:   0.7273347644805909
Testing loss:   0.49790387821197507
Testing MSE:    0.49790387821197507
Testing MAE:    0.5184180648803711

Epoch  5
Training loss:  0.3282754914164543
Training MSE:   0.3282754914164543
Training MAE:   0.41341633796691896
Testing loss:   0.18780651819705962
Testing MSE:    0.18780651819705962
Testing MAE:    0.30478638763427734

Epoch  6
Training loss:  0.1543535665050149
Training MSE:   0.1543535665050149
Training MAE:   0.2719874771118164
Testing loss:   0.13127041885852814
Testing MSE:    0.13127041885852814
Testing MAE:    0.27246314086914064

Epoch  7
Training loss:  0.10959731060713529
Training MSE:   0.10959731060713529
Training MAE:   0.22900059318542482
Testing loss:   0.07274072785973548
Testing MSE:    0.07274072785973548
Testing MAE:    0.17709015350341797

Epoch  8
Training loss:  0.10998463492691517
Training MSE:   0.10998463492691517
Training MAE:   0.2412582374572754
Testing loss:   0.06342370011210442
Testing MSE:    0.06342370011210442
Testing MAE:    0.1721925537109375

Epoch  9
Training loss:  0.0985640368975699
Training MSE:   0.0985640368975699
Training MAE:   0.23015469284057616
Testing loss:   0.0655397320151329
Testing MSE:    0.0655397320151329
Testing MAE:    0.18724582061767578

Training loss:  0.0662982703357935
Training MSE:   0.0662982703357935
Training MAE:   0.18648458290100098

Testing loss:  0.0655397320151329
Testing MSE:   0.0655397320151329
Testing MAE:   0.18724582061767578

Number of layers:  3
Number of units in layer [1,2,3]:  [15, 5, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  23558.435050396536
Training MSE:   23558.435050396536
Training MAE:   70.32987390861511
Testing loss:   7.986617829895019
Testing MSE:    7.986617829895019
Testing MAE:    2.221086279296875

Epoch  1
Training loss:  2.109765208303928
Training MSE:   2.109765208303928
Training MAE:   1.006072081375122
Testing loss:   0.5372650416374206
Testing MSE:    0.5372650416374206
Testing MAE:    0.5365074951171875

Epoch  2
Training loss:  0.31928839827775957
Training MSE:   0.31928839827775957
Training MAE:   0.3759083827972412
Testing loss:   0.19275921094417572
Testing MSE:    0.19275921094417572
Testing MAE:    0.2893365234375

Epoch  3
Training loss:  0.1400099881976843
Training MSE:   0.1400099881976843
Training MAE:   0.22320764808654786
Testing loss:   0.10301818448901176
Testing MSE:    0.10301818448901176
Testing MAE:    0.18886327209472656

Epoch  4
Training loss:  0.09349056500196457
Training MSE:   0.09349056500196457
Training MAE:   0.18362780303955079
Testing loss:   0.07379188129901885
Testing MSE:    0.07379188129901885
Testing MAE:    0.16290943145751954

Epoch  5
Training loss:  0.07606965043023228
Training MSE:   0.07606965043023228
Training MAE:   0.17468001594543456
Testing loss:   0.07243686441779136
Testing MSE:    0.07243686441779136
Testing MAE:    0.18747825927734374

Epoch  6
Training loss:  0.06718029253929854
Training MSE:   0.06718029253929854
Training MAE:   0.17122204093933105
Testing loss:   0.0582289673268795
Testing MSE:    0.0582289673268795
Testing MAE:    0.16501507720947264

Epoch  7
Training loss:  0.06651498970687389
Training MSE:   0.06651498970687389
Training MAE:   0.1770700481414795
Testing loss:   0.05659236730933189
Testing MSE:    0.05659236730933189
Testing MAE:    0.1707639602661133

Epoch  8
Training loss:  0.07124371715337038
Training MSE:   0.07124371715337038
Training MAE:   0.19359971656799316
Testing loss:   0.11368205498456956
Testing MSE:    0.11368205498456956
Testing MAE:    0.27679911041259764

Epoch  9
Training loss:  0.06604160268232226
Training MSE:   0.06604160268232226
Training MAE:   0.19014182968139648
Testing loss:   0.05031062037944794
Testing MSE:    0.05031062037944794
Testing MAE:    0.167935107421875

Training loss:  0.049137390235066414
Training MSE:   0.049137390235066414
Training MAE:   0.16502812843322753

Testing loss:  0.05031062037944794
Testing MSE:   0.05031062037944794
Testing MAE:   0.167935107421875

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 25, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10460.771121229172
Training MSE:   10460.771121229172
Training MAE:   35.7041110080719
Testing loss:   8.75662597579956
Testing MSE:    8.75662597579956
Testing MAE:    2.2702379760742186

Epoch  1
Training loss:  3.342936372756958
Training MSE:   3.342936372756958
Training MAE:   1.3144159637451172
Testing loss:   0.6729622483730316
Testing MSE:    0.6729622483730316
Testing MAE:    0.6195541366577149

Epoch  2
Training loss:  0.30265942337214946
Training MSE:   0.30265942337214946
Training MAE:   0.37745705070495605
Testing loss:   0.15996671068668367
Testing MSE:    0.15996671068668367
Testing MAE:    0.2712349990844727

Epoch  3
Training loss:  0.11624120232760907
Training MSE:   0.11624120232760907
Training MAE:   0.223346044921875
Testing loss:   0.08892019721865654
Testing MSE:    0.08892019721865654
Testing MAE:    0.19870237579345704

Epoch  4
Training loss:  0.0902055886387825
Training MSE:   0.0902055886387825
Training MAE:   0.2037406032562256
Testing loss:   0.11832303185462952
Testing MSE:    0.11832303185462952
Testing MAE:    0.27019397583007815

Epoch  5
Training loss:  0.0819753679767251
Training MSE:   0.0819753679767251
Training MAE:   0.20115619888305664
Testing loss:   0.06106608656644821
Testing MSE:    0.06106608656644821
Testing MAE:    0.1671669677734375

Epoch  6
Training loss:  0.08850664491802454
Training MSE:   0.08850664491802454
Training MAE:   0.21752515449523926
Testing loss:   0.06753550330996513
Testing MSE:    0.06753550330996513
Testing MAE:    0.18153507080078124

Epoch  7
Training loss:  0.10263892837390304
Training MSE:   0.10263892837390304
Training MAE:   0.24189515266418457
Testing loss:   0.11778811997175216
Testing MSE:    0.11778811997175216
Testing MAE:    0.2772445449829102

Epoch  8
Training loss:  0.1124084669418633
Training MSE:   0.1124084669418633
Training MAE:   0.2568259380340576
Testing loss:   0.22133382115364075
Testing MSE:    0.22133382115364075
Testing MAE:    0.4232327041625977

Epoch  9
Training loss:  0.11020547444075346
Training MSE:   0.11020547444075346
Training MAE:   0.25135213546752927
Testing loss:   0.09079384689331055
Testing MSE:    0.09079384689331055
Testing MAE:    0.2412047317504883

Training loss:  0.09179851238429547
Training MSE:   0.09179851238429547
Training MAE:   0.24139535751342772

Testing loss:  0.09079384689331055
Testing MSE:   0.09079384689331055
Testing MAE:   0.2412047317504883

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 25, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  6399.535017953492
Training MSE:   6399.535017953492
Training MAE:   25.53568325805664
Testing loss:   34.22737325744629
Testing MSE:    34.22737325744629
Testing MAE:    4.639646347045899

Epoch  1
Training loss:  23.886488482666017
Training MSE:   23.886488482666017
Training MAE:   3.8317936763763427
Testing loss:   14.036670657348633
Testing MSE:    14.036670657348633
Testing MAE:    2.937490382385254

Epoch  2
Training loss:  8.898160486602784
Training MSE:   8.898160486602784
Training MAE:   2.3161724857330324
Testing loss:   4.4476758651733395
Testing MSE:    4.4476758651733395
Testing MAE:    1.6364430908203125

Epoch  3
Training loss:  2.4973619623661043
Training MSE:   2.4973619623661043
Training MAE:   1.2085058498382568
Testing loss:   1.0790602776527405
Testing MSE:    1.0790602776527405
Testing MAE:    0.8222893798828125

Epoch  4
Training loss:  0.590522344082594
Training MSE:   0.590522344082594
Training MAE:   0.577505260848999
Testing loss:   0.2611399509191513
Testing MSE:    0.2611399509191513
Testing MAE:    0.3817066421508789

Epoch  5
Training loss:  0.1971941957294941
Training MSE:   0.1971941957294941
Training MAE:   0.31739024276733396
Testing loss:   0.13520717995166778
Testing MSE:    0.13520717995166778
Testing MAE:    0.2545889282226563

Epoch  6
Training loss:  0.13164947701841592
Training MSE:   0.13164947701841592
Training MAE:   0.26063450202941896
Testing loss:   0.08024682368040084
Testing MSE:    0.08024682368040084
Testing MAE:    0.1887353042602539

Epoch  7
Training loss:  0.12316513670831919
Training MSE:   0.12316513670831919
Training MAE:   0.26084827575683595
Testing loss:   0.06154520641565323
Testing MSE:    0.06154520641565323
Testing MAE:    0.17397129974365233

Epoch  8
Training loss:  0.14673997550308704
Training MSE:   0.14673997550308704
Training MAE:   0.2947411880493164
Testing loss:   0.05541213828921318
Testing MSE:    0.05541213828921318
Testing MAE:    0.1596987350463867

Epoch  9
Training loss:  0.12858159708753228
Training MSE:   0.12858159708753228
Training MAE:   0.27503843269348144
Testing loss:   0.4385309557914734
Testing MSE:    0.4385309557914734
Testing MAE:    0.6273470046997071

Training loss:  0.4362463320732117
Training MSE:   0.4362463320732117
Training MAE:   0.6272719577789306

Testing loss:  0.4385309557914734
Testing MSE:   0.4385309557914734
Testing MAE:   0.6273470046997071

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 25, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  11816.186563760377
Training MSE:   11816.186563760377
Training MAE:   38.20029396781921
Testing loss:   13.318197123718262
Testing MSE:    13.318197123718262
Testing MAE:    2.780539225769043

Epoch  1
Training loss:  4.763268828940392
Training MSE:   4.763268828940392
Training MAE:   1.4978039348602294
Testing loss:   0.7027969202041626
Testing MSE:    0.7027969202041626
Testing MAE:    0.5977071014404297

Epoch  2
Training loss:  0.3811998872965574
Training MSE:   0.3811998872965574
Training MAE:   0.38980440139770506
Testing loss:   0.1646792900323868
Testing MSE:    0.1646792900323868
Testing MAE:    0.2473214584350586

Epoch  3
Training loss:  0.14506721503585576
Training MSE:   0.14506721503585576
Training MAE:   0.2313044662475586
Testing loss:   0.10943261662721634
Testing MSE:    0.10943261662721634
Testing MAE:    0.21001141967773437

Epoch  4
Training loss:  0.09873603598028421
Training MSE:   0.09873603598028421
Training MAE:   0.20575645751953126
Testing loss:   0.09596104127168656
Testing MSE:    0.09596104127168656
Testing MAE:    0.21645706481933594

Epoch  5
Training loss:  0.09055710886120796
Training MSE:   0.09055710886120796
Training MAE:   0.2112756763458252
Testing loss:   0.0746145114839077
Testing MSE:    0.0746145114839077
Testing MAE:    0.1896533676147461

Epoch  6
Training loss:  0.0883021496027708
Training MSE:   0.0883021496027708
Training MAE:   0.21547660598754884
Testing loss:   0.14892702393531798
Testing MSE:    0.14892702393531798
Testing MAE:    0.324684049987793

Epoch  7
Training loss:  0.10710610178112984
Training MSE:   0.10710610178112984
Training MAE:   0.24689604606628418
Testing loss:   0.058284327811002734
Testing MSE:    0.058284327811002734
Testing MAE:    0.17815791778564452

Epoch  8
Training loss:  0.12075371758937836
Training MSE:   0.12075371758937836
Training MAE:   0.26667310333251953
Testing loss:   0.11746326065063477
Testing MSE:    0.11746326065063477
Testing MAE:    0.2893355712890625

Epoch  9
Training loss:  0.12077594365701079
Training MSE:   0.12077594365701079
Training MAE:   0.26595197563171386
Testing loss:   0.09091833748817443
Testing MSE:    0.09091833748817443
Testing MAE:    0.2504589172363281

Training loss:  0.09140748052597046
Training MSE:   0.09140748052597046
Training MAE:   0.2505042003631592

Testing loss:  0.09091833748817443
Testing MSE:   0.09091833748817443
Testing MAE:   0.2504589172363281

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 25, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9093.426959140015
Training MSE:   9093.426959140015
Training MAE:   34.333616846466064
Testing loss:   63.53645653076172
Testing MSE:    63.53645653076172
Testing MAE:    6.226297802734375

Epoch  1
Training loss:  49.205327284240724
Training MSE:   49.205327284240724
Training MAE:   5.489417230987549
Testing loss:   35.90864387512207
Testing MSE:    35.90864387512207
Testing MAE:    4.677977030944824

Epoch  2
Training loss:  23.248856506729126
Training MSE:   23.248856506729126
Training MAE:   3.7517412605285645
Testing loss:   13.197808805847169
Testing MSE:    13.197808805847169
Testing MAE:    2.86312724609375

Epoch  3
Training loss:  6.678621596717835
Training MSE:   6.678621596717835
Training MAE:   1.9713096355438233
Testing loss:   2.512844313240051
Testing MSE:    2.512844313240051
Testing MAE:    1.2199564651489259

Epoch  4
Training loss:  1.0754383398413658
Training MSE:   1.0754383398413658
Training MAE:   0.7617961341857911
Testing loss:   0.33475619726181033
Testing MSE:    0.33475619726181033
Testing MAE:    0.4352073471069336

Epoch  5
Training loss:  0.18639733524918556
Training MSE:   0.18639733524918556
Training MAE:   0.3098174430847168
Testing loss:   0.11288029930591584
Testing MSE:    0.11288029930591584
Testing MAE:    0.24655725555419922

Epoch  6
Training loss:  0.10286401217877865
Training MSE:   0.10286401217877865
Training MAE:   0.22939748191833495
Testing loss:   0.08299563436508178
Testing MSE:    0.08299563436508178
Testing MAE:    0.20597512969970702

Epoch  7
Training loss:  0.10731810416132212
Training MSE:   0.10731810416132212
Training MAE:   0.2401184009552002
Testing loss:   0.1438384977221489
Testing MSE:    0.1438384977221489
Testing MAE:    0.3114320922851562

Epoch  8
Training loss:  0.11376196021735668
Training MSE:   0.11376196021735668
Training MAE:   0.251329439163208
Testing loss:   0.14939161474704743
Testing MSE:    0.14939161474704743
Testing MAE:    0.3235722900390625

Epoch  9
Training loss:  0.11577206456512212
Training MSE:   0.11577206456512212
Training MAE:   0.2571971862792969
Testing loss:   0.0629010061442852
Testing MSE:    0.0629010061442852
Testing MAE:    0.1805360092163086

Training loss:  0.06255306605696678
Training MSE:   0.06255306605696678
Training MAE:   0.17906908683776857

Testing loss:  0.0629010061442852
Testing MSE:   0.0629010061442852
Testing MAE:   0.1805360092163086

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 25, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  6704.255969698143
Training MSE:   6704.255969698143
Training MAE:   26.703239954376222
Testing loss:   2.3668037405014037
Testing MSE:    2.3668037405014037
Testing MAE:    1.154534602355957

Epoch  1
Training loss:  0.48469736478328707
Training MSE:   0.48469736478328707
Training MAE:   0.44963793449401857
Testing loss:   0.15916103574037552
Testing MSE:    0.15916103574037552
Testing MAE:    0.2323485366821289

Epoch  2
Training loss:  0.10328689088076352
Training MSE:   0.10328689088076352
Training MAE:   0.20329821243286134
Testing loss:   0.09431936724185944
Testing MSE:    0.09431936724185944
Testing MAE:    0.18765712585449218

Epoch  3
Training loss:  0.0889284039914608
Training MSE:   0.0889284039914608
Training MAE:   0.20386604804992675
Testing loss:   0.07396655904054641
Testing MSE:    0.07396655904054641
Testing MAE:    0.1851204345703125

Epoch  4
Training loss:  0.08086972569301724
Training MSE:   0.08086972569301724
Training MAE:   0.20183403091430663
Testing loss:   0.09717690111398697
Testing MSE:    0.09717690111398697
Testing MAE:    0.23107296295166016

Epoch  5
Training loss:  0.0779660431548953
Training MSE:   0.0779660431548953
Training MAE:   0.20231170539855958
Testing loss:   0.060724264109134674
Testing MSE:    0.060724264109134674
Testing MAE:    0.17020560760498046

Epoch  6
Training loss:  0.07689283208251
Training MSE:   0.07689283208251
Training MAE:   0.2036527858734131
Testing loss:   0.049020730119943616
Testing MSE:    0.049020730119943616
Testing MAE:    0.15863107299804688

Epoch  7
Training loss:  0.10411626283079385
Training MSE:   0.10411626283079385
Training MAE:   0.24690807304382323
Testing loss:   0.05673250725865364
Testing MSE:    0.05673250725865364
Testing MAE:    0.1819683609008789

Epoch  8
Training loss:  0.11675461927577853
Training MSE:   0.11675461927577853
Training MAE:   0.261409729385376
Testing loss:   0.13747847567796706
Testing MSE:    0.13747847567796706
Testing MAE:    0.3291754638671875

Epoch  9
Training loss:  0.10183126367256046
Training MSE:   0.10183126367256046
Training MAE:   0.2455959701538086
Testing loss:   0.32177252554893493
Testing MSE:    0.32177252554893493
Testing MAE:    0.53979501953125

Training loss:  0.32115804015398025
Training MSE:   0.32115804015398025
Training MAE:   0.5398806118011474

Testing loss:  0.32177252554893493
Testing MSE:   0.32177252554893493
Testing MAE:   0.53979501953125

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 20, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  8509.804086071777
Training MSE:   8509.804086071777
Training MAE:   35.175190420532225
Testing loss:   159.0861742553711
Testing MSE:    159.0861742553711
Testing MAE:    9.99633598022461

Epoch  1
Training loss:  111.97278907470704
Training MSE:   111.97278907470704
Training MAE:   8.342234549713135
Testing loss:   70.75823872070312
Testing MSE:    70.75823872070312
Testing MAE:    6.667060289001465

Epoch  2
Training loss:  42.18735349578857
Training MSE:   42.18735349578857
Training MAE:   5.034756091690063
Testing loss:   19.558313256835937
Testing MSE:    19.558313256835937
Testing MAE:    3.4804684005737303

Epoch  3
Training loss:  9.236383465385437
Training MSE:   9.236383465385437
Training MAE:   2.2822703983306885
Testing loss:   3.1898646923065184
Testing MSE:    3.1898646923065184
Testing MAE:    1.3663207489013671

Epoch  4
Training loss:  1.5018322233200074
Training MSE:   1.5018322233200074
Training MAE:   0.903120163345337
Testing loss:   0.47091499128341674
Testing MSE:    0.47091499128341674
Testing MAE:    0.5112664199829101

Epoch  5
Training loss:  0.27779628484249114
Training MSE:   0.27779628484249114
Training MAE:   0.3734705024719238
Testing loss:   0.15804083268642424
Testing MSE:    0.15804083268642424
Testing MAE:    0.2899322799682617

Epoch  6
Training loss:  0.13225491760522126
Training MSE:   0.13225491760522126
Training MAE:   0.25020326805114745
Testing loss:   0.10281581616997719
Testing MSE:    0.10281581616997719
Testing MAE:    0.2120559036254883

Epoch  7
Training loss:  0.14619448561072348
Training MSE:   0.14619448561072348
Training MAE:   0.28074404907226563
Testing loss:   0.3669945249557495
Testing MSE:    0.3669945249557495
Testing MAE:    0.5577601119995117

Epoch  8
Training loss:  0.14064292982667684
Training MSE:   0.14064292982667684
Training MAE:   0.2792787242889404
Testing loss:   0.25110527448654174
Testing MSE:    0.25110527448654174
Testing MAE:    0.4466938491821289

Epoch  9
Training loss:  0.13416233955919743
Training MSE:   0.13416233955919743
Training MAE:   0.276034122467041
Testing loss:   0.06092831954360008
Testing MSE:    0.06092831954360008
Testing MAE:    0.17449613342285156

Training loss:  0.06069973279833794
Training MSE:   0.06069973279833794
Training MAE:   0.17290692863464355

Testing loss:  0.06092831954360008
Testing MSE:   0.06092831954360008
Testing MAE:   0.17449613342285156

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 20, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7363.188627041626
Training MSE:   7363.188627041626
Training MAE:   25.7187394241333
Testing loss:   9.707320095825196
Testing MSE:    9.707320095825196
Testing MAE:    2.5040414016723633

Epoch  1
Training loss:  5.933855135345459
Training MSE:   5.933855135345459
Training MAE:   1.9098563179016113
Testing loss:   3.133341513442993
Testing MSE:    3.133341513442993
Testing MAE:    1.4018110153198242

Epoch  2
Training loss:  2.0386391724348067
Training MSE:   2.0386391724348067
Training MAE:   1.0972341495513915
Testing loss:   1.3310226219177246
Testing MSE:    1.3310226219177246
Testing MAE:    0.9132123138427735

Epoch  3
Training loss:  0.8132369835495948
Training MSE:   0.8132369835495948
Training MAE:   0.6796290027618408
Testing loss:   0.5010377209186554
Testing MSE:    0.5010377209186554
Testing MAE:    0.5194985229492187

Epoch  4
Training loss:  0.4050387921929359
Training MSE:   0.4050387921929359
Training MAE:   0.467429931640625
Testing loss:   0.24632500960826872
Testing MSE:    0.24632500960826872
Testing MAE:    0.3703518859863281

Epoch  5
Training loss:  0.2191168692678213
Training MSE:   0.2191168692678213
Training MAE:   0.3369285488128662
Testing loss:   0.22757856137752533
Testing MSE:    0.22757856137752533
Testing MAE:    0.3977292694091797

Epoch  6
Training loss:  0.15683147195875644
Training MSE:   0.15683147195875644
Training MAE:   0.285126432800293
Testing loss:   0.07698868263959885
Testing MSE:    0.07698868263959885
Testing MAE:    0.1799761672973633

Epoch  7
Training loss:  0.14290057918354868
Training MSE:   0.14290057918354868
Training MAE:   0.2808613697052002
Testing loss:   0.18702407772541046
Testing MSE:    0.18702407772541046
Testing MAE:    0.36175322265625

Epoch  8
Training loss:  0.16155221326351166
Training MSE:   0.16155221326351166
Training MAE:   0.3050990814208984
Testing loss:   0.11081617704629898
Testing MSE:    0.11081617704629898
Testing MAE:    0.26958843688964845

Epoch  9
Training loss:  0.15134685326963662
Training MSE:   0.15134685326963662
Training MAE:   0.2990342273712158
Testing loss:   0.053430598264932634
Testing MSE:    0.053430598264932634
Testing MAE:    0.1609473876953125

Training loss:  0.054488371604681014
Training MSE:   0.054488371604681014
Training MAE:   0.16047335357666015

Testing loss:  0.053430598264932634
Testing MSE:   0.053430598264932634
Testing MAE:   0.1609473876953125

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 20, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  12973.204892193604
Training MSE:   12973.204892193604
Training MAE:   43.91548383331299
Testing loss:   78.19441001586914
Testing MSE:    78.19441001586914
Testing MAE:    6.943326484680176

Epoch  1
Training loss:  4.049102573299408
Training MSE:   4.049102573299408
Training MAE:   1.0094612380981445
Testing loss:   0.15063535495996475
Testing MSE:    0.15063535495996475
Testing MAE:    0.22622928466796874

Epoch  2
Training loss:  0.10378359705656767
Training MSE:   0.10378359705656767
Training MAE:   0.20614937782287598
Testing loss:   0.08563004002571106
Testing MSE:    0.08563004002571106
Testing MAE:    0.18081749114990234

Epoch  3
Training loss:  0.07819890148341656
Training MSE:   0.07819890148341656
Training MAE:   0.18951513175964355
Testing loss:   0.06382064026892185
Testing MSE:    0.06382064026892185
Testing MAE:    0.16633654327392577

Epoch  4
Training loss:  0.06787608133777977
Training MSE:   0.06787608133777977
Training MAE:   0.18079105224609374
Testing loss:   0.05356767523288727
Testing MSE:    0.05356767523288727
Testing MAE:    0.16048926391601562

Epoch  5
Training loss:  0.06248021338880062
Training MSE:   0.06248021338880062
Training MAE:   0.17664254035949706
Testing loss:   0.07656658244132995
Testing MSE:    0.07656658244132995
Testing MAE:    0.21634741821289064

Epoch  6
Training loss:  0.06654845015034079
Training MSE:   0.06654845015034079
Training MAE:   0.18863461952209473
Testing loss:   0.04710849922895431
Testing MSE:    0.04710849922895431
Testing MAE:    0.1476551300048828

Epoch  7
Training loss:  0.07597380872741341
Training MSE:   0.07597380872741341
Training MAE:   0.2058573329925537
Testing loss:   0.20224809432029725
Testing MSE:    0.20224809432029725
Testing MAE:    0.4121843307495117

Epoch  8
Training loss:  0.09350324849784374
Training MSE:   0.09350324849784374
Training MAE:   0.23324545402526856
Testing loss:   0.1175547828912735
Testing MSE:    0.1175547828912735
Testing MAE:    0.29936554565429685

Epoch  9
Training loss:  0.09044917019382119
Training MSE:   0.09044917019382119
Training MAE:   0.22819808807373046
Testing loss:   0.06815360826253891
Testing MSE:    0.06815360826253891
Testing MAE:    0.2064026107788086

Training loss:  0.06936100250482559
Training MSE:   0.06936100250482559
Training MAE:   0.20695977935791016

Testing loss:  0.06815360826253891
Testing MSE:   0.06815360826253891
Testing MAE:   0.2064026107788086

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 20, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  6089.025436940956
Training MSE:   6089.025436940956
Training MAE:   23.341188147354124
Testing loss:   4.308612491607666
Testing MSE:    4.308612491607666
Testing MAE:    1.6000975982666015

Epoch  1
Training loss:  3.237681229352951
Training MSE:   3.237681229352951
Training MAE:   1.387060824584961
Testing loss:   2.535058221054077
Testing MSE:    2.535058221054077
Testing MAE:    1.2539813110351563

Epoch  2
Training loss:  2.075695723104477
Training MSE:   2.075695723104477
Training MAE:   1.111794814682007
Testing loss:   1.651490478515625
Testing MSE:    1.651490478515625
Testing MAE:    1.0169372024536132

Epoch  3
Training loss:  1.2080556494712829
Training MSE:   1.2080556494712829
Training MAE:   0.8400668857574463
Testing loss:   0.9899533563613891
Testing MSE:    0.9899533563613891
Testing MAE:    0.7329760513305664

Epoch  4
Training loss:  0.518659958332777
Training MSE:   0.518659958332777
Training MAE:   0.5303299163818359
Testing loss:   0.274378440952301
Testing MSE:    0.274378440952301
Testing MAE:    0.38813916778564456

Epoch  5
Training loss:  0.18387612876743079
Training MSE:   0.18387612876743079
Training MAE:   0.29904869651794436
Testing loss:   0.11089414526224137
Testing MSE:    0.11089414526224137
Testing MAE:    0.22261795654296876

Epoch  6
Training loss:  0.0973277434848249
Training MSE:   0.0973277434848249
Training MAE:   0.21632804679870604
Testing loss:   0.07929464481472968
Testing MSE:    0.07929464481472968
Testing MAE:    0.19273982849121093

Epoch  7
Training loss:  0.09174946913868189
Training MSE:   0.09174946913868189
Training MAE:   0.22003841285705567
Testing loss:   0.07320433673262595
Testing MSE:    0.07320433673262595
Testing MAE:    0.20568607788085938

Epoch  8
Training loss:  0.08813686614483594
Training MSE:   0.08813686614483594
Training MAE:   0.2199029468536377
Testing loss:   0.04919749580621719
Testing MSE:    0.04919749580621719
Testing MAE:    0.150694287109375

Epoch  9
Training loss:  0.09826668874174356
Training MSE:   0.09826668874174356
Training MAE:   0.23696111526489258
Testing loss:   0.0422983903080225
Testing MSE:    0.0422983903080225
Testing MAE:    0.14169631652832032

Training loss:  0.04251752084568143
Training MSE:   0.04251752084568143
Training MAE:   0.14103125762939453

Testing loss:  0.0422983903080225
Testing MSE:   0.0422983903080225
Testing MAE:   0.14169631652832032

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 20, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  6807.769912975693
Training MSE:   6807.769912975693
Training MAE:   25.56032883758545
Testing loss:   11.37232368927002
Testing MSE:    11.37232368927002
Testing MAE:    2.72011854095459

Epoch  1
Training loss:  3.91927102394104
Training MSE:   3.91927102394104
Training MAE:   1.5355211879730224
Testing loss:   2.1217464124679566
Testing MSE:    2.1217464124679566
Testing MAE:    1.1516286544799805

Epoch  2
Training loss:  1.411061523628235
Training MSE:   1.411061523628235
Training MAE:   0.9284728672027588
Testing loss:   0.9019309650421142
Testing MSE:    0.9019309650421142
Testing MAE:    0.7467599746704101

Epoch  3
Training loss:  0.6534021563172341
Training MSE:   0.6534021563172341
Training MAE:   0.6270412528991699
Testing loss:   0.4220014802455902
Testing MSE:    0.4220014802455902
Testing MAE:    0.5074114486694336

Epoch  4
Training loss:  0.30880378757417204
Training MSE:   0.30880378757417204
Training MAE:   0.41923197860717776
Testing loss:   0.21265978000164032
Testing MSE:    0.21265978000164032
Testing MAE:    0.36133441925048826

Epoch  5
Training loss:  0.15147023890018463
Training MSE:   0.15147023890018463
Training MAE:   0.28535067291259764
Testing loss:   0.10895435038805008
Testing MSE:    0.10895435038805008
Testing MAE:    0.2397414566040039

Epoch  6
Training loss:  0.10593121833130717
Training MSE:   0.10593121833130717
Training MAE:   0.24067604751586913
Testing loss:   0.07132469263076782
Testing MSE:    0.07132469263076782
Testing MAE:    0.2037941375732422

Epoch  7
Training loss:  0.10544481063038111
Training MSE:   0.10544481063038111
Training MAE:   0.24532341384887696
Testing loss:   0.16349125328063965
Testing MSE:    0.16349125328063965
Testing MAE:    0.350051155090332

Epoch  8
Training loss:  0.1111162324681878
Training MSE:   0.1111162324681878
Training MAE:   0.2562131191253662
Testing loss:   0.29517424845695495
Testing MSE:    0.29517424845695495
Testing MAE:    0.5016032669067383

Epoch  9
Training loss:  0.11164737763404846
Training MSE:   0.11164737763404846
Training MAE:   0.2565292907714844
Testing loss:   0.07678164100646973
Testing MSE:    0.07678164100646973
Testing MAE:    0.2220049331665039

Training loss:  0.07700993193686008
Training MSE:   0.07700993193686008
Training MAE:   0.22152960662841797

Testing loss:  0.07678164100646973
Testing MSE:   0.07678164100646973
Testing MAE:   0.2220049331665039

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 15, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  12503.861412127686
Training MSE:   12503.861412127686
Training MAE:   41.38286136894226
Testing loss:   62.55097061157227
Testing MSE:    62.55097061157227
Testing MAE:    6.335338096618653

Epoch  1
Training loss:  48.22079437255859
Training MSE:   48.22079437255859
Training MAE:   5.526387014770508
Testing loss:   33.34430373840332
Testing MSE:    33.34430373840332
Testing MAE:    4.63736667175293

Epoch  2
Training loss:  22.51442153892517
Training MSE:   22.51442153892517
Training MAE:   3.769221686553955
Testing loss:   13.797209950256347
Testing MSE:    13.797209950256347
Testing MAE:    2.981187969970703

Epoch  3
Training loss:  7.175017117595672
Training MSE:   7.175017117595672
Training MAE:   2.094686227416992
Testing loss:   2.89351029548645
Testing MSE:    2.89351029548645
Testing MAE:    1.3612825469970704

Epoch  4
Training loss:  1.3488295741438865
Training MSE:   1.3488295741438865
Training MAE:   0.872875399017334
Testing loss:   0.39055151791572573
Testing MSE:    0.39055151791572573
Testing MAE:    0.46276207427978516

Epoch  5
Training loss:  0.23008751656115056
Training MSE:   0.23008751656115056
Training MAE:   0.3309235763549805
Testing loss:   0.14876047669649123
Testing MSE:    0.14876047669649123
Testing MAE:    0.2511605346679687

Epoch  6
Training loss:  0.13557339746505023
Training MSE:   0.13557339746505023
Training MAE:   0.24424237442016603
Testing loss:   0.08324186816811562
Testing MSE:    0.08324186816811562
Testing MAE:    0.16656558227539062

Epoch  7
Training loss:  0.11181406822800637
Training MSE:   0.11181406822800637
Training MAE:   0.23060840072631836
Testing loss:   0.0741136867582798
Testing MSE:    0.0741136867582798
Testing MAE:    0.1878282440185547

Epoch  8
Training loss:  0.11467149323001503
Training MSE:   0.11467149323001503
Training MAE:   0.2510134738922119
Testing loss:   0.12651822457313538
Testing MSE:    0.12651822457313538
Testing MAE:    0.299289469909668

Epoch  9
Training loss:  0.11324405726790428
Training MSE:   0.11324405726790428
Training MAE:   0.2521302646636963
Testing loss:   0.046520436081290244
Testing MSE:    0.046520436081290244
Testing MAE:    0.14350042572021485

Training loss:  0.04697163541987538
Training MSE:   0.04697163541987538
Training MAE:   0.1427828769683838

Testing loss:  0.046520436081290244
Testing MSE:   0.046520436081290244
Testing MAE:   0.14350042572021485

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 15, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  8493.84138186493
Training MSE:   8493.84138186493
Training MAE:   29.242899604034424
Testing loss:   21.44188759460449
Testing MSE:    21.44188759460449
Testing MAE:    3.5623098068237304

Epoch  1
Training loss:  14.905622004318237
Training MSE:   14.905622004318237
Training MAE:   2.9524038475036622
Testing loss:   8.560638487243653
Testing MSE:    8.560638487243653
Testing MAE:    2.254288343811035

Epoch  2
Training loss:  4.325600024986267
Training MSE:   4.325600024986267
Training MAE:   1.5557764465332031
Testing loss:   1.4531836317062379
Testing MSE:    1.4531836317062379
Testing MAE:    0.9293153564453125

Epoch  3
Training loss:  0.792099412381649
Training MSE:   0.792099412381649
Training MAE:   0.6733872776031494
Testing loss:   0.3596928062438965
Testing MSE:    0.3596928062438965
Testing MAE:    0.45560465698242186

Epoch  4
Training loss:  0.2564284543603659
Training MSE:   0.2564284543603659
Training MAE:   0.38264140701293947
Testing loss:   0.1272947041273117
Testing MSE:    0.1272947041273117
Testing MAE:    0.2490624237060547

Epoch  5
Training loss:  0.1249106741413474
Training MSE:   0.1249106741413474
Training MAE:   0.25043525772094727
Testing loss:   0.1000859893798828
Testing MSE:    0.1000859893798828
Testing MAE:    0.23264160308837892

Epoch  6
Training loss:  0.11479028757810593
Training MSE:   0.11479028757810593
Training MAE:   0.2447070514678955
Testing loss:   0.06877806287407875
Testing MSE:    0.06877806287407875
Testing MAE:    0.18097848358154298

Epoch  7
Training loss:  0.1439693482324481
Training MSE:   0.1439693482324481
Training MAE:   0.2849592582702637
Testing loss:   0.075656905823946
Testing MSE:    0.075656905823946
Testing MAE:    0.20681634521484374

Epoch  8
Training loss:  0.15872311853021384
Training MSE:   0.15872311853021384
Training MAE:   0.3061359508514404
Testing loss:   0.27404587960243226
Testing MSE:    0.27404587960243226
Testing MAE:    0.47485394134521486

Epoch  9
Training loss:  0.14682221833467485
Training MSE:   0.14682221833467485
Training MAE:   0.29625163879394534
Testing loss:   0.13060142111778258
Testing MSE:    0.13060142111778258
Testing MAE:    0.2986947463989258

Training loss:  0.13202175430357457
Training MSE:   0.13202175430357457
Training MAE:   0.298063676071167

Testing loss:  0.13060142111778258
Testing MSE:   0.13060142111778258
Testing MAE:   0.2986947463989258

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 15, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7548.825674518203
Training MSE:   7548.825674518203
Training MAE:   26.69512060775757
Testing loss:   13.3656585647583
Testing MSE:    13.3656585647583
Testing MAE:    2.890670101928711

Epoch  1
Training loss:  4.147916398692131
Training MSE:   4.147916398692131
Training MAE:   1.4786549980163575
Testing loss:   0.7240539900779724
Testing MSE:    0.7240539900779724
Testing MAE:    0.6669577072143554

Epoch  2
Training loss:  0.32626041696667674
Training MSE:   0.32626041696667674
Training MAE:   0.4164579891204834
Testing loss:   0.11305280232429504
Testing MSE:    0.11305280232429504
Testing MAE:    0.23251414184570313

Epoch  3
Training loss:  0.09477297815829515
Training MSE:   0.09477297815829515
Training MAE:   0.20394229316711426
Testing loss:   0.07399007726311684
Testing MSE:    0.07399007726311684
Testing MAE:    0.17371866912841796

Epoch  4
Training loss:  0.0799615788705647
Training MSE:   0.0799615788705647
Training MAE:   0.19189851722717285
Testing loss:   0.14848720021247863
Testing MSE:    0.14848720021247863
Testing MAE:    0.31014056549072266

Epoch  5
Training loss:  0.08220691919624805
Training MSE:   0.08220691919624805
Training MAE:   0.20363185729980468
Testing loss:   0.10264720709323882
Testing MSE:    0.10264720709323882
Testing MAE:    0.24287429351806641

Epoch  6
Training loss:  0.0814191788122058
Training MSE:   0.0814191788122058
Training MAE:   0.20791210517883302
Testing loss:   0.29654166922569275
Testing MSE:    0.29654166922569275
Testing MAE:    0.5003870483398437

Epoch  7
Training loss:  0.11384083595275879
Training MSE:   0.11384083595275879
Training MAE:   0.2575601676940918
Testing loss:   0.06767062171697616
Testing MSE:    0.06767062171697616
Testing MAE:    0.1903282196044922

Epoch  8
Training loss:  0.10840300922840834
Training MSE:   0.10840300922840834
Training MAE:   0.2511714595794678
Testing loss:   0.07337614232301712
Testing MSE:    0.07337614232301712
Testing MAE:    0.21500685424804689

Epoch  9
Training loss:  0.10796283979788422
Training MSE:   0.10796283979788422
Training MAE:   0.25409800605773925
Testing loss:   0.04431360730230808
Testing MSE:    0.04431360730230808
Testing MAE:    0.1528366714477539

Training loss:  0.04471247982382774
Training MSE:   0.04471247982382774
Training MAE:   0.15227706794738768

Testing loss:  0.04431360730230808
Testing MSE:   0.04431360730230808
Testing MAE:   0.1528366714477539

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 15, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10249.53100503006
Training MSE:   10249.53100503006
Training MAE:   35.97725221710205
Testing loss:   23.46458533630371
Testing MSE:    23.46458533630371
Testing MAE:    3.872916539001465

Epoch  1
Training loss:  18.728956172561645
Training MSE:   18.728956172561645
Training MAE:   3.458818151092529
Testing loss:   14.21301824798584
Testing MSE:    14.21301824798584
Testing MAE:    3.0599777420043943

Epoch  2
Training loss:  9.182802106666564
Training MSE:   9.182802106666564
Training MAE:   2.3938681518554685
Testing loss:   5.283144379425049
Testing MSE:    5.283144379425049
Testing MAE:    1.8647410415649415

Epoch  3
Training loss:  2.1589641621232034
Training MSE:   2.1589641621232034
Training MAE:   1.092852460861206
Testing loss:   0.5430048182487488
Testing MSE:    0.5430048182487488
Testing MAE:    0.5611961318969727

Epoch  4
Training loss:  0.2776115136951208
Training MSE:   0.2776115136951208
Training MAE:   0.3751792343139648
Testing loss:   0.16484442307949065
Testing MSE:    0.16484442307949065
Testing MAE:    0.30010530700683596

Epoch  5
Training loss:  0.10608713602721691
Training MSE:   0.10608713602721691
Training MAE:   0.21258402061462403
Testing loss:   0.07565949967503548
Testing MSE:    0.07565949967503548
Testing MAE:    0.174125341796875

Epoch  6
Training loss:  0.0846241717159748
Training MSE:   0.0846241717159748
Training MAE:   0.1955815414428711
Testing loss:   0.0657254723072052
Testing MSE:    0.0657254723072052
Testing MAE:    0.17125937652587891

Epoch  7
Training loss:  0.08538737069219351
Training MSE:   0.08538737069219351
Training MAE:   0.2080193172454834
Testing loss:   0.14053064153194428
Testing MSE:    0.14053064153194428
Testing MAE:    0.31378865051269533

Epoch  8
Training loss:  0.10438999455571174
Training MSE:   0.10438999455571174
Training MAE:   0.24101221618652344
Testing loss:   0.10860642914772034
Testing MSE:    0.10860642914772034
Testing MAE:    0.2588145263671875

Epoch  9
Training loss:  0.09613603440225124
Training MSE:   0.09613603440225124
Training MAE:   0.23187750053405762
Testing loss:   0.07973931000232697
Testing MSE:    0.07973931000232697
Testing MAE:    0.22477752227783204

Training loss:  0.08026603164970875
Training MSE:   0.08026603164970875
Training MAE:   0.22461635437011718

Testing loss:  0.07973931000232697
Testing MSE:   0.07973931000232697
Testing MAE:   0.22477752227783204

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 15, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10646.010025834656
Training MSE:   10646.010025834656
Training MAE:   39.81356120223999
Testing loss:   50.71135155639649
Testing MSE:    50.71135155639649
Testing MAE:    5.6878804718017575

Epoch  1
Training loss:  42.68206192321777
Training MSE:   42.68206192321777
Training MAE:   5.186348516464234
Testing loss:   34.286218988037106
Testing MSE:    34.286218988037106
Testing MAE:    4.659719329833984

Epoch  2
Training loss:  25.501339517211914
Training MSE:   25.501339517211914
Training MAE:   3.9866812858581544
Testing loss:   17.36505251159668
Testing MSE:    17.36505251159668
Testing MAE:    3.3340669143676758

Epoch  3
Training loss:  10.47463476524353
Training MSE:   10.47463476524353
Training MAE:   2.5194624366760254
Testing loss:   5.117730086517334
Testing MSE:    5.117730086517334
Testing MAE:    1.7885974884033202

Epoch  4
Training loss:  2.5307036205768587
Training MSE:   2.5307036205768587
Training MAE:   1.1927437496185302
Testing loss:   0.8338352692604065
Testing MSE:    0.8338352692604065
Testing MAE:    0.6929502380371094

Epoch  5
Training loss:  0.4015888360261917
Training MSE:   0.4015888360261917
Training MAE:   0.4548980930328369
Testing loss:   0.19279599351882934
Testing MSE:    0.19279599351882934
Testing MAE:    0.3318183029174805

Epoch  6
Training loss:  0.13248849566578866
Training MSE:   0.13248849566578866
Training MAE:   0.25314473609924315
Testing loss:   0.1129426084637642
Testing MSE:    0.1129426084637642
Testing MAE:    0.23233938751220704

Epoch  7
Training loss:  0.09952126573473215
Training MSE:   0.09952126573473215
Training MAE:   0.21665741424560547
Testing loss:   0.14307996735572814
Testing MSE:    0.14307996735572814
Testing MAE:    0.2887754180908203

Epoch  8
Training loss:  0.09967183069586753
Training MSE:   0.09967183069586753
Training MAE:   0.2249113277435303
Testing loss:   0.07419942023158073
Testing MSE:    0.07419942023158073
Testing MAE:    0.19604178619384766

Epoch  9
Training loss:  0.10709096584022045
Training MSE:   0.10709096584022045
Training MAE:   0.24119871711730956
Testing loss:   0.08963600476980209
Testing MSE:    0.08963600476980209
Testing MAE:    0.21981827697753906

Training loss:  0.09093627249449492
Training MSE:   0.09093627249449492
Training MAE:   0.21901022109985352

Testing loss:  0.08963600476980209
Testing MSE:   0.08963600476980209
Testing MAE:   0.21981827697753906

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 10, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  8429.852338690185
Training MSE:   8429.852338690185
Training MAE:   34.96977240791321
Testing loss:   96.84925600585937
Testing MSE:    96.84925600585937
Testing MAE:    7.750169293212891

Epoch  1
Training loss:  44.24493461699486
Training MSE:   44.24493461699486
Training MAE:   4.591372601699829
Testing loss:   1.00697983751297
Testing MSE:    1.00697983751297
Testing MAE:    0.7368421417236328

Epoch  2
Training loss:  0.29426699847877025
Training MSE:   0.29426699847877025
Training MAE:   0.36066480522155764
Testing loss:   0.1268315591096878
Testing MSE:    0.1268315591096878
Testing MAE:    0.22673337860107423

Epoch  3
Training loss:  0.10632973182499408
Training MSE:   0.10632973182499408
Training MAE:   0.22723114280700685
Testing loss:   0.0929141735315323
Testing MSE:    0.0929141735315323
Testing MAE:    0.20502974548339845

Epoch  4
Training loss:  0.09873658102452755
Training MSE:   0.09873658102452755
Training MAE:   0.22540188446044923
Testing loss:   0.0933234167277813
Testing MSE:    0.0933234167277813
Testing MAE:    0.2230748031616211

Epoch  5
Training loss:  0.09383476009517908
Training MSE:   0.09383476009517908
Training MAE:   0.22264864883422852
Testing loss:   0.07151746649742126
Testing MSE:    0.07151746649742126
Testing MAE:    0.18355610961914062

Epoch  6
Training loss:  0.0977440217986703
Training MSE:   0.0977440217986703
Training MAE:   0.23232722091674804
Testing loss:   0.17209329781532287
Testing MSE:    0.17209329781532287
Testing MAE:    0.35729901580810547

Epoch  7
Training loss:  0.12462288751974702
Training MSE:   0.12462288751974702
Training MAE:   0.26889227867126464
Testing loss:   0.06914588541984558
Testing MSE:    0.06914588541984558
Testing MAE:    0.19881089477539063

Epoch  8
Training loss:  0.12159723474234342
Training MSE:   0.12159723474234342
Training MAE:   0.26713710441589356
Testing loss:   0.04747784127891064
Testing MSE:    0.04747784127891064
Testing MAE:    0.14936218566894532

Epoch  9
Training loss:  0.12243299978598952
Training MSE:   0.12243299978598952
Training MAE:   0.2687796382904053
Testing loss:   0.08421102135181427
Testing MSE:    0.08421102135181427
Testing MAE:    0.22456878509521486

Training loss:  0.08406623774021864
Training MSE:   0.08406623774021864
Training MAE:   0.22402371597290038

Testing loss:  0.08421102135181427
Testing MSE:   0.08421102135181427
Testing MAE:   0.22456878509521486

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 10, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  6022.749658282471
Training MSE:   6022.749658282471
Training MAE:   23.655179391479493
Testing loss:   16.213778997802734
Testing MSE:    16.213778997802734
Testing MAE:    3.0680125915527343

Epoch  1
Training loss:  10.778250131988525
Training MSE:   10.778250131988525
Training MAE:   2.4735324378967287
Testing loss:   6.728405471801758
Testing MSE:    6.728405471801758
Testing MAE:    1.9614006454467774

Epoch  2
Training loss:  4.140530777025223
Training MSE:   4.140530777025223
Training MAE:   1.5216521244049073
Testing loss:   1.96792612323761
Testing MSE:    1.96792612323761
Testing MAE:    1.0790547790527343

Epoch  3
Training loss:  1.1679150511026382
Training MSE:   1.1679150511026382
Training MAE:   0.8189263565063477
Testing loss:   0.6802360893249512
Testing MSE:    0.6802360893249512
Testing MAE:    0.6651971115112305

Epoch  4
Training loss:  0.38334738683104513
Training MSE:   0.38334738683104513
Training MAE:   0.4657778289794922
Testing loss:   0.23661554913520813
Testing MSE:    0.23661554913520813
Testing MAE:    0.3585440612792969

Epoch  5
Training loss:  0.15641272113472224
Training MSE:   0.15641272113472224
Training MAE:   0.2814125534057617
Testing loss:   0.12025877710580826
Testing MSE:    0.12025877710580826
Testing MAE:    0.2426870330810547

Epoch  6
Training loss:  0.11250074911266565
Training MSE:   0.11250074911266565
Training MAE:   0.238253120803833
Testing loss:   0.08165253047943115
Testing MSE:    0.08165253047943115
Testing MAE:    0.20890015411376953

Epoch  7
Training loss:  0.12021415157020092
Training MSE:   0.12021415157020092
Training MAE:   0.2575161777496338
Testing loss:   0.0658613721549511
Testing MSE:    0.0658613721549511
Testing MAE:    0.1746267593383789

Epoch  8
Training loss:  0.1316491567403078
Training MSE:   0.1316491567403078
Training MAE:   0.27794785385131837
Testing loss:   0.15440692727565766
Testing MSE:    0.15440692727565766
Testing MAE:    0.3362889251708984

Epoch  9
Training loss:  0.125027560120821
Training MSE:   0.125027560120821
Training MAE:   0.2698448040008545
Testing loss:   0.15631872849464418
Testing MSE:    0.15631872849464418
Testing MAE:    0.3430831970214844

Training loss:  0.15639571530818938
Training MSE:   0.15639571530818938
Training MAE:   0.34434317550659177

Testing loss:  0.15631872849464418
Testing MSE:   0.15631872849464418
Testing MAE:   0.3430831970214844

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 10, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9871.327599024964
Training MSE:   9871.327599024964
Training MAE:   34.545046646881104
Testing loss:   17.850485734558106
Testing MSE:    17.850485734558106
Testing MAE:    3.1895410034179688

Epoch  1
Training loss:  12.511681739234925
Training MSE:   12.511681739234925
Training MAE:   2.6618993637084962
Testing loss:   7.3222910560607914
Testing MSE:    7.3222910560607914
Testing MAE:    2.0564730865478515

Epoch  2
Training loss:  3.8279143495082857
Training MSE:   3.8279143495082857
Training MAE:   1.4499430709838868
Testing loss:   1.3098181400299072
Testing MSE:    1.3098181400299072
Testing MAE:    0.8629239181518554

Epoch  3
Training loss:  0.5744390394926071
Training MSE:   0.5744390394926071
Training MAE:   0.5278287658691406
Testing loss:   0.20665721002817153
Testing MSE:    0.20665721002817153
Testing MAE:    0.31948021240234376

Epoch  4
Training loss:  0.16907276087403297
Training MSE:   0.16907276087403297
Training MAE:   0.2790076988220215
Testing loss:   0.22653479855060576
Testing MSE:    0.22653479855060576
Testing MAE:    0.361725927734375

Epoch  5
Training loss:  0.11976407391279936
Training MSE:   0.11976407391279936
Training MAE:   0.22823469200134278
Testing loss:   0.08379488167762757
Testing MSE:    0.08379488167762757
Testing MAE:    0.18437423248291016

Epoch  6
Training loss:  0.10196500607132912
Training MSE:   0.10196500607132912
Training MAE:   0.21686886978149414
Testing loss:   0.08465045218467712
Testing MSE:    0.08465045218467712
Testing MAE:    0.1952102569580078

Epoch  7
Training loss:  0.09665967303216458
Training MSE:   0.09665967303216458
Training MAE:   0.2200561954498291
Testing loss:   0.21300675048828124
Testing MSE:    0.21300675048828124
Testing MAE:    0.4060880859375

Epoch  8
Training loss:  0.10598736044168472
Training MSE:   0.10598736044168472
Training MAE:   0.2397268424987793
Testing loss:   0.14335925362110138
Testing MSE:    0.14335925362110138
Testing MAE:    0.31950204467773435

Epoch  9
Training loss:  0.11470772097706795
Training MSE:   0.11470772097706795
Training MAE:   0.2526216026306152
Testing loss:   0.08698121556043625
Testing MSE:    0.08698121556043625
Testing MAE:    0.22115406188964845

Training loss:  0.0896079740256071
Training MSE:   0.0896079740256071
Training MAE:   0.220937015914917

Testing loss:  0.08698121556043625
Testing MSE:   0.08698121556043625
Testing MAE:   0.22115406188964845

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 10, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9231.801019293212
Training MSE:   9231.801019293212
Training MAE:   36.3544575592041
Testing loss:   91.07867150268555
Testing MSE:    91.07867150268555
Testing MAE:    7.417819969177246

Epoch  1
Training loss:  53.26824412765503
Training MSE:   53.26824412765503
Training MAE:   5.581212933731079
Testing loss:   20.20910196685791
Testing MSE:    20.20910196685791
Testing MAE:    3.6028187850952147

Epoch  2
Training loss:  5.885891040110588
Training MSE:   5.885891040110588
Training MAE:   1.7295659923553466
Testing loss:   0.7866680465698243
Testing MSE:    0.7866680465698243
Testing MAE:    0.6908260894775391

Epoch  3
Training loss:  0.3892054066747427
Training MSE:   0.3892054066747427
Training MAE:   0.451673726272583
Testing loss:   0.1970699681520462
Testing MSE:    0.1970699681520462
Testing MAE:    0.31683173828125

Epoch  4
Training loss:  0.12799005728811025
Training MSE:   0.12799005728811025
Training MAE:   0.22974595031738282
Testing loss:   0.09529768794775009
Testing MSE:    0.09529768794775009
Testing MAE:    0.19412759857177733

Epoch  5
Training loss:  0.08730152970328928
Training MSE:   0.08730152970328928
Training MAE:   0.18573529510498046
Testing loss:   0.07228976352214814
Testing MSE:    0.07228976352214814
Testing MAE:    0.15968421173095704

Epoch  6
Training loss:  0.08472570512145758
Training MSE:   0.08472570512145758
Training MAE:   0.19808361206054687
Testing loss:   0.062069697338342666
Testing MSE:    0.062069697338342666
Testing MAE:    0.1615096954345703

Epoch  7
Training loss:  0.0925698185145855
Training MSE:   0.0925698185145855
Training MAE:   0.22102583389282227
Testing loss:   0.12907220059633254
Testing MSE:    0.12907220059633254
Testing MAE:    0.299908854675293

Epoch  8
Training loss:  0.09541116178631782
Training MSE:   0.09541116178631782
Training MAE:   0.22768462333679199
Testing loss:   0.05319980838894844
Testing MSE:    0.05319980838894844
Testing MAE:    0.164135546875

Epoch  9
Training loss:  0.10707931421324611
Training MSE:   0.10707931421324611
Training MAE:   0.24853359031677247
Testing loss:   0.04925150871276855
Testing MSE:    0.04925150871276855
Testing MAE:    0.16028372802734375

Training loss:  0.04918001406565309
Training MSE:   0.04918001406565309
Training MAE:   0.15952938652038573

Testing loss:  0.04925150871276855
Testing MSE:   0.04925150871276855
Testing MAE:   0.16028372802734375

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 10, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9529.360418112945
Training MSE:   9529.360418112945
Training MAE:   33.65231536674499
Testing loss:   18.50822554016113
Testing MSE:    18.50822554016113
Testing MAE:    3.3976081161499025

Epoch  1
Training loss:  13.717738221740722
Training MSE:   13.717738221740722
Training MAE:   2.912634080505371
Testing loss:   9.154081160736084
Testing MSE:    9.154081160736084
Testing MAE:    2.397083122253418

Epoch  2
Training loss:  6.247019708251953
Training MSE:   6.247019708251953
Training MAE:   1.9537976280212401
Testing loss:   3.714575556945801
Testing MSE:    3.714575556945801
Testing MAE:    1.519760823059082

Epoch  3
Training loss:  2.4028041749954223
Training MSE:   2.4028041749954223
Training MAE:   1.2011609195709227
Testing loss:   1.3742296842575072
Testing MSE:    1.3742296842575072
Testing MAE:    0.9151458389282227

Epoch  4
Training loss:  0.7772330640435219
Training MSE:   0.7772330640435219
Training MAE:   0.6716961017608643
Testing loss:   0.3832118885040283
Testing MSE:    0.3832118885040283
Testing MAE:    0.47422444610595704

Epoch  5
Training loss:  0.214979617574811
Training MSE:   0.214979617574811
Training MAE:   0.33600725021362304
Testing loss:   0.11273344786167144
Testing MSE:    0.11273344786167144
Testing MAE:    0.23318698272705077

Epoch  6
Training loss:  0.10681507970541716
Training MSE:   0.10681507970541716
Training MAE:   0.2227086772918701
Testing loss:   0.09425537388324738
Testing MSE:    0.09425537388324738
Testing MAE:    0.22370797424316408

Epoch  7
Training loss:  0.09619933247566223
Training MSE:   0.09619933247566223
Training MAE:   0.21989776153564453
Testing loss:   0.06040282362103462
Testing MSE:    0.06040282362103462
Testing MAE:    0.16496776580810546

Epoch  8
Training loss:  0.1022210254818201
Training MSE:   0.1022210254818201
Training MAE:   0.23549517822265625
Testing loss:   0.0615768725335598
Testing MSE:    0.0615768725335598
Testing MAE:    0.18154742736816407

Epoch  9
Training loss:  0.103750328142941
Training MSE:   0.103750328142941
Training MAE:   0.24338804626464844
Testing loss:   0.06348634960055351
Testing MSE:    0.06348634960055351
Testing MAE:    0.18192000732421876

Training loss:  0.06443342483192682
Training MSE:   0.06443342483192682
Training MAE:   0.18142180824279786

Testing loss:  0.06348634960055351
Testing MSE:   0.06348634960055351
Testing MAE:   0.18192000732421876

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 5, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  18540.01906654358
Training MSE:   18540.01906654358
Training MAE:   56.22461203804016
Testing loss:   24.40098360595703
Testing MSE:    24.40098360595703
Testing MAE:    3.9155705917358397

Epoch  1
Training loss:  15.322117284584046
Training MSE:   15.322117284584046
Training MAE:   3.0639461296081545
Testing loss:   6.929728561401367
Testing MSE:    6.929728561401367
Testing MAE:    2.07359130859375

Epoch  2
Training loss:  3.4027049820423128
Training MSE:   3.4027049820423128
Training MAE:   1.4011215698242188
Testing loss:   1.1478435296058656
Testing MSE:    1.1478435296058656
Testing MAE:    0.8472172729492188

Epoch  3
Training loss:  0.47330973784327507
Training MSE:   0.47330973784327507
Training MAE:   0.5092363307952881
Testing loss:   0.17259237608909606
Testing MSE:    0.17259237608909606
Testing MAE:    0.31531048889160157

Epoch  4
Training loss:  0.1326489085674286
Training MSE:   0.1326489085674286
Training MAE:   0.2530025638580322
Testing loss:   0.09594352191686631
Testing MSE:    0.09594352191686631
Testing MAE:    0.2204779571533203

Epoch  5
Training loss:  0.08838056864887477
Training MSE:   0.08838056864887477
Training MAE:   0.2067844093322754
Testing loss:   0.09505027869939804
Testing MSE:    0.09505027869939804
Testing MAE:    0.2364995620727539

Epoch  6
Training loss:  0.07794697299078107
Training MSE:   0.07794697299078107
Training MAE:   0.20257509841918944
Testing loss:   0.0523340264081955
Testing MSE:    0.0523340264081955
Testing MAE:    0.1688801010131836

Epoch  7
Training loss:  0.07940316999480128
Training MSE:   0.07940316999480128
Training MAE:   0.21042890548706056
Testing loss:   0.0432603451102972
Testing MSE:    0.0432603451102972
Testing MAE:    0.14362050018310546

Epoch  8
Training loss:  0.0849782591663301
Training MSE:   0.0849782591663301
Training MAE:   0.22021661224365235
Testing loss:   0.040638565436005594
Testing MSE:    0.040638565436005594
Testing MAE:    0.14408231048583983

Epoch  9
Training loss:  0.08435957664251327
Training MSE:   0.08435957664251327
Training MAE:   0.2221128288269043
Testing loss:   0.03787436427474022
Testing MSE:    0.03787436427474022
Testing MAE:    0.13873650360107423

Training loss:  0.03813923872485757
Training MSE:   0.03813923872485757
Training MAE:   0.13794877471923828

Testing loss:  0.03787436427474022
Testing MSE:   0.03787436427474022
Testing MAE:   0.13873650360107423

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 5, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10934.597246337891
Training MSE:   10934.597246337891
Training MAE:   36.789366422271726
Testing loss:   30.401965567016603
Testing MSE:    30.401965567016603
Testing MAE:    4.379567451477051

Epoch  1
Training loss:  22.293316674423217
Training MSE:   22.293316674423217
Training MAE:   3.73449694480896
Testing loss:   14.309254592895508
Testing MSE:    14.309254592895508
Testing MAE:    2.992006195068359

Epoch  2
Training loss:  7.97792563419342
Training MSE:   7.97792563419342
Training MAE:   2.1902611446380615
Testing loss:   3.1544012672424318
Testing MSE:    3.1544012672424318
Testing MAE:    1.3886493087768554

Epoch  3
Training loss:  1.2978378916740418
Training MSE:   1.2978378916740418
Training MAE:   0.8416750923156738
Testing loss:   0.38358026225566866
Testing MSE:    0.38358026225566866
Testing MAE:    0.45713414611816405

Epoch  4
Training loss:  0.2418273197889328
Training MSE:   0.2418273197889328
Training MAE:   0.3550872016906738
Testing loss:   0.15463354699611664
Testing MSE:    0.15463354699611664
Testing MAE:    0.2803786148071289

Epoch  5
Training loss:  0.13530730452239514
Training MSE:   0.13530730452239514
Training MAE:   0.25521690368652344
Testing loss:   0.11198591275215149
Testing MSE:    0.11198591275215149
Testing MAE:    0.24186198425292968

Epoch  6
Training loss:  0.09352039606273174
Training MSE:   0.09352039606273174
Training MAE:   0.20547477951049806
Testing loss:   0.070202910476923
Testing MSE:    0.070202910476923
Testing MAE:    0.17430591583251953

Epoch  7
Training loss:  0.09006041395068169
Training MSE:   0.09006041395068169
Training MAE:   0.21147151565551758
Testing loss:   0.07405946980118752
Testing MSE:    0.07405946980118752
Testing MAE:    0.18489774322509767

Epoch  8
Training loss:  0.09928474694341421
Training MSE:   0.09928474694341421
Training MAE:   0.2305629871368408
Testing loss:   0.08714375366568565
Testing MSE:    0.08714375366568565
Testing MAE:    0.21705293884277344

Epoch  9
Training loss:  0.10191695831418038
Training MSE:   0.10191695831418038
Training MAE:   0.23971372184753417
Testing loss:   0.07226682176589966
Testing MSE:    0.07226682176589966
Testing MAE:    0.19340673828125

Training loss:  0.07303432898819447
Training MSE:   0.07303432898819447
Training MAE:   0.19320601234436036

Testing loss:  0.07226682176589966
Testing MSE:   0.07226682176589966
Testing MAE:   0.19340673828125

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 5, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9216.710026715087
Training MSE:   9216.710026715087
Training MAE:   34.122334519577024
Testing loss:   34.36028942260742
Testing MSE:    34.36028942260742
Testing MAE:    4.315039962768554

Epoch  1
Training loss:  26.361398584365844
Training MSE:   26.361398584365844
Training MAE:   3.75475943069458
Testing loss:   17.14951108703613
Testing MSE:    17.14951108703613
Testing MAE:    3.072644647216797

Epoch  2
Training loss:  12.47130952720642
Training MSE:   12.47130952720642
Training MAE:   2.6111377990722655
Testing loss:   8.006817031097412
Testing MSE:    8.006817031097412
Testing MAE:    2.1472901260375976

Epoch  3
Training loss:  5.289467253017426
Training MSE:   5.289467253017426
Training MAE:   1.7104562019348144
Testing loss:   3.235651847457886
Testing MSE:    3.235651847457886
Testing MAE:    1.3901433532714844

Epoch  4
Training loss:  1.79451650929451
Training MSE:   1.79451650929451
Training MAE:   0.9958813850402832
Testing loss:   0.8263297141075134
Testing MSE:    0.8263297141075134
Testing MAE:    0.6838293746948242

Epoch  5
Training loss:  0.5301571851313114
Training MSE:   0.5301571851313114
Training MAE:   0.5367373252868652
Testing loss:   0.2720699314594269
Testing MSE:    0.2720699314594269
Testing MAE:    0.3719339660644531

Epoch  6
Training loss:  0.19832122755646706
Training MSE:   0.19832122755646706
Training MAE:   0.31130731353759766
Testing loss:   0.12195655978918075
Testing MSE:    0.12195655978918075
Testing MAE:    0.2382222396850586

Epoch  7
Training loss:  0.12232042369246483
Training MSE:   0.12232042369246483
Training MAE:   0.23952833671569823
Testing loss:   0.11415441122055053
Testing MSE:    0.11415441122055053
Testing MAE:    0.2571527740478516

Epoch  8
Training loss:  0.10438251003473997
Training MSE:   0.10438251003473997
Training MAE:   0.23013693504333496
Testing loss:   0.07018731881976127
Testing MSE:    0.07018731881976127
Testing MAE:    0.17950345001220702

Epoch  9
Training loss:  0.10492454531639814
Training MSE:   0.10492454531639814
Training MAE:   0.23982575302124023
Testing loss:   0.057436583429574965
Testing MSE:    0.057436583429574965
Testing MAE:    0.16158070220947265

Training loss:  0.058563514085114
Training MSE:   0.058563514085114
Training MAE:   0.1604427703857422

Testing loss:  0.057436583429574965
Testing MSE:   0.057436583429574965
Testing MAE:   0.16158070220947265

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 5, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  12913.919727552033
Training MSE:   12913.919727552033
Training MAE:   44.15952466621399
Testing loss:   26.10156389465332
Testing MSE:    26.10156389465332
Testing MAE:    4.144180978393555

Epoch  1
Training loss:  21.514066610717773
Training MSE:   21.514066610717773
Training MAE:   3.7454923347473144
Testing loss:   16.789256202697754
Testing MSE:    16.789256202697754
Testing MAE:    3.347886178588867

Epoch  2
Training loss:  11.608791538238526
Training MSE:   11.608791538238526
Training MAE:   2.7346895092010497
Testing loss:   6.971352502441406
Testing MSE:    6.971352502441406
Testing MAE:    2.1365251586914065

Epoch  3
Training loss:  3.952456908464432
Training MSE:   3.952456908464432
Training MAE:   1.564026904296875
Testing loss:   1.8552428657531739
Testing MSE:    1.8552428657531739
Testing MAE:    1.0955182983398437

Epoch  4
Training loss:  0.9098455799460411
Training MSE:   0.9098455799460411
Training MAE:   0.7395379459381104
Testing loss:   0.40061150522232053
Testing MSE:    0.40061150522232053
Testing MAE:    0.49511503448486327

Epoch  5
Training loss:  0.24331698730289936
Training MSE:   0.24331698730289936
Training MAE:   0.37436023445129396
Testing loss:   0.15057930029630662
Testing MSE:    0.15057930029630662
Testing MAE:    0.2824057174682617

Epoch  6
Training loss:  0.10818278574198484
Training MSE:   0.10818278574198484
Training MAE:   0.23938548164367676
Testing loss:   0.1118135991692543
Testing MSE:    0.1118135991692543
Testing MAE:    0.2395619598388672

Epoch  7
Training loss:  0.0949819649130106
Training MSE:   0.0949819649130106
Training MAE:   0.22671342544555664
Testing loss:   0.1868135098695755
Testing MSE:    0.1868135098695755
Testing MAE:    0.35842004852294923

Epoch  8
Training loss:  0.10481739543452859
Training MSE:   0.10481739543452859
Training MAE:   0.24437820739746094
Testing loss:   0.13115582307577134
Testing MSE:    0.13115582307577134
Testing MAE:    0.2878116973876953

Epoch  9
Training loss:  0.09322238907217979
Training MSE:   0.09322238907217979
Training MAE:   0.23034702758789063
Testing loss:   0.12205218139886856
Testing MSE:    0.12205218139886856
Testing MAE:    0.27969119720458985

Training loss:  0.12090843944251538
Training MSE:   0.12090843944251538
Training MAE:   0.279087992477417

Testing loss:  0.12205218139886856
Testing MSE:   0.12205218139886856
Testing MAE:   0.27969119720458985

Number of layers:  3
Number of units in layer [1,2,3]:  [10, 5, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  8314.44521249237
Training MSE:   8314.44521249237
Training MAE:   34.65390023727417
Testing loss:   51.05067710571289
Testing MSE:    51.05067710571289
Testing MAE:    5.622435525512695

Epoch  1
Training loss:  39.12363990478516
Training MSE:   39.12363990478516
Training MAE:   4.90557138710022
Testing loss:   26.439032305908203
Testing MSE:    26.439032305908203
Testing MAE:    4.036746005249023

Epoch  2
Training loss:  18.782433644485472
Training MSE:   18.782433644485472
Training MAE:   3.3807999210357664
Testing loss:   11.378600747680665
Testing MSE:    11.378600747680665
Testing MAE:    2.6190116622924804

Epoch  3
Training loss:  7.3117000782012935
Training MSE:   7.3117000782012935
Training MAE:   2.0790030895233156
Testing loss:   3.6024675510406494
Testing MSE:    3.6024675510406494
Testing MAE:    1.463323406982422

Epoch  4
Training loss:  2.03784927649498
Training MSE:   2.03784927649498
Training MAE:   1.0767706985473633
Testing loss:   0.9016429456710815
Testing MSE:    0.9016429456710815
Testing MAE:    0.7160409454345703

Epoch  5
Training loss:  0.49934577295184135
Training MSE:   0.49934577295184135
Training MAE:   0.5175628139495849
Testing loss:   0.24880090410709382
Testing MSE:    0.24880090410709382
Testing MAE:    0.351644108581543

Epoch  6
Training loss:  0.16680848426073788
Training MSE:   0.16680848426073788
Training MAE:   0.2725058235168457
Testing loss:   0.11943920748829842
Testing MSE:    0.11943920748829842
Testing MAE:    0.2154077117919922

Epoch  7
Training loss:  0.0991823468990624
Training MSE:   0.0991823468990624
Training MAE:   0.2072870616912842
Testing loss:   0.07927631179094315
Testing MSE:    0.07927631179094315
Testing MAE:    0.1640589080810547

Epoch  8
Training loss:  0.10120511726886033
Training MSE:   0.10120511726886033
Training MAE:   0.2275012077331543
Testing loss:   0.1265265550494194
Testing MSE:    0.1265265550494194
Testing MAE:    0.27395928192138674

Epoch  9
Training loss:  0.08947653629258275
Training MSE:   0.08947653629258275
Training MAE:   0.21841430397033693
Testing loss:   0.08295829033255576
Testing MSE:    0.08295829033255576
Testing MAE:    0.19785802154541016

Training loss:  0.07288281036764384
Training MSE:   0.07288281036764384
Training MAE:   0.19606913757324218

Testing loss:  0.08295829033255576
Testing MSE:   0.08295829033255576
Testing MAE:   0.19785802154541016

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 25, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7934.330523951721
Training MSE:   7934.330523951721
Training MAE:   32.56745298042297
Testing loss:   85.83025908813477
Testing MSE:    85.83025908813477
Testing MAE:    7.463605538940429

Epoch  1
Training loss:  45.19423362216949
Training MSE:   45.19423362216949
Training MAE:   4.99440892906189
Testing loss:   4.441315117645264
Testing MSE:    4.441315117645264
Testing MAE:    1.6543729461669923

Epoch  2
Training loss:  1.1783867865979671
Training MSE:   1.1783867865979671
Training MAE:   0.7453687179565429
Testing loss:   0.1525782992899418
Testing MSE:    0.1525782992899418
Testing MAE:    0.2638850173950195

Epoch  3
Training loss:  0.10148626345694065
Training MSE:   0.10148626345694065
Training MAE:   0.20712899589538575
Testing loss:   0.07183890353143216
Testing MSE:    0.07183890353143216
Testing MAE:    0.160236865234375

Epoch  4
Training loss:  0.07971674600988626
Training MSE:   0.07971674600988626
Training MAE:   0.18876558837890625
Testing loss:   0.07790676227807998
Testing MSE:    0.07790676227807998
Testing MAE:    0.18577792205810548

Epoch  5
Training loss:  0.07683298814743757
Training MSE:   0.07683298814743757
Training MAE:   0.1931582790374756
Testing loss:   0.05770641568303108
Testing MSE:    0.05770641568303108
Testing MAE:    0.15650509948730468

Epoch  6
Training loss:  0.07794661581516266
Training MSE:   0.07794661581516266
Training MAE:   0.2020761547088623
Testing loss:   0.0698728823184967
Testing MSE:    0.0698728823184967
Testing MAE:    0.1916737014770508

Epoch  7
Training loss:  0.08585102680996061
Training MSE:   0.08585102680996061
Training MAE:   0.21770917434692383
Testing loss:   0.24645805711746216
Testing MSE:    0.24645805711746216
Testing MAE:    0.4582240676879883

Epoch  8
Training loss:  0.11361408787295223
Training MSE:   0.11361408787295223
Training MAE:   0.25805649528503416
Testing loss:   0.06314332639575004
Testing MSE:    0.06314332639575004
Testing MAE:    0.19751370544433594

Epoch  9
Training loss:  0.08861131009608507
Training MSE:   0.08861131009608507
Training MAE:   0.22832308235168458
Testing loss:   0.1624732799053192
Testing MSE:    0.1624732799053192
Testing MAE:    0.3656878158569336

Training loss:  0.16174091358184814
Training MSE:   0.16174091358184814
Training MAE:   0.36589778938293455

Testing loss:  0.1624732799053192
Testing MSE:   0.1624732799053192
Testing MAE:   0.3656878158569336

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 25, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  5884.248092660522
Training MSE:   5884.248092660522
Training MAE:   23.776295575714112
Testing loss:   25.178083850097657
Testing MSE:    25.178083850097657
Testing MAE:    3.8841049514770507

Epoch  1
Training loss:  16.376444954490662
Training MSE:   16.376444954490662
Training MAE:   3.0854969482421875
Testing loss:   7.568399425506592
Testing MSE:    7.568399425506592
Testing MAE:    2.1679530197143553

Epoch  2
Training loss:  2.369257149231434
Training MSE:   2.369257149231434
Training MAE:   1.0972083225250244
Testing loss:   0.5002976967811584
Testing MSE:    0.5002976967811584
Testing MAE:    0.57628779296875

Epoch  3
Training loss:  0.221693609303236
Training MSE:   0.221693609303236
Training MAE:   0.34550932540893553
Testing loss:   0.4001591980457306
Testing MSE:    0.4001591980457306
Testing MAE:    0.5381672119140625

Epoch  4
Training loss:  0.15226020776331425
Training MSE:   0.15226020776331425
Training MAE:   0.28425978279113767
Testing loss:   0.18452520153522492
Testing MSE:    0.18452520153522492
Testing MAE:    0.33359661407470703

Epoch  5
Training loss:  0.14322158261835574
Training MSE:   0.14322158261835574
Training MAE:   0.27935456886291504
Testing loss:   0.08716840811371804
Testing MSE:    0.08716840811371804
Testing MAE:    0.21312203674316407

Epoch  6
Training loss:  0.1484911977469921
Training MSE:   0.1484911977469921
Training MAE:   0.28965151710510256
Testing loss:   0.07841419596672058
Testing MSE:    0.07841419596672058
Testing MAE:    0.2025403091430664

Epoch  7
Training loss:  0.17246160572320224
Training MSE:   0.17246160572320224
Training MAE:   0.3203810745239258
Testing loss:   0.10369908901453018
Testing MSE:    0.10369908901453018
Testing MAE:    0.2550703903198242

Epoch  8
Training loss:  0.16976769189685584
Training MSE:   0.16976769189685584
Training MAE:   0.31750870933532716
Testing loss:   0.1751214626789093
Testing MSE:    0.1751214626789093
Testing MAE:    0.3564367889404297

Epoch  9
Training loss:  0.15977568076848983
Training MSE:   0.15977568076848983
Training MAE:   0.30923438148498533
Testing loss:   0.06049092957377434
Testing MSE:    0.06049092957377434
Testing MAE:    0.1803706817626953

Training loss:  0.06110356075912714
Training MSE:   0.06110356075912714
Training MAE:   0.17860806694030762

Testing loss:  0.06049092957377434
Testing MSE:   0.06049092957377434
Testing MAE:   0.1803706817626953

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 25, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  8895.148441222382
Training MSE:   8895.148441222382
Training MAE:   31.452198291397096
Testing loss:   24.87141290588379
Testing MSE:    24.87141290588379
Testing MAE:    3.959853128051758

Epoch  1
Training loss:  20.052129807281496
Training MSE:   20.052129807281496
Training MAE:   3.55328242149353
Testing loss:   14.622136480712891
Testing MSE:    14.622136480712891
Testing MAE:    3.0248074401855467

Epoch  2
Training loss:  10.881542592048644
Training MSE:   10.881542592048644
Training MAE:   2.613488172531128
Testing loss:   7.047989810943603
Testing MSE:    7.047989810943603
Testing MAE:    2.1105051361083986

Epoch  3
Training loss:  4.772836429023743
Training MSE:   4.772836429023743
Training MAE:   1.7216443336486817
Testing loss:   2.606839402580261
Testing MSE:    2.606839402580261
Testing MAE:    1.2859054794311524

Epoch  4
Training loss:  1.5214875891685486
Training MSE:   1.5214875891685486
Training MAE:   0.9525469947814942
Testing loss:   0.6383984854698181
Testing MSE:    0.6383984854698181
Testing MAE:    0.6238392532348633

Epoch  5
Training loss:  0.37951659077703953
Training MSE:   0.37951659077703953
Training MAE:   0.44900766143798826
Testing loss:   0.17081723309755326
Testing MSE:    0.17081723309755326
Testing MAE:    0.2758103561401367

Epoch  6
Training loss:  0.1563727529153228
Training MSE:   0.1563727529153228
Training MAE:   0.269487805557251
Testing loss:   0.1549011271238327
Testing MSE:    0.1549011271238327
Testing MAE:    0.3156493255615234

Epoch  7
Training loss:  0.10464610279724001
Training MSE:   0.10464610279724001
Training MAE:   0.22533962936401367
Testing loss:   0.06248908776044845
Testing MSE:    0.06248908776044845
Testing MAE:    0.16796362762451172

Epoch  8
Training loss:  0.10645602074190974
Training MSE:   0.10645602074190974
Training MAE:   0.24238315773010255
Testing loss:   0.05616242272257805
Testing MSE:    0.05616242272257805
Testing MAE:    0.16902388763427734

Epoch  9
Training loss:  0.10588495102673769
Training MSE:   0.10588495102673769
Training MAE:   0.24630911865234376
Testing loss:   0.10557432932853698
Testing MSE:    0.10557432932853698
Testing MAE:    0.2724917755126953

Training loss:  0.10514070819616318
Training MSE:   0.10514070819616318
Training MAE:   0.2730703365325928

Testing loss:  0.10557432932853698
Testing MSE:   0.10557432932853698
Testing MAE:   0.2724917755126953

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 25, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  16854.132481118773
Training MSE:   16854.132481118773
Training MAE:   52.25830231704712
Testing loss:   33.38244663696289
Testing MSE:    33.38244663696289
Testing MAE:    4.652920205688477

Epoch  1
Training loss:  26.78087191848755
Training MSE:   26.78087191848755
Training MAE:   4.146173494338989
Testing loss:   20.03608140258789
Testing MSE:    20.03608140258789
Testing MAE:    3.6011315994262696

Epoch  2
Training loss:  14.793388125228882
Training MSE:   14.793388125228882
Training MAE:   3.058870343399048
Testing loss:   9.82265020904541
Testing MSE:    9.82265020904541
Testing MAE:    2.508745393371582

Epoch  3
Training loss:  6.38122190208435
Training MSE:   6.38122190208435
Training MAE:   1.97653125
Testing loss:   3.3275241271972655
Testing MSE:    3.3275241271972655
Testing MAE:    1.423632894897461

Epoch  4
Training loss:  1.9701600979089737
Training MSE:   1.9701600979089737
Training MAE:   1.06902476272583
Testing loss:   0.9171398579597473
Testing MSE:    0.9171398579597473
Testing MAE:    0.7194145385742188

Epoch  5
Training loss:  0.5402247783124446
Training MSE:   0.5402247783124446
Training MAE:   0.5327644844055176
Testing loss:   0.25977339854240417
Testing MSE:    0.25977339854240417
Testing MAE:    0.3595853469848633

Epoch  6
Training loss:  0.19791854113042354
Training MSE:   0.19791854113042354
Training MAE:   0.2944853065490723
Testing loss:   0.13462682849764823
Testing MSE:    0.13462682849764823
Testing MAE:    0.22548656616210938

Epoch  7
Training loss:  0.1260104421183467
Training MSE:   0.1260104421183467
Training MAE:   0.22526542434692381
Testing loss:   0.12430929691791534
Testing MSE:    0.12430929691791534
Testing MAE:    0.23705111694335937

Epoch  8
Training loss:  0.10876449543908238
Training MSE:   0.10876449543908238
Training MAE:   0.22098660964965822
Testing loss:   0.10542787141799927
Testing MSE:    0.10542787141799927
Testing MAE:    0.22616288299560547

Epoch  9
Training loss:  0.09675476234033704
Training MSE:   0.09675476234033704
Training MAE:   0.2153536029815674
Testing loss:   0.1004677139878273
Testing MSE:    0.1004677139878273
Testing MAE:    0.24605728912353517

Training loss:  0.09910237271487712
Training MSE:   0.09910237271487712
Training MAE:   0.2455314697265625

Testing loss:  0.1004677139878273
Testing MSE:   0.1004677139878273
Testing MAE:   0.24605728912353517

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 25, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  8846.413750509644
Training MSE:   8846.413750509644
Training MAE:   32.603722731781005
Testing loss:   17.77507398376465
Testing MSE:    17.77507398376465
Testing MAE:    3.238433514404297

Epoch  1
Training loss:  16.147122105979918
Training MSE:   16.147122105979918
Training MAE:   3.0575262840271
Testing loss:   12.688017028808593
Testing MSE:    12.688017028808593
Testing MAE:    2.724211543273926

Epoch  2
Training loss:  10.537150975227355
Training MSE:   10.537150975227355
Training MAE:   2.453028366088867
Testing loss:   7.358661876678466
Testing MSE:    7.358661876678466
Testing MAE:    2.057303746032715

Epoch  3
Training loss:  5.4771776208877565
Training MSE:   5.4771776208877565
Training MAE:   1.750490242767334
Testing loss:   3.1609343128204346
Testing MSE:    3.1609343128204346
Testing MAE:    1.3304140090942382

Epoch  4
Training loss:  2.0164633125782014
Training MSE:   2.0164633125782014
Training MAE:   1.0497563381195067
Testing loss:   0.9814882248878479
Testing MSE:    0.9814882248878479
Testing MAE:    0.7252603530883789

Epoch  5
Training loss:  0.5860068433344364
Training MSE:   0.5860068433344364
Training MAE:   0.553492936706543
Testing loss:   0.2802742644190788
Testing MSE:    0.2802742644190788
Testing MAE:    0.3743394027709961

Epoch  6
Training loss:  0.21835845171511173
Training MSE:   0.21835845171511173
Training MAE:   0.3204075855255127
Testing loss:   0.13259825286865234
Testing MSE:    0.13259825286865234
Testing MAE:    0.22602757415771485

Epoch  7
Training loss:  0.13682633506804703
Training MSE:   0.13682633506804703
Training MAE:   0.24547187881469726
Testing loss:   0.09194773492217063
Testing MSE:    0.09194773492217063
Testing MAE:    0.18715846405029296

Epoch  8
Training loss:  0.11528887906968593
Training MSE:   0.11528887906968593
Training MAE:   0.23369713401794434
Testing loss:   0.12625705922842026
Testing MSE:    0.12625705922842026
Testing MAE:    0.2596462188720703

Epoch  9
Training loss:  0.11176214643120766
Training MSE:   0.11176214643120766
Training MAE:   0.24066518287658692
Testing loss:   0.0654001992225647
Testing MSE:    0.0654001992225647
Testing MAE:    0.171429443359375

Training loss:  0.06429999458491802
Training MSE:   0.06429999458491802
Training MAE:   0.17055178527832032

Testing loss:  0.0654001992225647
Testing MSE:   0.0654001992225647
Testing MAE:   0.171429443359375

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 20, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  12049.92667784729
Training MSE:   12049.92667784729
Training MAE:   41.377693287658694
Testing loss:   96.36842153930664
Testing MSE:    96.36842153930664
Testing MAE:    7.673370332336426

Epoch  1
Training loss:  63.63143453826904
Training MSE:   63.63143453826904
Training MAE:   6.314250830841065
Testing loss:   37.22362798461914
Testing MSE:    37.22362798461914
Testing MAE:    4.908925660705567

Epoch  2
Training loss:  21.22658230819702
Training MSE:   21.22658230819702
Training MAE:   3.6401717895507812
Testing loss:   8.679124435424805
Testing MSE:    8.679124435424805
Testing MAE:    2.374010693359375

Epoch  3
Training loss:  3.8087132757902147
Training MSE:   3.8087132757902147
Training MAE:   1.49772848777771
Testing loss:   0.9438484901428222
Testing MSE:    0.9438484901428222
Testing MAE:    0.7545720657348632

Epoch  4
Training loss:  0.46457225722670553
Training MSE:   0.46457225722670553
Training MAE:   0.5022567848205567
Testing loss:   0.2662983754634857
Testing MSE:    0.2662983754634857
Testing MAE:    0.410164192199707

Epoch  5
Training loss:  0.2124174930125475
Training MSE:   0.2124174930125475
Training MAE:   0.3400469707489014
Testing loss:   0.1754238561987877
Testing MSE:    0.1754238561987877
Testing MAE:    0.3087365295410156

Epoch  6
Training loss:  0.1847371040046215
Training MSE:   0.1847371040046215
Training MAE:   0.3226392150878906
Testing loss:   0.2595109968185425
Testing MSE:    0.2595109968185425
Testing MAE:    0.4371627151489258

Epoch  7
Training loss:  0.19304077359437943
Training MSE:   0.19304077359437943
Training MAE:   0.3365966136932373
Testing loss:   0.12265970153808593
Testing MSE:    0.12265970153808593
Testing MAE:    0.2603201416015625

Epoch  8
Training loss:  0.18281310069561005
Training MSE:   0.18281310069561005
Training MAE:   0.3298669303894043
Testing loss:   0.07952254080176353
Testing MSE:    0.07952254080176353
Testing MAE:    0.203089306640625

Epoch  9
Training loss:  0.18469770422130824
Training MSE:   0.18469770422130824
Training MAE:   0.3320451705932617
Testing loss:   0.14147634553909302
Testing MSE:    0.14147634553909302
Testing MAE:    0.3134720230102539

Training loss:  0.1428237975746393
Training MSE:   0.1428237975746393
Training MAE:   0.31447241401672366

Testing loss:  0.14147634553909302
Testing MSE:   0.14147634553909302
Testing MAE:   0.3134720230102539

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 20, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  18164.690244114685
Training MSE:   18164.690244114685
Training MAE:   55.94891200332641
Testing loss:   51.98128617553711
Testing MSE:    51.98128617553711
Testing MAE:    5.751158744812011

Epoch  1
Training loss:  25.888493647766115
Training MSE:   25.888493647766115
Training MAE:   3.9333221019744875
Testing loss:   8.193183290863038
Testing MSE:    8.193183290863038
Testing MAE:    2.2573789443969727

Epoch  2
Training loss:  2.6929227294504643
Training MSE:   2.6929227294504643
Training MAE:   1.159706267929077
Testing loss:   0.3152658625602722
Testing MSE:    0.3152658625602722
Testing MAE:    0.40009771423339846

Epoch  3
Training loss:  0.12901799663454294
Training MSE:   0.12901799663454294
Training MAE:   0.24430885391235352
Testing loss:   0.08962031298875808
Testing MSE:    0.08962031298875808
Testing MAE:    0.21657587280273438

Epoch  4
Training loss:  0.06481985308602452
Training MSE:   0.06481985308602452
Training MAE:   0.1749785053253174
Testing loss:   0.054481788831949235
Testing MSE:    0.054481788831949235
Testing MAE:    0.16386512298583986

Epoch  5
Training loss:  0.061404637472331526
Training MSE:   0.061404637472331526
Training MAE:   0.1777374153137207
Testing loss:   0.07112967249155044
Testing MSE:    0.07112967249155044
Testing MAE:    0.20876920013427736

Epoch  6
Training loss:  0.06701513703987003
Training MSE:   0.06701513703987003
Training MAE:   0.19073341560363768
Testing loss:   0.08285265245437622
Testing MSE:    0.08285265245437622
Testing MAE:    0.2292817108154297

Epoch  7
Training loss:  0.06854855731800198
Training MSE:   0.06854855731800198
Training MAE:   0.19501261825561522
Testing loss:   0.06340206245779992
Testing MSE:    0.06340206245779992
Testing MAE:    0.19941778259277343

Epoch  8
Training loss:  0.07501435278058052
Training MSE:   0.07501435278058052
Training MAE:   0.20694430541992187
Testing loss:   0.04456750429272652
Testing MSE:    0.04456750429272652
Testing MAE:    0.1498326950073242

Epoch  9
Training loss:  0.08168655521571636
Training MSE:   0.08168655521571636
Training MAE:   0.21510942001342773
Testing loss:   0.03359743785560131
Testing MSE:    0.03359743785560131
Testing MAE:    0.1297761520385742

Training loss:  0.0345535834684968
Training MSE:   0.0345535834684968
Training MAE:   0.13064229736328126

Testing loss:  0.03359743785560131
Testing MSE:   0.03359743785560131
Testing MAE:   0.1297761520385742

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 20, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  16199.386483035278
Training MSE:   16199.386483035278
Training MAE:   50.207622777938845
Testing loss:   79.26061163330078
Testing MSE:    79.26061163330078
Testing MAE:    6.950084378051757

Epoch  1
Training loss:  55.74147625427246
Training MSE:   55.74147625427246
Training MAE:   5.743847420501709
Testing loss:   25.06411388244629
Testing MSE:    25.06411388244629
Testing MAE:    3.916534716796875

Epoch  2
Training loss:  10.9791379570961
Training MSE:   10.9791379570961
Training MAE:   2.4154150764465334
Testing loss:   2.3474094093322755
Testing MSE:    2.3474094093322755
Testing MAE:    1.1603985504150391

Epoch  3
Training loss:  1.023195007944107
Training MSE:   1.023195007944107
Training MAE:   0.7148553829193115
Testing loss:   0.3143171044588089
Testing MSE:    0.3143171044588089
Testing MAE:    0.4138866149902344

Epoch  4
Training loss:  0.2253762904137373
Training MSE:   0.2253762904137373
Training MAE:   0.30995044021606444
Testing loss:   0.1324181223988533
Testing MSE:    0.1324181223988533
Testing MAE:    0.24914879150390626

Epoch  5
Training loss:  0.13339664890915157
Training MSE:   0.13339664890915157
Training MAE:   0.23297910079956055
Testing loss:   0.07765776131153107
Testing MSE:    0.07765776131153107
Testing MAE:    0.17476720733642578

Epoch  6
Training loss:  0.10545390835851431
Training MSE:   0.10545390835851431
Training MAE:   0.21794601402282715
Testing loss:   0.06381512503623962
Testing MSE:    0.06381512503623962
Testing MAE:    0.1639359359741211

Epoch  7
Training loss:  0.1071323000729084
Training MSE:   0.1071323000729084
Training MAE:   0.2335347267150879
Testing loss:   0.1075041918873787
Testing MSE:    0.1075041918873787
Testing MAE:    0.2517049468994141

Epoch  8
Training loss:  0.12586631528586148
Training MSE:   0.12586631528586148
Training MAE:   0.2673300785064697
Testing loss:   0.2746048716545105
Testing MSE:    0.2746048716545105
Testing MAE:    0.47829107208251953

Epoch  9
Training loss:  0.1007655082963407
Training MSE:   0.1007655082963407
Training MAE:   0.23768695831298828
Testing loss:   0.07482300993204116
Testing MSE:    0.07482300993204116
Testing MAE:    0.2081765838623047

Training loss:  0.07754366536587477
Training MSE:   0.07754366536587477
Training MAE:   0.20835196342468262

Testing loss:  0.07482300993204116
Testing MSE:   0.07482300993204116
Testing MAE:   0.2081765838623047

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 20, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  11098.025984516908
Training MSE:   11098.025984516908
Training MAE:   37.444160440444946
Testing loss:   36.261171710205076
Testing MSE:    36.261171710205076
Testing MAE:    4.599924923706054

Epoch  1
Training loss:  28.59950661087036
Training MSE:   28.59950661087036
Training MAE:   4.097357713317871
Testing loss:   20.37678561706543
Testing MSE:    20.37678561706543
Testing MAE:    3.4620030532836914

Epoch  2
Training loss:  13.764410289955139
Training MSE:   13.764410289955139
Training MAE:   2.8207144954681396
Testing loss:   7.979605937957763
Testing MSE:    7.979605937957763
Testing MAE:    2.2206950714111326

Epoch  3
Training loss:  4.226238510084152
Training MSE:   4.226238510084152
Training MAE:   1.536699229812622
Testing loss:   2.1014839347839356
Testing MSE:    2.1014839347839356
Testing MAE:    1.1672872695922851

Epoch  4
Training loss:  0.9234238761544228
Training MSE:   0.9234238761544228
Training MAE:   0.7107001266479492
Testing loss:   0.4570003108501434
Testing MSE:    0.4570003108501434
Testing MAE:    0.5265352828979493

Epoch  5
Training loss:  0.27821605768203733
Training MSE:   0.27821605768203733
Training MAE:   0.3787890411376953
Testing loss:   0.17592655341625213
Testing MSE:    0.17592655341625213
Testing MAE:    0.2881236358642578

Epoch  6
Training loss:  0.14756791767776012
Training MSE:   0.14756791767776012
Training MAE:   0.26219949073791504
Testing loss:   0.21293286232948302
Testing MSE:    0.21293286232948302
Testing MAE:    0.3885507080078125

Epoch  7
Training loss:  0.12114519659876824
Training MSE:   0.12114519659876824
Training MAE:   0.24731646423339843
Testing loss:   0.0764232986330986
Testing MSE:    0.0764232986330986
Testing MAE:    0.19175911712646485

Epoch  8
Training loss:  0.1267667734399438
Training MSE:   0.1267667734399438
Training MAE:   0.2642015193939209
Testing loss:   0.10154209874868393
Testing MSE:    0.10154209874868393
Testing MAE:    0.23551981048583984

Epoch  9
Training loss:  0.11518897026479244
Training MSE:   0.11518897026479244
Training MAE:   0.25433448295593264
Testing loss:   0.2283225881576538
Testing MSE:    0.2283225881576538
Testing MAE:    0.4215853225708008

Training loss:  0.23037035195827485
Training MSE:   0.23037035195827485
Training MAE:   0.4221566795349121

Testing loss:  0.2283225881576538
Testing MSE:   0.2283225881576538
Testing MAE:   0.4215853225708008

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 20, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  12882.725027453613
Training MSE:   12882.725027453613
Training MAE:   48.750842416381836
Testing loss:   251.02007763671875
Testing MSE:    251.02007763671875
Testing MAE:    12.370226025390625

Epoch  1
Training loss:  186.6604742828369
Training MSE:   186.6604742828369
Training MAE:   10.653434511566163
Testing loss:   134.41523518066407
Testing MSE:    134.41523518066407
Testing MAE:    8.96086678314209

Epoch  2
Training loss:  88.4059932220459
Training MSE:   88.4059932220459
Training MAE:   7.305007207870483
Testing loss:   53.08250664672852
Testing MSE:    53.08250664672852
Testing MAE:    5.672990985107422

Epoch  3
Training loss:  31.7362409740448
Training MSE:   31.7362409740448
Training MAE:   4.341157134628296
Testing loss:   15.016638188171386
Testing MSE:    15.016638188171386
Testing MAE:    3.0044102264404295

Epoch  4
Training loss:  7.360593052577973
Training MSE:   7.360593052577973
Training MAE:   2.0543689182281493
Testing loss:   2.379649642372131
Testing MSE:    2.379649642372131
Testing MAE:    1.1940440505981444

Epoch  5
Training loss:  1.1414442497253419
Training MSE:   1.1414442497253419
Training MAE:   0.7900745079040528
Testing loss:   0.37322259097099303
Testing MSE:    0.37322259097099303
Testing MAE:    0.4536834716796875

Epoch  6
Training loss:  0.26495207066833976
Training MSE:   0.26495207066833976
Training MAE:   0.37448736267089844
Testing loss:   0.13115954942703248
Testing MSE:    0.13115954942703248
Testing MAE:    0.2503132080078125

Epoch  7
Training loss:  0.14664619428366424
Training MSE:   0.14664619428366424
Training MAE:   0.27071107177734377
Testing loss:   0.11446638903617859
Testing MSE:    0.11446638903617859
Testing MAE:    0.23558490753173827

Epoch  8
Training loss:  0.1257624684497714
Training MSE:   0.1257624684497714
Training MAE:   0.2552951271057129
Testing loss:   0.11230490211248398
Testing MSE:    0.11230490211248398
Testing MAE:    0.26391707000732423

Epoch  9
Training loss:  0.12262101457417011
Training MSE:   0.12262101457417011
Training MAE:   0.25872304306030275
Testing loss:   0.07874387233257293
Testing MSE:    0.07874387233257293
Testing MAE:    0.21125752868652345

Training loss:  0.08078723179548979
Training MSE:   0.08078723179548979
Training MAE:   0.21074965858459474

Testing loss:  0.07874387233257293
Testing MSE:   0.07874387233257293
Testing MAE:   0.21125752868652345

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 15, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  14033.912311064149
Training MSE:   14033.912311064149
Training MAE:   45.14122200241089
Testing loss:   51.971411584472655
Testing MSE:    51.971411584472655
Testing MAE:    5.625753811645508

Epoch  1
Training loss:  43.87383007659912
Training MSE:   43.87383007659912
Training MAE:   5.138525207138062
Testing loss:   34.66049870300293
Testing MSE:    34.66049870300293
Testing MAE:    4.580129270935059

Epoch  2
Training loss:  26.198445778656005
Training MSE:   26.198445778656005
Training MAE:   3.95846455039978
Testing loss:   17.737037521362303
Testing MSE:    17.737037521362303
Testing MAE:    3.2937955184936523

Epoch  3
Training loss:  11.869908407402038
Training MSE:   11.869908407402038
Training MAE:   2.6532815727233885
Testing loss:   6.55874813156128
Testing MSE:    6.55874813156128
Testing MAE:    1.9784126663208008

Epoch  4
Training loss:  3.8548546984672547
Training MSE:   3.8548546984672547
Training MAE:   1.5036803691864014
Testing loss:   1.6735393993377685
Testing MSE:    1.6735393993377685
Testing MAE:    1.0135062515258788

Epoch  5
Training loss:  0.8863234714627266
Training MSE:   0.8863234714627266
Training MAE:   0.717496155166626
Testing loss:   0.4033813980579376
Testing MSE:    0.4033813980579376
Testing MAE:    0.5080584548950196

Epoch  6
Training loss:  0.15653076403588057
Training MSE:   0.15653076403588057
Training MAE:   0.29581054496765136
Testing loss:   0.10328425308465958
Testing MSE:    0.10328425308465958
Testing MAE:    0.25298580169677737

Epoch  7
Training loss:  0.07923356087356806
Training MSE:   0.07923356087356806
Training MAE:   0.2093032485961914
Testing loss:   0.08504723429679871
Testing MSE:    0.08504723429679871
Testing MAE:    0.23134936676025392

Epoch  8
Training loss:  0.08298450652807951
Training MSE:   0.08298450652807951
Training MAE:   0.21698326148986816
Testing loss:   0.049362259542942044
Testing MSE:    0.049362259542942044
Testing MAE:    0.16089491119384766

Epoch  9
Training loss:  0.09429564995765687
Training MSE:   0.09429564995765687
Training MAE:   0.23469727516174316
Testing loss:   0.08499879282712937
Testing MSE:    0.08499879282712937
Testing MAE:    0.2298387451171875

Training loss:  0.08668769121468067
Training MSE:   0.08668769121468067
Training MAE:   0.22991921882629396

Testing loss:  0.08499879282712937
Testing MSE:   0.08499879282712937
Testing MAE:   0.2298387451171875

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 15, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  9805.242126867675
Training MSE:   9805.242126867675
Training MAE:   33.2273584815979
Testing loss:   32.65748714447022
Testing MSE:    32.65748714447022
Testing MAE:    4.586193542480469

Epoch  1
Training loss:  22.7164694480896
Training MSE:   22.7164694480896
Training MAE:   3.8081291954040526
Testing loss:   15.140440707397461
Testing MSE:    15.140440707397461
Testing MAE:    3.1333556045532225

Epoch  2
Training loss:  9.596052759361267
Training MSE:   9.596052759361267
Training MAE:   2.4516086082458495
Testing loss:   5.543430744552612
Testing MSE:    5.543430744552612
Testing MAE:    1.882237287902832

Epoch  3
Training loss:  3.332331151008606
Training MSE:   3.332331151008606
Training MAE:   1.4432271743774414
Testing loss:   1.752573644065857
Testing MSE:    1.752573644065857
Testing MAE:    1.0658214416503906

Epoch  4
Training loss:  0.9839702111721039
Training MSE:   0.9839702111721039
Training MAE:   0.7783503349304199
Testing loss:   0.3936070507526398
Testing MSE:    0.3936070507526398
Testing MAE:    0.49971463317871095

Epoch  5
Training loss:  0.2155963267058134
Training MSE:   0.2155963267058134
Training MAE:   0.35081695289611814
Testing loss:   0.10124188492298127
Testing MSE:    0.10124188492298127
Testing MAE:    0.24159300842285156

Epoch  6
Training loss:  0.08987610757350922
Training MSE:   0.08987610757350922
Training MAE:   0.2157918155670166
Testing loss:   0.10248509027957917
Testing MSE:    0.10248509027957917
Testing MAE:    0.2568184310913086

Epoch  7
Training loss:  0.10172256337478756
Training MSE:   0.10172256337478756
Training MAE:   0.23735087509155273
Testing loss:   0.17396868188381195
Testing MSE:    0.17396868188381195
Testing MAE:    0.35937314453125

Epoch  8
Training loss:  0.10746248151436448
Training MSE:   0.10746248151436448
Training MAE:   0.24656194839477538
Testing loss:   0.055198947203159335
Testing MSE:    0.055198947203159335
Testing MAE:    0.17541609497070312

Epoch  9
Training loss:  0.1053284920476377
Training MSE:   0.1053284920476377
Training MAE:   0.24944375648498535
Testing loss:   0.1552971734523773
Testing MSE:    0.1552971734523773
Testing MAE:    0.34810177154541017

Training loss:  0.15633960582613946
Training MSE:   0.15633960582613946
Training MAE:   0.3499216323852539

Testing loss:  0.1552971734523773
Testing MSE:   0.1552971734523773
Testing MAE:   0.34810177154541017

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 15, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  7965.859910202026
Training MSE:   7965.859910202026
Training MAE:   31.82608811149597
Testing loss:   75.19236786499023
Testing MSE:    75.19236786499023
Testing MAE:    6.803442132568359

Epoch  1
Training loss:  48.64440938186645
Training MSE:   48.64440938186645
Training MAE:   5.41574330368042
Testing loss:   26.419875891113282
Testing MSE:    26.419875891113282
Testing MAE:    4.029221762084961

Epoch  2
Training loss:  14.145529643440247
Training MSE:   14.145529643440247
Training MAE:   2.863344229888916
Testing loss:   5.677592044067382
Testing MSE:    5.677592044067382
Testing MAE:    1.8260751831054687

Epoch  3
Training loss:  3.0840967186927797
Training MSE:   3.0840967186927797
Training MAE:   1.2979195934295655
Testing loss:   1.2694111366271972
Testing MSE:    1.2694111366271972
Testing MAE:    0.8309416213989258

Epoch  4
Training loss:  0.81026668612957
Training MSE:   0.81026668612957
Training MAE:   0.656468197631836
Testing loss:   0.3843104166030884
Testing MSE:    0.3843104166030884
Testing MAE:    0.4564392623901367

Epoch  5
Training loss:  0.29546968510448934
Training MSE:   0.29546968510448934
Training MAE:   0.3920047492980957
Testing loss:   0.23487179296016694
Testing MSE:    0.23487179296016694
Testing MAE:    0.3538144500732422

Epoch  6
Training loss:  0.15390012185424568
Training MSE:   0.15390012185424568
Training MAE:   0.27749382972717285
Testing loss:   0.10203858337402344
Testing MSE:    0.10203858337402344
Testing MAE:    0.21957453155517578

Epoch  7
Training loss:  0.1316050590157509
Training MSE:   0.1316050590157509
Training MAE:   0.267275061416626
Testing loss:   0.07103730947375297
Testing MSE:    0.07103730947375297
Testing MAE:    0.18350746307373048

Epoch  8
Training loss:  0.1267182524174452
Training MSE:   0.1267182524174452
Training MAE:   0.26673358192443847
Testing loss:   0.13498758602142333
Testing MSE:    0.13498758602142333
Testing MAE:    0.30506937866210937

Epoch  9
Training loss:  0.12485517912432552
Training MSE:   0.12485517912432552
Training MAE:   0.2678398136138916
Testing loss:   0.05974858389496803
Testing MSE:    0.05974858389496803
Testing MAE:    0.17928141632080077

Training loss:  0.06111042034626007
Training MSE:   0.06111042034626007
Training MAE:   0.17806071281433106

Testing loss:  0.05974858389496803
Testing MSE:   0.05974858389496803
Testing MAE:   0.17928141632080077

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 15, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  13813.627608709718
Training MSE:   13813.627608709718
Training MAE:   49.11387930870056
Testing loss:   164.4693822631836
Testing MSE:    164.4693822631836
Testing MAE:    10.323974911499024

Epoch  1
Training loss:  127.27534948730468
Training MSE:   127.27534948730468
Training MAE:   8.996700615310669
Testing loss:   94.54400046386719
Testing MSE:    94.54400046386719
Testing MAE:    7.736441682434082

Epoch  2
Training loss:  64.82579441680909
Training MSE:   64.82579441680909
Training MAE:   6.380255911254883
Testing loss:   40.569880505371096
Testing MSE:    40.569880505371096
Testing MAE:    5.136540716552735

Epoch  3
Training loss:  23.882606217575074
Training MSE:   23.882606217575074
Training MAE:   3.8237831050872804
Testing loss:   11.367227033996581
Testing MSE:    11.367227033996581
Testing MAE:    2.667850126647949

Epoch  4
Training loss:  5.753755105924606
Training MSE:   5.753755105924606
Training MAE:   1.8192054512023925
Testing loss:   2.206260891342163
Testing MSE:    2.206260891342163
Testing MAE:    1.1746852127075196

Epoch  5
Training loss:  0.9812341630220414
Training MSE:   0.9812341630220414
Training MAE:   0.7233747844696045
Testing loss:   0.35231758937835694
Testing MSE:    0.35231758937835694
Testing MAE:    0.4326170227050781

Epoch  6
Training loss:  0.23356480396389961
Training MSE:   0.23356480396389961
Training MAE:   0.3359857646942139
Testing loss:   0.1976031383752823
Testing MSE:    0.1976031383752823
Testing MAE:    0.3408860565185547

Epoch  7
Training loss:  0.13279638138115407
Training MSE:   0.13279638138115407
Training MAE:   0.24348161239624022
Testing loss:   0.10178619311451911
Testing MSE:    0.10178619311451911
Testing MAE:    0.21027766571044923

Epoch  8
Training loss:  0.11788876289874316
Training MSE:   0.11788876289874316
Training MAE:   0.23525403900146485
Testing loss:   0.13556601754426956
Testing MSE:    0.13556601754426956
Testing MAE:    0.27300212707519533

Epoch  9
Training loss:  0.11026670483648777
Training MSE:   0.11026670483648777
Training MAE:   0.23671976203918457
Testing loss:   0.06400491196513176
Testing MSE:    0.06400491196513176
Testing MAE:    0.16156535339355468

Training loss:  0.0630552902199328
Training MSE:   0.0630552902199328
Training MAE:   0.16000695877075197

Testing loss:  0.06400491196513176
Testing MSE:   0.06400491196513176
Testing MAE:   0.16156535339355468

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 15, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  15608.12326345215
Training MSE:   15608.12326345215
Training MAE:   55.08212157440185
Testing loss:   191.12242202148437
Testing MSE:    191.12242202148437
Testing MAE:    11.041666455078126

Epoch  1
Training loss:  156.80541831054688
Training MSE:   156.80541831054688
Training MAE:   9.97636029434204
Testing loss:   125.56029282226562
Testing MSE:    125.56029282226562
Testing MAE:    8.930720486450195

Epoch  2
Training loss:  93.78132110595703
Training MSE:   93.78132110595703
Training MAE:   7.664035416412354
Testing loss:   65.76066227416992
Testing MSE:    65.76066227416992
Testing MAE:    6.4320404251098635

Epoch  3
Training loss:  44.023076770782474
Training MSE:   44.023076770782474
Training MAE:   5.201533057785034
Testing loss:   25.535539361572265
Testing MSE:    25.535539361572265
Testing MAE:    3.995556558227539

Epoch  4
Training loss:  14.825271245765686
Training MSE:   14.825271245765686
Training MAE:   2.956798610305786
Testing loss:   6.45399856262207
Testing MSE:    6.45399856262207
Testing MAE:    1.9765285354614257

Epoch  5
Training loss:  3.2139854516983033
Training MSE:   3.2139854516983033
Training MAE:   1.3300525032043458
Testing loss:   1.2131122163772583
Testing MSE:    1.2131122163772583
Testing MAE:    0.851513119506836

Epoch  6
Training loss:  0.5810768716931343
Training MSE:   0.5810768716931343
Training MAE:   0.5357303253173828
Testing loss:   0.245276194190979
Testing MSE:    0.245276194190979
Testing MAE:    0.333194499206543

Epoch  7
Training loss:  0.21676086440980435
Training MSE:   0.21676086440980435
Training MAE:   0.3115635841369629
Testing loss:   0.1487514183998108
Testing MSE:    0.1487514183998108
Testing MAE:    0.25451346588134766

Epoch  8
Training loss:  0.156666912586987
Training MSE:   0.156666912586987
Training MAE:   0.2667855857849121
Testing loss:   0.13448745048046112
Testing MSE:    0.13448745048046112
Testing MAE:    0.2616717788696289

Epoch  9
Training loss:  0.1411940391793847
Training MSE:   0.1411940391793847
Training MAE:   0.26390224876403806
Testing loss:   0.10407271041870117
Testing MSE:    0.10407271041870117
Testing MAE:    0.21407745971679687

Training loss:  0.10525633397400379
Training MSE:   0.10525633397400379
Training MAE:   0.2135833740234375

Testing loss:  0.10407271041870117
Testing MSE:   0.10407271041870117
Testing MAE:   0.21407745971679687

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 10, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  22778.712189682006
Training MSE:   22778.712189682006
Training MAE:   65.38754944534301
Testing loss:   14.180660813903808
Testing MSE:    14.180660813903808
Testing MAE:    2.9897897064208983

Epoch  1
Training loss:  6.557851729106903
Training MSE:   6.557851729106903
Training MAE:   1.954870648574829
Testing loss:   2.294927395248413
Testing MSE:    2.294927395248413
Testing MAE:    1.199491307067871

Epoch  2
Training loss:  1.1705574546217918
Training MSE:   1.1705574546217918
Training MAE:   0.825661710357666
Testing loss:   0.39731135601997375
Testing MSE:    0.39731135601997375
Testing MAE:    0.49084422454833987

Epoch  3
Training loss:  0.20564265151917935
Training MSE:   0.20564265151917935
Training MAE:   0.3320197772979736
Testing loss:   0.08611328328847885
Testing MSE:    0.08611328328847885
Testing MAE:    0.20385343017578125

Epoch  4
Training loss:  0.07028099714070558
Training MSE:   0.07028099714070558
Training MAE:   0.1832868839263916
Testing loss:   0.1743528251886368
Testing MSE:    0.1743528251886368
Testing MAE:    0.3721297271728516

Epoch  5
Training loss:  0.05242386578693986
Training MSE:   0.05242386578693986
Training MAE:   0.16206477241516112
Testing loss:   0.042157776951789856
Testing MSE:    0.042157776951789856
Testing MAE:    0.14906182708740234

Epoch  6
Training loss:  0.05699843433201313
Training MSE:   0.05699843433201313
Training MAE:   0.176059676361084
Testing loss:   0.04452170603275299
Testing MSE:    0.04452170603275299
Testing MAE:    0.15391758422851562

Epoch  7
Training loss:  0.063695894247666
Training MSE:   0.063695894247666
Training MAE:   0.18963229598999023
Testing loss:   0.13435361063480378
Testing MSE:    0.13435361063480378
Testing MAE:    0.3252831314086914

Epoch  8
Training loss:  0.06730756654068827
Training MSE:   0.06730756654068827
Training MAE:   0.19652915878295898
Testing loss:   0.07765031690597535
Testing MSE:    0.07765031690597535
Testing MAE:    0.23120623474121094

Epoch  9
Training loss:  0.07640388956069946
Training MSE:   0.07640388956069946
Training MAE:   0.21122860565185547
Testing loss:   0.09913217182159424
Testing MSE:    0.09913217182159424
Testing MAE:    0.2704274688720703

Training loss:  0.10094920099973678
Training MSE:   0.10094920099973678
Training MAE:   0.2711808261871338

Testing loss:  0.09913217182159424
Testing MSE:   0.09913217182159424
Testing MAE:   0.2704274688720703

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 10, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  24472.472857836914
Training MSE:   24472.472857836914
Training MAE:   74.52824816970825
Testing loss:   25.319876727294922
Testing MSE:    25.319876727294922
Testing MAE:    4.041345011901855

Epoch  1
Training loss:  23.980191275024413
Training MSE:   23.980191275024413
Training MAE:   3.9336470184326173
Testing loss:   21.894439569091798
Testing MSE:    21.894439569091798
Testing MAE:    3.7524035415649415

Epoch  2
Training loss:  19.614034034729006
Training MSE:   19.614034034729006
Training MAE:   3.5537858432769776
Testing loss:   16.684126721191408
Testing MSE:    16.684126721191408
Testing MAE:    3.2733280685424804

Epoch  3
Training loss:  13.679857423400879
Training MSE:   13.679857423400879
Training MAE:   2.9590536220550536
Testing loss:   11.219476037597657
Testing MSE:    11.219476037597657
Testing MAE:    2.628472132873535

Epoch  4
Training loss:  7.5538098415374755
Training MSE:   7.5538098415374755
Training MAE:   2.1867225967407227
Testing loss:   5.014259725189209
Testing MSE:    5.014259725189209
Testing MAE:    1.7578163391113282

Epoch  5
Training loss:  2.9773175174713136
Training MSE:   2.9773175174713136
Training MAE:   1.357125386428833
Testing loss:   1.4987208072662352
Testing MSE:    1.4987208072662352
Testing MAE:    0.9648113906860352

Epoch  6
Training loss:  0.8132679454445839
Training MSE:   0.8132679454445839
Training MAE:   0.6896166725158691
Testing loss:   0.3801045949935913
Testing MSE:    0.3801045949935913
Testing MAE:    0.47644174041748044

Epoch  7
Training loss:  0.21801759963482617
Training MSE:   0.21801759963482617
Training MAE:   0.3319807804107666
Testing loss:   0.13322629264593125
Testing MSE:    0.13322629264593125
Testing MAE:    0.25451875

Epoch  8
Training loss:  0.10992585098743439
Training MSE:   0.10992585098743439
Training MAE:   0.21782898788452149
Testing loss:   0.08213591597676277
Testing MSE:    0.08213591597676277
Testing MAE:    0.19345110931396484

Epoch  9
Training loss:  0.07844274197816849
Training MSE:   0.07844274197816849
Training MAE:   0.18555444679260255
Testing loss:   0.05148534673154354
Testing MSE:    0.05148534673154354
Testing MAE:    0.14098821258544922

Training loss:  0.05214197432249784
Training MSE:   0.05214197432249784
Training MAE:   0.14020427589416504

Testing loss:  0.05148534673154354
Testing MSE:   0.05148534673154354
Testing MAE:   0.14098821258544922

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 10, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  10963.86501190796
Training MSE:   10963.86501190796
Training MAE:   38.42701890106201
Testing loss:   20.960537673950196
Testing MSE:    20.960537673950196
Testing MAE:    3.6384032333374026

Epoch  1
Training loss:  13.016962174987793
Training MSE:   13.016962174987793
Training MAE:   2.7849647769927977
Testing loss:   5.533777017211914
Testing MSE:    5.533777017211914
Testing MAE:    1.840653092956543

Epoch  2
Training loss:  2.7836314113378524
Training MSE:   2.7836314113378524
Training MAE:   1.2527908493041993
Testing loss:   0.7767744659423829
Testing MSE:    0.7767744659423829
Testing MAE:    0.6736908966064453

Epoch  3
Training loss:  0.2912932518288493
Training MSE:   0.2912932518288493
Training MAE:   0.36752036781311037
Testing loss:   0.11102648780345917
Testing MSE:    0.11102648780345917
Testing MAE:    0.20791934356689454

Epoch  4
Training loss:  0.09386570479050278
Training MSE:   0.09386570479050278
Training MAE:   0.18879423522949218
Testing loss:   0.07431213959753513
Testing MSE:    0.07431213959753513
Testing MAE:    0.1627930404663086

Epoch  5
Training loss:  0.07568986353948712
Training MSE:   0.07568986353948712
Training MAE:   0.17629515838623047
Testing loss:   0.06701638003587723
Testing MSE:    0.06701638003587723
Testing MAE:    0.1699335418701172

Epoch  6
Training loss:  0.07034219888150692
Training MSE:   0.07034219888150692
Training MAE:   0.17915148124694824
Testing loss:   0.05564132443070412
Testing MSE:    0.05564132443070412
Testing MAE:    0.15160422210693358

Epoch  7
Training loss:  0.06754806986004114
Training MSE:   0.06754806986004114
Training MAE:   0.18152082481384277
Testing loss:   0.05325607236623764
Testing MSE:    0.05325607236623764
Testing MAE:    0.15862526550292969

Epoch  8
Training loss:  0.07555001775771379
Training MSE:   0.07555001775771379
Training MAE:   0.20101165046691893
Testing loss:   0.07003565219044686
Testing MSE:    0.07003565219044686
Testing MAE:    0.19379830780029297

Epoch  9
Training loss:  0.08650410391613841
Training MSE:   0.08650410391613841
Training MAE:   0.2142170124053955
Testing loss:   0.042633136793971065
Testing MSE:    0.042633136793971065
Testing MAE:    0.14367045135498047

Training loss:  0.042510448688268664
Training MSE:   0.042510448688268664
Training MAE:   0.14294565620422364

Testing loss:  0.042633136793971065
Testing MSE:   0.042633136793971065
Testing MAE:   0.14367045135498047

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 10, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  12069.41956816101
Training MSE:   12069.41956816101
Training MAE:   44.65115527381897
Testing loss:   95.41851310424805
Testing MSE:    95.41851310424805
Testing MAE:    7.773412928771973

Epoch  1
Training loss:  80.52628220825196
Training MSE:   80.52628220825196
Training MAE:   7.1122081798553465
Testing loss:   65.91412022094727
Testing MSE:    65.91412022094727
Testing MAE:    6.461690885925293

Epoch  2
Training loss:  48.78760940093994
Training MSE:   48.78760940093994
Training MAE:   5.498770631408691
Testing loss:   35.26749491882324
Testing MSE:    35.26749491882324
Testing MAE:    4.631421762084961

Epoch  3
Training loss:  20.989867811203002
Training MSE:   20.989867811203002
Training MAE:   3.548859461212158
Testing loss:   11.670376774597168
Testing MSE:    11.670376774597168
Testing MAE:    2.6115503829956053

Epoch  4
Training loss:  5.49695563120842
Training MSE:   5.49695563120842
Training MAE:   1.7565530223846435
Testing loss:   2.4061535627365114
Testing MSE:    2.4061535627365114
Testing MAE:    1.1443343292236328

Epoch  5
Training loss:  0.9925661969304085
Training MSE:   0.9925661969304085
Training MAE:   0.7189655410766601
Testing loss:   0.3859557600021362
Testing MSE:    0.3859557600021362
Testing MAE:    0.4411148361206055

Epoch  6
Training loss:  0.26545571073889734
Training MSE:   0.26545571073889734
Training MAE:   0.35973791275024414
Testing loss:   0.17028208383321763
Testing MSE:    0.17028208383321763
Testing MAE:    0.28736179962158204

Epoch  7
Training loss:  0.162577132332325
Training MSE:   0.162577132332325
Training MAE:   0.27700447959899904
Testing loss:   0.14593934555053711
Testing MSE:    0.14593934555053711
Testing MAE:    0.28043467407226563

Epoch  8
Training loss:  0.14747737732827662
Training MSE:   0.14747737732827662
Training MAE:   0.26800249366760254
Testing loss:   0.09309994097352028
Testing MSE:    0.09309994097352028
Testing MAE:    0.19515526275634765

Epoch  9
Training loss:  0.12923559184968472
Training MSE:   0.12923559184968472
Training MAE:   0.2555492687225342
Testing loss:   0.1544551105260849
Testing MSE:    0.1544551105260849
Testing MAE:    0.31540277252197263

Training loss:  0.1529759430974722
Training MSE:   0.1529759430974722
Training MAE:   0.31515050506591796

Testing loss:  0.1544551105260849
Testing MSE:   0.1544551105260849
Testing MAE:   0.31540277252197263

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 10, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  16042.58929215393
Training MSE:   16042.58929215393
Training MAE:   54.51164924240112
Testing loss:   64.07225382080078
Testing MSE:    64.07225382080078
Testing MAE:    6.4976419235229494

Epoch  1
Training loss:  55.649619241333006
Training MSE:   55.649619241333006
Training MAE:   6.032734895324707
Testing loss:   47.96377446899414
Testing MSE:    47.96377446899414
Testing MAE:    5.609227670288086

Epoch  2
Training loss:  37.14887248458862
Training MSE:   37.14887248458862
Training MAE:   4.914979388046264
Testing loss:   27.268103198242187
Testing MSE:    27.268103198242187
Testing MAE:    4.246116725158691

Epoch  3
Training loss:  17.69032152366638
Training MSE:   17.69032152366638
Training MAE:   3.3689892051696777
Testing loss:   9.244393334960938
Testing MSE:    9.244393334960938
Testing MAE:    2.451351002502441

Epoch  4
Training loss:  3.3319928621768953
Training MSE:   3.3319928621768953
Training MAE:   1.3752208461761475
Testing loss:   0.6296430899620056
Testing MSE:    0.6296430899620056
Testing MAE:    0.6072958297729493

Epoch  5
Training loss:  0.2940985832929611
Training MSE:   0.2940985832929611
Training MAE:   0.3814242973327637
Testing loss:   0.1667241276860237
Testing MSE:    0.1667241276860237
Testing MAE:    0.2843481384277344

Epoch  6
Training loss:  0.13021965653300285
Training MSE:   0.13021965653300285
Training MAE:   0.23443091354370116
Testing loss:   0.12203369519114494
Testing MSE:    0.12203369519114494
Testing MAE:    0.24432247467041016

Epoch  7
Training loss:  0.1017237163476646
Training MSE:   0.1017237163476646
Training MAE:   0.2101807514190674
Testing loss:   0.08257309795618058
Testing MSE:    0.08257309795618058
Testing MAE:    0.17677430877685546

Epoch  8
Training loss:  0.10024029357582331
Training MSE:   0.10024029357582331
Training MAE:   0.21964684371948243
Testing loss:   0.07343564987778664
Testing MSE:    0.07343564987778664
Testing MAE:    0.18295188446044922

Epoch  9
Training loss:  0.10519154871851207
Training MSE:   0.10519154871851207
Training MAE:   0.23482740020751952
Testing loss:   0.10012557556629181
Testing MSE:    0.10012557556629181
Testing MAE:    0.24518228454589844

Training loss:  0.09788031326532363
Training MSE:   0.09788031326532363
Training MAE:   0.24395387077331543

Testing loss:  0.10012557556629181
Testing MSE:   0.10012557556629181
Testing MAE:   0.24518228454589844

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 5, 25]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  14325.84114388733
Training MSE:   14325.84114388733
Training MAE:   49.26034800491333
Testing loss:   41.82303330078125
Testing MSE:    41.82303330078125
Testing MAE:    5.039204141235351

Epoch  1
Training loss:  36.007309896850586
Training MSE:   36.007309896850586
Training MAE:   4.626311859130859
Testing loss:   29.650479037475584
Testing MSE:    29.650479037475584
Testing MAE:    4.202841004943847

Epoch  2
Training loss:  24.917225909423827
Training MSE:   24.917225909423827
Training MAE:   3.8416318004608154
Testing loss:   19.792066766357422
Testing MSE:    19.792066766357422
Testing MAE:    3.4464718276977537

Epoch  3
Training loss:  14.347261615753174
Training MSE:   14.347261615753174
Training MAE:   2.901266822814941
Testing loss:   9.241253730773925
Testing MSE:    9.241253730773925
Testing MAE:    2.343076770019531

Epoch  4
Training loss:  6.151525237751007
Training MSE:   6.151525237751007
Training MAE:   1.8845632621765136
Testing loss:   3.1675684341430665
Testing MSE:    3.1675684341430665
Testing MAE:    1.3590030548095704

Epoch  5
Training loss:  1.8184333564043045
Training MSE:   1.8184333564043045
Training MAE:   1.0067077667236328
Testing loss:   0.8524718091964721
Testing MSE:    0.8524718091964721
Testing MAE:    0.6905546676635742

Epoch  6
Training loss:  0.512008642488718
Training MSE:   0.512008642488718
Training MAE:   0.5285478797912597
Testing loss:   0.26946409739255905
Testing MSE:    0.26946409739255905
Testing MAE:    0.3702638214111328

Epoch  7
Training loss:  0.2160278658002615
Training MSE:   0.2160278658002615
Training MAE:   0.3282585330963135
Testing loss:   0.1844796732187271
Testing MSE:    0.1844796732187271
Testing MAE:    0.30959318389892576

Epoch  8
Training loss:  0.12778211121559144
Training MSE:   0.12778211121559144
Training MAE:   0.24500432472229003
Testing loss:   0.1317433771252632
Testing MSE:    0.1317433771252632
Testing MAE:    0.28244242553710935

Epoch  9
Training loss:  0.10850822895616292
Training MSE:   0.10850822895616292
Training MAE:   0.23293028717041014
Testing loss:   0.08257728917598724
Testing MSE:    0.08257728917598724
Testing MAE:    0.2075040817260742

Training loss:  0.08405394194424153
Training MSE:   0.08405394194424153
Training MAE:   0.20762994003295898

Testing loss:  0.08257728917598724
Testing MSE:   0.08257728917598724
Testing MAE:   0.2075040817260742

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 5, 20]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  18224.256379431154
Training MSE:   18224.256379431154
Training MAE:   55.5813826587677
Testing loss:   23.861360073852538
Testing MSE:    23.861360073852538
Testing MAE:    3.8023875732421875

Epoch  1
Training loss:  17.956587325668334
Training MSE:   17.956587325668334
Training MAE:   3.280710034942627
Testing loss:   12.803988356781005
Testing MSE:    12.803988356781005
Testing MAE:    2.78220693359375

Epoch  2
Training loss:  9.264554377746583
Training MSE:   9.264554377746583
Training MAE:   2.348476248550415
Testing loss:   6.394063681793213
Testing MSE:    6.394063681793213
Testing MAE:    1.95912138671875

Epoch  3
Training loss:  4.653980006980896
Training MSE:   4.653980006980896
Training MAE:   1.6691222240447998
Testing loss:   3.0301489864349365
Testing MSE:    3.0301489864349365
Testing MAE:    1.3566077178955078

Epoch  4
Training loss:  2.0391199716091157
Training MSE:   2.0391199716091157
Training MAE:   1.1020842483520508
Testing loss:   1.157416562461853
Testing MSE:    1.157416562461853
Testing MAE:    0.8363667556762695

Epoch  5
Training loss:  0.7491841409802437
Training MSE:   0.7491841409802437
Training MAE:   0.6588351367950439
Testing loss:   0.4210065351009369
Testing MSE:    0.4210065351009369
Testing MAE:    0.501045361328125

Epoch  6
Training loss:  0.265262509277463
Training MSE:   0.265262509277463
Training MAE:   0.373814835357666
Testing loss:   0.14716823618412017
Testing MSE:    0.14716823618412017
Testing MAE:    0.269427197265625

Epoch  7
Training loss:  0.12801676910221577
Training MSE:   0.12801676910221577
Training MAE:   0.2452503463745117
Testing loss:   0.11153875283002854
Testing MSE:    0.11153875283002854
Testing MAE:    0.25181824340820314

Epoch  8
Training loss:  0.09190001141428948
Training MSE:   0.09190001141428948
Training MAE:   0.21008808479309082
Testing loss:   0.08312136229872703
Testing MSE:    0.08312136229872703
Testing MAE:    0.20010313720703124

Epoch  9
Training loss:  0.08394582788646221
Training MSE:   0.08394582788646221
Training MAE:   0.20705854721069336
Testing loss:   0.0525632271617651
Testing MSE:    0.0525632271617651
Testing MAE:    0.1519168167114258

Training loss:  0.05294358439818025
Training MSE:   0.05294358439818025
Training MAE:   0.1505799877166748

Testing loss:  0.0525632271617651
Testing MSE:   0.0525632271617651
Testing MAE:   0.1519168167114258

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 5, 15]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  17592.796487902833
Training MSE:   17592.796487902833
Training MAE:   57.71668761100769
Testing loss:   75.93081846923828
Testing MSE:    75.93081846923828
Testing MAE:    6.864622923278809

Epoch  1
Training loss:  66.33396190338135
Training MSE:   66.33396190338135
Training MAE:   6.453654448699951
Testing loss:   57.344017700195316
Testing MSE:    57.344017700195316
Testing MAE:    5.933941143798828

Epoch  2
Training loss:  45.067276348114014
Training MSE:   45.067276348114014
Training MAE:   5.30781441116333
Testing loss:   33.84964576721191
Testing MSE:    33.84964576721191
Testing MAE:    4.575029638671875

Epoch  3
Training loss:  23.4380881023407
Training MSE:   23.4380881023407
Training MAE:   3.8078570301055907
Testing loss:   15.798168655395507
Testing MSE:    15.798168655395507
Testing MAE:    3.2144066024780273

Epoch  4
Training loss:  8.1408809715271
Training MSE:   8.1408809715271
Training MAE:   2.199370822906494
Testing loss:   3.6013103454589843
Testing MSE:    3.6013103454589843
Testing MAE:    1.5033338088989259

Epoch  5
Training loss:  1.5202909114956855
Training MSE:   1.5202909114956855
Training MAE:   0.9146613739013671
Testing loss:   0.39217830390930175
Testing MSE:    0.39217830390930175
Testing MAE:    0.46901715240478514

Epoch  6
Training loss:  0.15771204649209977
Training MSE:   0.15771204649209977
Training MAE:   0.2776907863616943
Testing loss:   0.11572597119808196
Testing MSE:    0.11572597119808196
Testing MAE:    0.24137600402832032

Epoch  7
Training loss:  0.08184622263908387
Training MSE:   0.08184622263908387
Training MAE:   0.1922043888092041
Testing loss:   0.07129333269000053
Testing MSE:    0.07129333269000053
Testing MAE:    0.1781030776977539

Epoch  8
Training loss:  0.0830724742770195
Training MSE:   0.0830724742770195
Training MAE:   0.20280454788208008
Testing loss:   0.10612747920751571
Testing MSE:    0.10612747920751571
Testing MAE:    0.24679059448242188

Epoch  9
Training loss:  0.08574695712327957
Training MSE:   0.08574695712327957
Training MAE:   0.21218323364257813
Testing loss:   0.05783085775375366
Testing MSE:    0.05783085775375366
Testing MAE:    0.17180366821289061

Training loss:  0.05819516357183457
Training MSE:   0.05819516357183457
Training MAE:   0.17052679862976075

Testing loss:  0.05783085775375366
Testing MSE:   0.05783085775375366
Testing MAE:   0.17180366821289061

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 5, 10]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  28301.161299786378
Training MSE:   28301.161299786378
Training MAE:   80.04219962425232
Testing loss:   75.43951734619141
Testing MSE:    75.43951734619141
Testing MAE:    6.887004446411133

Epoch  1
Training loss:  55.82366964569092
Training MSE:   55.82366964569092
Training MAE:   5.87812486000061
Testing loss:   44.900950079345705
Testing MSE:    44.900950079345705
Testing MAE:    5.228126681518555

Epoch  2
Training loss:  32.98356830215454
Training MSE:   32.98356830215454
Training MAE:   4.509438983917236
Testing loss:   23.19610377807617
Testing MSE:    23.19610377807617
Testing MAE:    3.7402485961914063

Epoch  3
Training loss:  14.627381386566162
Training MSE:   14.627381386566162
Training MAE:   2.9824396129608153
Testing loss:   8.351178327941895
Testing MSE:    8.351178327941895
Testing MAE:    2.2377265411376954

Epoch  4
Training loss:  4.662422047996521
Training MSE:   4.662422047996521
Training MAE:   1.6675785682678224
Testing loss:   2.331764316177368
Testing MSE:    2.331764316177368
Testing MAE:    1.2235859573364258

Epoch  5
Training loss:  1.1058558977961541
Training MSE:   1.1058558977961541
Training MAE:   0.801636018371582
Testing loss:   0.43091729173660276
Testing MSE:    0.43091729173660276
Testing MAE:    0.5015481597900391

Epoch  6
Training loss:  0.2618791178673506
Training MSE:   0.2618791178673506
Training MAE:   0.383846817779541
Testing loss:   0.13358012973070144
Testing MSE:    0.13358012973070144
Testing MAE:    0.27114383697509764

Epoch  7
Training loss:  0.11878589295297862
Training MSE:   0.11878589295297862
Training MAE:   0.2532855579376221
Testing loss:   0.18419402401447296
Testing MSE:    0.18419402401447296
Testing MAE:    0.36271780395507813

Epoch  8
Training loss:  0.09875520499795676
Training MSE:   0.09875520499795676
Training MAE:   0.23314559135437013
Testing loss:   0.06813994261026382
Testing MSE:    0.06813994261026382
Testing MAE:    0.18030435485839844

Epoch  9
Training loss:  0.09584894197434186
Training MSE:   0.09584894197434186
Training MAE:   0.2306975070953369
Testing loss:   0.06366241137385369
Testing MSE:    0.06366241137385369
Testing MAE:    0.1855920654296875

Training loss:  0.06378223142325878
Training MSE:   0.06378223142325878
Training MAE:   0.18590231590270997

Testing loss:  0.06366241137385369
Testing MSE:   0.06366241137385369
Testing MAE:   0.1855920654296875

Number of layers:  3
Number of units in layer [1,2,3]:  [5, 5, 5]
Activation function in layer [1,2,3]:  ['relu', 'relu', 'relu']
Loss function:  mean_squared_error
Number of epochs:  10
Batch size:  32

Epoch  0
Training loss:  22913.219720544435
Training MSE:   22913.219720544435
Training MAE:   76.37932068481446
Testing loss:   140.82968544921874
Testing MSE:    140.82968544921874
Testing MAE:    9.468919631958007

Epoch  1
Training loss:  121.5542468902588
Training MSE:   121.5542468902588
Training MAE:   8.844481871414185
Testing loss:   107.87034089355468
Testing MSE:    107.87034089355468
Testing MAE:    8.305983517456054

Epoch  2
Training loss:  86.34182083435059
Training MSE:   86.34182083435059
Training MAE:   7.433668569946289
Testing loss:   69.2541439086914
Testing MSE:    69.2541439086914
Testing MAE:    6.657025695800781

Epoch  3
Training loss:  49.980234284210205
Training MSE:   49.980234284210205
Training MAE:   5.627573739624023
Testing loss:   34.44463626098633
Testing MSE:    34.44463626098633
Testing MAE:    4.662087109375

Epoch  4
Training loss:  21.49884084777832
Training MSE:   21.49884084777832
Training MAE:   3.6550543319702147
Testing loss:   12.194295655822755
Testing MSE:    12.194295655822755
Testing MAE:    2.798931523132324

Epoch  5
Training loss:  5.947713934612274
Training MSE:   5.947713934612274
Training MAE:   1.8753203556060791
Testing loss:   2.324074127960205
Testing MSE:    2.324074127960205
Testing MAE:    1.1967232315063476

Epoch  6
Training loss:  1.0474341324687004
Training MSE:   1.0474341324687004
Training MAE:   0.754506318283081
Testing loss:   0.4550937334537506
Testing MSE:    0.4550937334537506
Testing MAE:    0.48815906677246096

Epoch  7
Training loss:  0.2570125069886446
Training MSE:   0.2570125069886446
Training MAE:   0.35002184448242185
Testing loss:   0.16065172107219697
Testing MSE:    0.16065172107219697
Testing MAE:    0.26712108917236327

Epoch  8
Training loss:  0.1308943969771266
Training MSE:   0.1308943969771266
Training MAE:   0.2396030212402344
Testing loss:   0.2575061403512955
Testing MSE:    0.2575061403512955
Testing MAE:    0.41795584716796874

Epoch  9
Training loss:  0.09879737036824227
Training MSE:   0.09879737036824227
Training MAE:   0.21121088104248048
Testing loss:   0.09771582772731781
Testing MSE:    0.09771582772731781
Testing MAE:    0.2330626235961914

Training loss:  0.09784742660969496
Training MSE:   0.09784742660969496
Training MAE:   0.23214708366394043

Testing loss:  0.09771582772731781
Testing MSE:   0.09771582772731781
Testing MAE:   0.2330626235961914

